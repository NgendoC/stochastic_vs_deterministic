%%%%%%%%%%%%%%%%%%%%%
%% Document set-up %%
%%%%%%%%%%%%%%%%%%%%%

% Requirements:
  % Double-spaced
  % minimum 11pt font
  % Arial or Verdana font
  % 2 cm margins

\documentclass[a4paper, 11pt]{article} % sets document shape and font size

\usepackage[margin=2.0cm]{geometry} % set margins to 2cm
% \usepackage[document]{ragged2e} % make text left-aligned

\usepackage{setspace, caption}
\captionsetup{font=doublespacing} %double-spaced float captions
\doublespacing %double-spaced document
\setlength{\parindent}{2em} % 5 space indent

% change font to Arial
\renewcommand{\rmdefault}{phv} % Arial
\renewcommand{\sfdefault}{phv} % Arial

\renewcommand*\contentsname{} % removes Table of Contents' title

\usepackage{amsmath} % Needed for maths equations
\usepackage{graphicx}
\graphicspath{ {/home/evelina/OneDrive/MRes_BMR/Project_1/Work_folder/Plots/} } % Where the images will be found 
% \graphicspath{ {/Work_folder/Plots/} } %

% \usepackage{natbib}
% \usepackage[numbers]{natbib}
\usepackage[numbers]{natbib}
%\setcitestyle{round}

% for creating a flow chart
\usepackage{tikz} 
\usetikzlibrary{shapes.geometric, arrows}
\tikzstyle{sir} = [rectangle, rounded corners, minimum width=3cm, minimum height=1cm,text centered, draw=black, fill=red!0]
\tikzstyle{arrow} = [thick,->,>=stealth]

\usepackage{multirow} % for combining rows in tables

\usepackage{float} % for forcing figure placement

\usepackage{fontspec}

\usepackage[section]{placeins}

\usepackage{tabularx} % make table page-wide

%%%%%%%%%%%%%%%%%%%%%%%
%% Start of document %%
%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
\SweaveOpts{concordance=TRUE}
\setmainfont[Ligatures=TeX]{Verdana}

%%%%%%%%%%%
%% Title %%
%%%%%%%%%%%

\begin{titlepage}
    \begin{center}
        \vspace*{2.5cm}
        
        \textbf{Comparing methods from deterministic and stochastic infectious disease modelling for parameter inference}
        
        \vspace{0.5cm}
        Project 1
        
        MRes Biomedical Research 
        
        Epidemiology, Evolution, and Control of Infectious Diseases Stream 
        
        \vspace{0.5cm}
        
        \textbf{Janetta E. Skarp}
        
        \vspace{2.5cm}
        
        \includegraphics[width=0.4\textwidth]{ICL_crest}
        % \includegraphics{ICL_crest.png}
        
        \vspace{2.5cm}
        
        Supervisor: Xavier Didelot\\ 
        Submitted: March 2018\\
        Department of Surgery and Cancer, Imperial College London
        
    \end{center}
    
\end{titlepage}

%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Originality statement %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\setcounter{section}{0}
%\renewcommand{\thesection}{\Roman{section}}
\section*{Statement of Originality}
\addcontentsline{toc}{section}{Statement of Originality}
I certify that this thesis, and the research to which it refers, are the product of my own work, conducted during the current year of the MRes in Biomedical Research at Imperial College London. Any ideas or quotations from the work of other people, published or otherwise, or from my own previous work are fully acknowledged in accordance with the standard referencing practices of the discipline.

I implemented my own outbreak-simulating stochastic SIR model and inference methods on the statistical computing language R, without the usage of any third-party packages excepting for the package \textit{deSolve} by Stoetaert et al., which I used for solving the differential equations of my deterministic SIR models. The group is credited in the relevant sections of this document. 
% Also acknowledge work of other researchers if necessary e.g. if you got samples from someone else.

%%%%%%%%%%%%%%
%% Abstract %%
%%%%%%%%%%%%%%

\newpage
\section*{Abstract}
\addcontentsline{toc}{section}{Abstract}
Different methods exist for inferring the ranges of values that a parameter might take in a model. These methods can broadly be categorised into deterministic methods, such as the residual error (RE) method, and stochastic methods, such as the Markov Chain Monte Carlo (MCMC) method. The aim of this project is to elucidate the conditions under which the RE method might be used just as well as the MCMC, if one is trying to quantify the range of values that a parameter might take. I created a stochastic population-based closed compartmental SIR model. With this model, I simulated altogether 30 different outbreaks for 6 different scenarios: with an $R_{0}$ of 1.5 or 6 with a population size of 50, 200, and 1000, with five repeats per type of combination. I then implemented RE and MCMC parameter inference methods and inferred the values that $\beta$ and $\gamma$ may take for each outbreak based on the removed curve obtained from the simulated outbreaks. A MCMC with a deterministic process was also explored. The MCMC inference method's 95\% credible intervals included the true $\beta$ and $\gamma$ values more often than the RE method. Within the RE method, the 95\% intervals were more consistent with including the true $\beta$ and $\gamma$ values as population size increased and with a higher $R_{0}$. As the running time and number of iterations required for convergence for the MCMC increases with increasing population size, eventually a trade-off of the RE method's narrower 95\% intervals for a much quicker running time might become a reasonable option. 
% 0.5 to 1 page

%%%%%%%%%%%%%%%%%%%%%%
%% Acknowledgements %%
%%%%%%%%%%%%%%%%%%%%%%
\newpage
\section*{Acknowledgements}
\addcontentsline{toc}{section}{Acknowledgements}

I would like to thank my supervisor, Dr Xavier Didelot, for his advice and support during the course of this project.

%%%%%%%%%%%%%%%%%%%%%%%
%% Table of contents %%
%%%%%%%%%%%%%%%%%%%%%%%

\newpage
\section*{Table of Contents}
\addcontentsline{toc}{section}{Table of Contents}
\vspace{-4em}
\tableofcontents

%%%%%%%%%%%%%%%%%%%
%% Abbreviations %%
%%%%%%%%%%%%%%%%%%%

\newpage
\section*{Abbreviations}
\addcontentsline{toc}{section}{Abbreviations}
\noindent $\beta$: per capita rate of infection

\noindent $\gamma$: recovery rate

\noindent MCMC: Markov Chain Monte Carlo

\noindent $R_{0}$: basic reproduction number

\noindent RE: residual error

\noindent SIR: susceptible, infected, removed



%%%%%%%%%%%%%%%%%%
%% Introduction %%
%%%%%%%%%%%%%%%%%%
\setcounter{section}{0}
\renewcommand{\thesection}{\arabic{section}}
\newpage
\section{Introduction}
% 5-10 pages

%%%%%%%%%%

\subsection{Compartmental models in infectious disease epidemiology}
Compartmental models have been used to model the movement of diseases in populations since their introduction to the field of epidemiology during the first half of the 20th century. Kermack and McKendrick were among the pioneers of this new method and in their joint paper published in 1927 they outlined how compartmental models could be used for infectious disease epidemiology \citep{Kermack1927}. As the name of the model suggests, a compartmental model splits the model population into different compartments depending on factors defined by the investigator, such as disease status or age group.
\newline
% Kermack and McKendrick 1927: http://rspa.royalsocietypublishing.org/content/royprsa/115/772/700.full.pdf

% SIR figure
\begin{figure}[H]
\begin{center}
\begin{tikzpicture}[node distance=5cm]
\node (S) [sir] {S};
\node (I) [sir, right of =S] {I};
\node (R) [sir, right of=I] {R};
\draw [arrow] (S) -- node[anchor=south] {$\beta S I$} (I);
\draw [arrow] (I) -- node[anchor=south] {$\gamma I$} (R);
\end{tikzpicture}
\caption{A closed compartmental SIR model. S: susceptible, I: infectious, R: removed, $\beta$: per capita rate of infection, $\gamma$: recovery rate}
\label{sir}
\end{center}
\end{figure}

One commonly used compartmental model is the SIR model which consists of three compartments: susceptible, infected, and removed (Fig. \ref{sir}). This type of SIR model is referred to as being closed, as there is no flow of individuals into the population or out of the population (i.e. births, deaths, or migration). 

There are two ways to implement an SIR model, the deterministic and stochastic method. For both a stochastic and deterministic SIR model, the population moves from the susceptible compartment to the infected compartment and from the infected compartment to the removed compartment with some rate $\beta$ and $\gamma$, respectively (Fig. \ref{sir}). The deterministic model is usually implemented in continuous time, implying that the difference between timesteps is vanishingly small. This results in the flows between compartments being described as differential equations:
\begin{align*}
\frac{dS}{dt} &= - \beta S I \\
\frac{dI}{dt} &= \beta S I - \gamma I \\
\frac{dR}{dt} &= \gamma I
\end{align*}
\newpage
\noindent As susceptibles need to come into contact with infected individuals in order to become infected themselves, $\beta$ is multiplied by the number of contacts made between members of the susceptible and infected compartments, which then results in the number of people who become newly infected \citep[p. 122-126]{Anderson1991}. Given that $\beta$ refers to the rate at which individuals become infected, the number of contacts is calculated by taking the product of the number of individuals susceptible and the number of individuals infected \citep[p. 122-126]{Anderson1991}. In order to calculate the number of infected individuals moving to the removed compartment, $\gamma$ is multiplied by the number of infected individuals.

Stochastic models can be used in continuous time, but it is often convenient to discretise time. Differences between timesteps are larger for discrete than continuous time. Therefore, the number of individuals in a given compartment at a given timestep is calculated from the previous timestep:
\begin{align*}
S_{i+1} &= S_{i} - b_{i} \\
I_{i+1} &= I_{i} + b_{i} - c_{i} \\
R_{i+1} &= R_{i} + c_{i}
\end{align*}

\noindent Here $S_{i}$, $I_{i}$, and $R_{i}$ refer to the number of individuals in each compartment at timestep $i$, while $S_{i+1}$, $I_{i+1}$, and $R_{i+1}$ refer to the numbers of individuals in the susceptible, infected, and removed compartments at the next timestep $i+1$. $b_{i}$ and $c_{i}$ refer to the number of newly infected and newly removed individuals at timestep $i$ respectively.

The number of individuals joining the infected compartment, $b_{i}$, or removed compartment, $c_{i}$ at timestep $i$ can be chosen randomly from a binomial distribution for a population-based stochastic compartmental SIR model: 
\begin{align*}
b_{i} &\sim Bin(S_{i}, 1 - e^{-\beta I_{i} dt}) \\
c_{i} &\sim Bin(I_{i}, 1 - e^{-\gamma dt})
\end{align*}

\noindent where $S_{i}$ and $I_{i}$ represent the number of susceptible and infected individuals at time $i$ respectively. The size of a timestep, the time elapsed between observed timepoints, is referred to by $dt$. From this, the numbers of susceptible, infected, and removed individuals at time $i+1$ can thus be calculated.

When one is deciding on a compartmental model to use, the differences between the two modelling methods should be taken into consideration. One of the major differences between the two is that a deterministic compartmental model always produces the same outcome when given the same input, while stochastic models show different outcomes with every run of the same input \citep{Rock2014}. Additionally, deterministic models are often easier to implement than stochastic models due to deterministic models making simplifying assumptions about population dynamics. These assumptions include the concept of a fraction of a person being infected, and that as long as the same proportion of people are infected at the start, there is no difference between outbreaks in large and small populations \citep{Rock2014,Roberts2015}. Deterministic models also require less computational power as every run is the same and therefore runs do not need to be repeated. Stochastic models, on the other hand, may be better for smaller populations where stochasticity of events is more pronounced and events such as epidemic fadeout are a more likely scenario than in bigger populations \citep{Rock2014}. It is therefore possible that sometimes the conclusions reached with the aid of stochastic and deterministic compartmental models may be similar, if the assumptions and limitations of the deterministic model are acknowledged and met. In such a case one might think that a deterministic model should be favoured due to it being less computationally taxing and quicker to implement.

When a model is being chosen, a researcher may not always ponder about what the best modelling strategy is for the task at hand, but rather use the method that they understand best or habitually use. As computers have become more powerful, the production of more computationally demanding stochastic models has gained popularity \citep{Rock2014}. This turn of events could be further enhanced by the acknowledgement of all outbreaks originally being stochastic processes \citep{Roberts2015}. 

Though compartmental models are useful for infectious disease modelling, it should be noted that they are not the only modelling method, and that there are cases for which neither the stochastic nor the deterministic compartmental model is the best model for the task at hand. An example of such a scenario would be modelling the spread of STIs, for which network modelling methods are also popular \citep{Garnett2002}.
% Paper for STI modelling methods: Garnett 2001 http://sti.bmj.com/content/sextrans/78/1/7.full.pdf

%%%%%%%%%%
\newpage
\subsection{Inference for compartmental infectious disease models}
% Base this section on papers discussed with Xavier

The well-known residual error (RE) method can be used for inference under a deterministic model. This method aims to minimise the discrepency between observed and expected data, calculated as the sum of squared differences between the value obtained from the model and the true data value. This is done through optimisation, where starting values for parameters are set, a deterministic model is simulated based on these chosen parameter values, the sum of squares is calculated, and the process is repeated until the RE is at a minimum. Ultimately, the aim of using this method is to retain the point estimate, the parameter combination that produces the lowest RE. The point estimate corresponds to the best fit of the deterministic model to the incomplete dataset it is being fitted to. One limitation of the RE method is that it does not capture the uncertainty in the parameter values, as it only calculates the point estimate.

Bootstrapping can be used to investigate the sensitivity of the best guess parameter combinations to slight changes in the data. This can be achieved by sampling with replacement from the data and repeating the optimisation process to reach the lowest RE as was done for the original dataset to obtain the point estimate. Repeated bootstrapping and subsequent optimisation of the data produces a range of parameter combinations. The breadth of this range can be used to estimate the amount of uncertainty around the point estimate.

Bayesian statistics can also be applied in order to infer ranges of values that a parameter in a model might take, typically through the usage of Markov Chain Monte Carlo (MCMC) methods \citep{ONeill1999}. Bayes' theorem lies at the heart of Bayesian statistics and states that:
\begin{align*}
P(\theta \mid x) \propto P(x \mid \theta)P(\theta)
\end{align*}
\noindent Here the prior distribution is defined as $P(\theta)$, the probability distribution of the values that the parameters in question might take before the data is observed. This is defined by the investigators' best guess for the values that parameters might take before taking into consideration the new data. $P(x \mid \theta)$, the likelihood function, refers to the probability of the observed data, $x$, occurring given the parameter value, $\theta$. The likelihood function can be multiplied by the prior distribution to produce the posterior distribution, $P(\theta \mid x)$, the probability of the parameter value given the data. 

The Markov Chain Monte Carlo inference method is rooted in Bayesian statistics. The uses of MCMC for parameter inference were first discovered in the late 20th century \citep{Gibson1998}. The method was quickly picked up by the infectious disease epidemiology community and its potential for infectious disease modelling was explored for multiple diseases and datasets, such as influenza and smallpox \citep{ONeill2002}. A random walk MCMC works by first setting a starting value for the parameter in question, and then randomly choosing another parameter value from a proposal distribution devised by the investigator. Such a distribution may for example be Normal with a mean of the current parameter value and a predefined standard deviation. It then calculates the posterior distribution for both the current parameter value and the proposed parameter value. 

The resulting posterior distributions are then compared by the usage of the Metropolis-Hastings ratio:
\begin{align*}
min \Big(1, \frac{p(\theta' \mid x)}{p(\theta \mid x)} \frac{q(\thata \mid \theta')}{q(\theta' \mid \theta)}\Big)
\end{align*}
Here $p$ refers to the posterior distribution and $q$ to the proposal distribution, $\theta$ refers to the current parameter value, $\theta'$ refers to the proposed parameter value, and $x$ refers to the observed data. If the resulting proportion is greater than 1, it implies that the proposed parameter value is more probable given the observed data and will take the place of the starting value as the new baseline parameter value. This will then be compared to a new proposed parameter value in the next iteration of the MCMC. To capture the uncertainty around the values that the inferred parameter might take, a worse proposed parameter value than the current one can be occasionally accepted by choosing a random number from a uniform distribution between 0 and 1 and accepting the proposed posterior if it is greater than this random number. This will lead to the proposed posterior to sometimes being accepted even if the proposed to baseline posterior ratio is less than 1. 

It can happen that the datasets that one wishes to use for parameter inference are incomplete. An example of this would be an outbreak where the dates of infection are not known but the dates of recovery are. MCMC methods can be adjusted to still estimate the ranges of values a parameter might take despite the unknown data. The infectious period could be assumed to be constant and thus an infectious curve can be calculated for each proposed change in the rates of infection and recovery \citep{Finkenstadt2000,Finkenstadt2002}. Alternatively, the unknown infection data can be included as an additional parameter in the MCMC \citep{ONeill1999}. A more abstract method can also be embarked upon, where the whole discrete stochastic SIR model is approximated through a diffusion process to estimate the values of unknown parameters \citep{Cauchemez2008}.

Stochastic methods, such as the MCMC, can be more difficult to implement than deterministic ones, such as the RE method. In order to have an optimally working MCMC, it may for example take multiple attempts to calibrate the proposal function to result in a sufficient acceptance rate of proposed parameters. Additionally, running a MCMC to completion often takes longer than for a RE. Furthermore, constructing a MCMC from scratch is more demanding than constructing a RE due to there being many distributions, the prior, likelihood, and proposal, to consider for each parameter of interest.  Also, the quality and quantity of the data available for model parameter inference should be considered, though the effects of data quality on inference will not be explored in this project. 
% If many factors are unknown, a complex stochastic modelling method making a multitude of assumptions may perform worse than a simpler deterministic model (mention HIV modelling or FMD example? - May 2003). 
% Find a couple of examples where the model used for the task may not have been the best?
% May (2003) Uses and abuses of mathematics in biology : Complex HIV model lost to a less complex model because the assumptions it made were wrong 

\subsection{Aims of this project}
In this project, I compare the parameter estimates of stochastic and deterministic models under different outbreak settings with varying population sizes and basic reproduction numbers ($R_{0}$). The aim of this is to elucidate the conditions under which a deterministic inference method, such as the residual error method, might be used just as well as a stochastic one, such as the MCMC, if one is trying to quantify the range of values that a parameter might take.

Considering that deterministic parameter inference methods are quicker to run, one might ask if the two could be combined to distil the best of both methods. MCMC has been combined with a deterministic process, for example in a paper exploring public health responses to bioterrorism \citep{Elderd2006}. Therefore, a MCMC with a deterministic process is also briefly explored preliminarily in this project.

%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Materials and methods %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage
\section{Materials and Methods}
% 5-10 pages

All data simulation and analysis presented in this report was conducted on R, a statistical computing language \citep{RCoreTeam2017}.

%%%%%%%%%%

\subsection{Simulating data for fitting}

I used a stochastic population-based closed compartmental SIR model, such as the one described in the Introduction (Fig. 1), to generate outbreaks with various basic reproduction numbers ($R_{0}$) in different-sized populations. Here $R_{0}$ refers to the number of susceptible individuals that an infectious individual would successfully infect in a completely susceptible population. 

I chose two different epidemic outbreak scenarios, one with an underlying $R_{0}$ of 1.5, on the lower end for a disease that could still cause an outbreak, and the other with a higher underlying $R_{0}$ of 6.  Both scenarios had a recovery rate, $\gamma$, of 0.15 at a given timepoint. Thus, the underlying per capita rate of infection, $\beta$, for the outbreaks could be calculated as 
\begin{align*}
\beta = \frac{R_{0} \times \gamma}{N}
\end{align*}

\noindent where N is the population size. 

I considered three different population sizes for both of the two outbreak scenarios: a small population size of 50, a medium population size of 200, and a large population size of 1000. The proportion of the population that was infected in each population at the beginning of the simulation was constant with 1 infectious individual in the population of 50, 4 infectious individuals in the population of 200, and 20 infectious individuals in the population of 1000. For each of the underlying outbreak strength and population size combinations (six combinations in total), I generated five different outbreaks with the devised stochastic model. Thus altogether, I simulated 30 distinct outbreaks. Each outbreak was set to be observed for 80 days at 0.5-day timesteps.

I used the data on the times of individuals' removal from the infectious compartment from these 30 outbreaks and various inference methods to estimate parameter value ranges for $\beta$ and $\gamma$. The estimates made through these methods can then be compared to the true underlying $\beta$ and $\gamma$, as they are also known due to the data being simulated.

%%%%%%%%%%

\subsection{Parameterisation}

\subsubsection{Residual error}

I used the residual error method to infer the values of $\beta$ and $\gamma$ given the removed curve from the simulated data made with the stochastic SIR model. Firstly, I constructed a deterministic SIR model with the aid of \textit{deSolve}, a differential equation solving R package \citep{Soetaert2010}. I then optimised the deterministic model's fit to the removed curve of the simulated data to receive a point estimate for the $\beta$ and $\gamma$ for each outbreak in addition the best-fit deterministic infectious curve. The fit of the model during the optimisation process was calculated as the sum of the squared differences between the model, $d_{model}^i$, and observation, $d_{observation}^i$, for each timepoint $i$:

\begin{align*}
\displaystyle\sum_{i}(d_{model}^i - d_{observation}^i)^2
\end{align*}

I gauged the uncertainty around the point estimate by bootstrapping. I achieved this by sampling timepoints with replacement from the original generated removed curve for a given outbreak and optimising the deterministic SIR model on that sampled curve. This would then bring into light whether there were any particular timepoints that might be driving the results of the optimisation process. 

For the simulated outbreaks with an underlying $R_{0}$ of 1.5 and $\gamma$ of 0.15, the RE starting points were set to 1.2 for $R_{0}$ and 0.10 for $\gamma$. For the outbreaks with underlying $R_{0}$ of 6 and $\gamma$ of 0.15, the RE starting points were set to 7.2 for $R_{0}$ and 0.10 for $\gamma$. For each of the 30 outbreaks, I calculated a point estimate and completed 1000 repetitions for bootstrapping. 
% Use R command citation() to figure out how to cite R packages e.g. citation(package = "deSolve")

%%%%%%%%%%
\newpage
\subsubsection{Markov Chain Monte Carlo}

I used a Metropolis-Hastings algorithm to infer the values of $\beta$ and $\gamma$ from an incomplete version of the data simulated with the stochastic SIR model I created. 

I calculated the likelihood distribution as follows: 
\begin{align*}
\displaystyle\prod_{i=1}^{N}p(b_{i}, c_{i} \mid \beta, \gamma)
\end{align*}

\noindent where
\begin{align*}
p(b_{i}, c_{i} \mid \beta, \gamma) = Bin(b_{i} \mid S_{i}, 1-e^{-\beta I_{i} dt}) \times Bin(c_{i} \mid I_{i}, 1-e^{-\gamma dt})
\end{align*}

Here $b_{i}$ and $c_{i}$ refer to the newly infected and recovered individuals and $S_{i}$ and $I_{i}$ refer to all susceptible and infectious individuals respectively at timepoint $i$. $N$ is the total population size. $\beta$ and $\gamma$ refer to the per capita rate of infection and recovery rate, while $dt$ refers to the size of the timestep. 

I set the prior distributions for $\beta$ and $\gamma$ to be uniform between the values of 0 and 100, implying that there was no original guess at the value that $\beta$ and $\gamma$ should take. I then calculated the posterior distribution as $likelihood \times prior$. When assembling the MCMC algorithm, the equations above were written in logarithmic form to avoid floating point inaccuracies.

Calculating the likelihood using the equation above requires knowledge of the number of infectious individuals at a given time, $I_{i}$. However, I considered that only the numbers of removed individuals are observed. I used a technique called data augmentation accommodate for this so that unknown data, such as $I_{i}$, is treated as an additional parameter \citep{Dyk,ONeill2010}. I adapted the MCMC to perform data augmentation on the infectious curve, meaning that it was to guess the number of infectious individuals at each timestep $i$. I obtained an initial guess of the infectious curve by guessing an initial $\gamma$ value, assuming a constant infectious period, and thus back-calculating the infectious curve from the known removed curve from the simulated data. This original infectious curve was then changed by proposing the addition or subtraction of one infectious person from a given timepoint $i$ and moving them to a randomly chosen neighbouring timepoint at every iteration of the MCMC.
\newpage
Whether or not a proposed change in $\beta$, $\gamma$, or the infectious curve was accepted was calculated with the Metropolis-Hastings ratio:
\begin{align*}
min \Big(1, \frac{p(\beta', \gamma' \mid b, c)}{p(\beta, \gamma \mid b, c)} \frac{q(\beta, \gamma \mid \beta', \gamma')}{q(\beta', \gamma' \mid \beta, \gamma)}\Big)
\end{align*}
where $p$ refers to the posterior distribution and $q$ refers to the proposal distribution. $b$ and $c$ refer to the data on newly recovered and newly infected individuals. $\beta$ and $\gamma$ refer to the current value assigned to the per capita rate of infection and recovery rate respectively, while $\beta'$ and $\gamma'$ refer to the proposed rates.  If the ratio is more than 1, the proposed change is accepted while if the ratio is below 1, the proposed change is rejected.

My proposal distributions for $\beta$ and $\gamma$ were Normally distributed with a mean of the current baseline parameter values and a standard deviation of a quarter of the initial $\beta$ and $\gamma$ estimates. Therefore the Metropolis-Hastings ratio above is reduced to only the posteriors due to the symmetric proposal density, and so $q(\beta' \gamma' \mid \beta, \gamma) = q(\beta, \gamma \mid \beta', \gamma')$. Therefore, the proposed change is accepted if the ratio of the proposed posterior to the existing posterior is higher than a randomly selected number from a uniform distribution between 0 and 1. This means that the move is always accepted if the proposed posterior is greater than the existing posterior, and that less advantageous moves are also occasionally accepted to allow the exploration of uncertainty in the parameter values. 

For the simulated outbreaks with an underlying $R_{0}$ of 1.5 and $\gamma$ of 0.15, the MCMC starting points were set to 1.2 for $R_{0}$ and 0.10 for $\gamma$. For the outbreaks with underlying $R_{0}$ of 6 and $\gamma$ of 0.15, the MCMC starting points were set to 7.2 for $R_{0}$ and 0.10 for $\gamma$. Each MCMC was run for 3.5 million iterations. I ran an additional 17.5 million iteration MCMC for the outbreaks with a population size of 1000. The burn-in period was set to 23\% of the iteration length for all MCMCs.

%%%%%%%%%%
\newpage
\subsubsection{Markov Chain Monte Carlo with a partly deterministic process}

To create a MCMC with a partly deterministic process, I altered the standard MCMC I had created in the previous section to only have two unknown parameters, $\beta$ and $\gamma$. I then changed the likelihood function so that it created a deterministic SIR model for every set of $\beta$ and $\gamma$ proposed in the MCMC. I used the \textit{deSolve} package to construct the deterministic model \citep{Soetaert2010}. I rounded the number of infectious individuals from the output of the deterministic model to the nearest integer for each timestep to avoid partial individuals from being infected. The susceptible curve was devised from the known removed curve from the simulated dataset, $R_{i}$, and the deterministic infectious curve, $I^{d}_{i}$, so that
\begin{align*}
S_{i} &= N - I^{d}_{i} - R_{i}
\end{align*}
where $N$ represents the total population size.

Similarly, the number of newly infected individuals, $b$, for each time step $i$ was also affected by the deterministic infectious curve, $I^{d}$, and the known simulated removed curve:
\begin{align*}
b_{i+1} &= I_{i+1}^{d} - I_{i}^{d} + R_{i+1} - R_{i}
\end{align*}

The values for $\beta$ and $\gamma$ for the first simulation for each of the six outbreak severity and population size combinations were inferred. The starting points for the MCMC with a partly deterministic process were the same as those for the standard MCMC: for an underlying $R_{0}$ of 1.5 and $\gamma$ of 0.15, the starting points were 1.2 and 0.10 for $R_{0}$ and $\gamma$ respectively. For outbreaks with an $R_{0}$ of 6 and $\gamma$ of 0.15, the starting points were 7.2 and 0.10 for $R_{0}$ and $\gamma$ respectively. Each MCMC with a partly deterministic process was run for 2.5 million iterations. The burn-in period was set to 23\% of the iteration length.

%%%%%%%%%%

%%%%%%%%%%%%%
%% Results %%
%%%%%%%%%%%%%

\newpage
\section{Results}
% 10-15 pages

%%%%%%%%%%

\subsection{The simulated outbreaks}

Six different outbreak scenarios were created: an outbreak with an $R_{0}$ of 1.5 and 6 for population sizes of 50, 200, and 1000 individuals. The initial proportion of infectious individuals at the beginning of each outbreak was the same for each population size. Five different outbreaks were simulated for each of the six scenarios. Figures 2 and 3 show the shapes of each simulated outbreak.
\newline

\begin{figure}[h]
\begin{center}
<<dataplot1, cache=FALSE, echo=FALSE, eval=TRUE, dpi=100, fig.pos="H", height=13, width=17, fig.lp = "Figure", fig.cap="Data plot", fig=TRUE>>=
# R0 = 1.5

# setwd("/home/evelina/Development/stochastic_vs_deterministic/")
true_R1.5_pop50_seed1 <- read.csv("data_pop50_R1.5_g0.15_14.csv")
true_R1.5_pop50_seed2 <- read.csv("data_pop50_R1.5_g0.15_22.csv")
true_R1.5_pop50_seed3 <- read.csv("data_pop50_R1.5_g0.15_40.csv")
true_R1.5_pop50_seed4 <- read.csv("data_pop50_R1.5_g0.15_43.csv")
true_R1.5_pop50_seed5 <- read.csv("data_pop50_R1.5_g0.15_95.csv")

true_R1.5_pop200_seed1 <- read.csv("data_pop200_R1.5_g0.15_81.csv")
true_R1.5_pop200_seed2 <- read.csv("data_pop200_R1.5_g0.15_93.csv")
true_R1.5_pop200_seed3 <- read.csv("data_pop200_R1.5_g0.15_102.csv")
true_R1.5_pop200_seed4 <- read.csv("data_pop200_R1.5_g0.15_202.csv")
true_R1.5_pop200_seed5 <- read.csv("data_pop200_R1.5_g0.15_246.csv")

true_R1.5_pop1000_seed1 <- read.csv("data_pop1000_R1.5_g0.15_177.csv")
true_R1.5_pop1000_seed2 <- read.csv("data_pop1000_R1.5_g0.15_246.csv")
true_R1.5_pop1000_seed3 <- read.csv("data_pop1000_R1.5_g0.15_469.csv")
true_R1.5_pop1000_seed4 <- read.csv("data_pop1000_R1.5_g0.15_520.csv")
true_R1.5_pop1000_seed5 <- read.csv("data_pop1000_R1.5_g0.15_784.csv")

letter_place = 72 
letter_size = 3
axis_size = 2.5
yaxis_size = 2
number_size = 2

# Plot for SIR model - R0 = 1.5
# plot.new()
par(mfrow = c(3,5), mar=c(5,5,0.2,0.2))
# Pop = 50
N = 50
# Seed 1
plot(x = true_R1.5_pop50_seed1$time, y = true_R1.5_pop50_seed1$I, type = "l", col = "red", ylim = c(0,N), xlab = "", ylab = "", cex.axis = number_size)
lines(true_R1.5_pop50_seed1$S, type = "l", ylim = c(0,N)) # susceptible
lines(true_R1.5_pop50_seed1$R, type = "l", col = "orange", ylim = c(0,N)) # removed
text(letter_place, 0.95*N, "A", cex = letter_size)
# Seed 2
plot(x = true_R1.5_pop50_seed2$time, y = true_R1.5_pop50_seed2$I, type = "l", col = "red", ylim = c(0,N), xlab = "", ylab = "", cex.axis = number_size)
lines(true_R1.5_pop50_seed2$S, type = "l", ylim = c(0,N)) # susceptible
lines(true_R1.5_pop50_seed2$R, type = "l", col = "orange", ylim = c(0,N)) # removed
text(letter_place, 0.95*N, "B", cex = letter_size)
# Seed 3
plot(x = true_R1.5_pop50_seed3$time, y = true_R1.5_pop50_seed3$I, type = "l", col = "red", ylim = c(0,N), xlab = "", ylab = "", cex.axis = number_size)
lines(true_R1.5_pop50_seed3$S, type = "l", ylim = c(0,N)) # susceptible
lines(true_R1.5_pop50_seed3$R, type = "l", col = "orange", ylim = c(0,N)) # removed
text(letter_place, 0.95*N, "C", cex = letter_size)
# Seed 4
plot(x = true_R1.5_pop50_seed4$time, y = true_R1.5_pop50_seed4$I, type = "l", col = "red", ylim = c(0,N), xlab = "", ylab = "", cex.axis = number_size)
lines(true_R1.5_pop50_seed4$S, type = "l", ylim = c(0,N)) # susceptible
lines(true_R1.5_pop50_seed4$R, type = "l", col = "orange", ylim = c(0,N)) # removed
text(letter_place, 0.95*N, "D", cex = letter_size)
# Seed 5
plot(x = true_R1.5_pop50_seed5$time, y = true_R1.5_pop50_seed5$I, type = "l", col = "red", ylim = c(0,N), xlab = "", ylab = "", cex.axis = number_size)
lines(true_R1.5_pop50_seed5$S, type = "l", ylim = c(0,N)) # susceptible
lines(true_R1.5_pop50_seed5$R, type = "l", col = "orange", ylim = c(0,N)) # removed
text(letter_place, 0.95*N, "E", cex = letter_size)
# Pop = 200
N = 200
# Seed 1
plot(x = true_R1.5_pop200_seed1$time, y = true_R1.5_pop200_seed1$I, type = "l", col = "red", ylim = c(0,N), xlab = "", ylab = "", cex.axis = number_size)
lines(true_R1.5_pop200_seed1$S, type = "l", ylim = c(0,N)) # susceptible
lines(true_R1.5_pop200_seed1$R, type = "l", col = "orange", ylim = c(0,N)) # removed
text(letter_place, 0.95*N, "F", cex = letter_size)
mtext("Number susceptible/infected/removed", 2, line = 3, cex = yaxis_size, outer = FALSE)
# Seed 2
plot(x = true_R1.5_pop200_seed2$time, y = true_R1.5_pop200_seed2$I, type = "l", col = "red", ylim = c(0,N), xlab = "", ylab = "", cex.axis = number_size)
lines(true_R1.5_pop200_seed2$S, type = "l", ylim = c(0,N)) # susceptible
lines(true_R1.5_pop200_seed2$R, type = "l", col = "orange", ylim = c(0,N)) # removed
text(letter_place, 0.95*N, "G", cex = letter_size)
# Seed 3
plot(x = true_R1.5_pop200_seed3$time, y = true_R1.5_pop200_seed3$I, type = "l", col = "red", ylim = c(0,N), xlab = "", ylab = "", cex.axis = number_size)
lines(true_R1.5_pop200_seed3$S, type = "l", ylim = c(0,N)) # susceptible
lines(true_R1.5_pop200_seed3$R, type = "l", col = "orange", ylim = c(0,N)) # removed
text(letter_place, 0.95*N, "H", cex = letter_size)
# Seed 4
plot(x = true_R1.5_pop200_seed4$time, y = true_R1.5_pop200_seed4$I, type = "l", col = "red", ylim = c(0,N), xlab = "", ylab = "", cex.axis = number_size)
lines(true_R1.5_pop200_seed4$S, type = "l", ylim = c(0,N)) # susceptible
lines(true_R1.5_pop200_seed4$R, type = "l", col = "orange", ylim = c(0,N)) # removed
text(letter_place, 0.95*N, "I", cex = letter_size)
# Seed 5
plot(x = true_R1.5_pop200_seed5$time, y = true_R1.5_pop200_seed5$I, type = "l", col = "red", ylim = c(0,N), xlab = "", ylab = "", cex.axis = number_size)
lines(true_R1.5_pop200_seed5$S, type = "l", ylim = c(0,N)) # susceptible
lines(true_R1.5_pop200_seed5$R, type = "l", col = "orange", ylim = c(0,N)) # removed
text(letter_place, 0.95*N, "J", cex = letter_size)
# Pop = 1000
N = 1000
# Seed 1
plot(x = true_R1.5_pop1000_seed1$time, y = true_R1.5_pop1000_seed1$I, type = "l", col = "red", ylim = c(0,N), xlab = "", ylab = "", cex.axis = number_size)
lines(true_R1.5_pop1000_seed1$S, type = "l", ylim = c(0,N)) # susceptible
lines(true_R1.5_pop1000_seed1$R, type = "l", col = "orange", ylim = c(0,N)) # removed
text(letter_place, 0.95*N, "K", cex = letter_size)
mtext("Time", 1, line = 3, cex = yaxis_size)
# Seed 2
plot(x = true_R1.5_pop1000_seed2$time, y = true_R1.5_pop1000_seed2$I, type = "l", col = "red", ylim = c(0,N), xlab = "", ylab = "", cex.axis = number_size)
lines(true_R1.5_pop1000_seed2$S, type = "l", ylim = c(0,N)) # susceptible
lines(true_R1.5_pop1000_seed2$R, type = "l", col = "orange", ylim = c(0,N)) # removed
text(letter_place, 0.95*N, "L", cex = letter_size)
mtext("Time", 1, line = 3, cex = yaxis_size)
# Seed 3
plot(x = true_R1.5_pop1000_seed3$time, y = true_R1.5_pop1000_seed3$I, type = "l", col = "red", ylim = c(0,N), xlab = "", ylab = "", cex.axis = number_size)
lines(true_R1.5_pop1000_seed3$S, type = "l", ylim = c(0,N)) # susceptible
lines(true_R1.5_pop1000_seed3$R, type = "l", col = "orange", ylim = c(0,N)) # removed
text(letter_place, 0.95*N, "M", cex = letter_size)
mtext("Time", 1, line = 3, cex = yaxis_size)
# Seed 4
plot(x = true_R1.5_pop1000_seed4$time, y = true_R1.5_pop1000_seed4$I, type = "l", col = "red", ylim = c(0,N), xlab = "", ylab = "", cex.axis = number_size)
lines(true_R1.5_pop1000_seed4$S, type = "l", ylim = c(0,N)) # susceptible
lines(true_R1.5_pop1000_seed4$R, type = "l", col = "orange", ylim = c(0,N)) # removed
text(letter_place, 0.95*N, "N", cex = letter_size)
mtext("Time", 1, line = 3, cex = yaxis_size)
# Seed 5
plot(x = true_R1.5_pop1000_seed5$time, y = true_R1.5_pop1000_seed5$I, type = "l", col = "red", ylim = c(0,N), xlab = "", ylab = "", cex.axis = number_size)
lines(true_R1.5_pop1000_seed5$S, type = "l", ylim = c(0,N)) # susceptible
lines(true_R1.5_pop1000_seed5$R, type = "l", col = "orange", ylim = c(0,N)) # removed
text(letter_place, 0.95*N, "O", cex = letter_size)
mtext("Time", 1, line = 3, cex = yaxis_size)
@
\caption{Panels showing how the number of susceptible (black), infectious (red), and removed (orange) individuals changes with time during an outbreak simulated with a stochastic SIR model with an $R_{0}$ of 1.5 and $\gamma$, recovery rate, of 0.15. Panels A-E depict a population size of 50, panels F-J depict a population size of 200, and panels K-O depict a population size of 1000.}
\label{outbreak15}
\end{center}
\end{figure}

The shapes of all the outbreaks with an $R_{0}$ of 1.5 are similar, with most outbreaks reaching their peak number of infected individuals at around 20 days (Fig. \ref{outbreak15}). Outbreaks with an $R_{0}$ of 6 had a higher peak in the infectious curve than outbreaks with an $R_{0}$ of 1.5 though the peak is also at around 20 days (Fig. \ref{outbreak6}). The variability in outbreak shape reduces with increasing population size for both outbreak severity.
\newline

\begin{figure}[h]
\begin{center}
<<dataplot2, cache=FALSE, echo=FALSE, eval=TRUE, dpi=100, fig.pos="H", height=13, width=17, fig.height=4, fig.lp = "Figure", fig.cap="Data plot", fig=TRUE>>=
# R0 = 6

true_R6_pop50_seed2 <- read.csv("data_pop50_R6_g0.15_1.csv")
true_R6_pop50_seed1 <- read.csv("data_pop50_R6_g0.15_2.csv")
true_R6_pop50_seed3 <- read.csv("data_pop50_R6_g0.15_3.csv")
true_R6_pop50_seed4 <- read.csv("data_pop50_R6_g0.15_4.csv")
true_R6_pop50_seed5 <- read.csv("data_pop50_R6_g0.15_5.csv")

true_R6_pop200_seed1 <- read.csv("data_pop200_R6_g0.15_6.csv")
true_R6_pop200_seed2 <- read.csv("data_pop200_R6_g0.15_7.csv")
true_R6_pop200_seed3 <- read.csv("data_pop200_R6_g0.15_8.csv")
true_R6_pop200_seed4 <- read.csv("data_pop200_R6_g0.15_9.csv")
true_R6_pop200_seed5 <- read.csv("data_pop200_R6_g0.15_10.csv")

true_R6_pop1000_seed1 <- read.csv("data_pop1000_R6_g0.15_11.csv")
true_R6_pop1000_seed2 <- read.csv("data_pop1000_R6_g0.15_12.csv")
true_R6_pop1000_seed3 <- read.csv("data_pop1000_R6_g0.15_13.csv")
true_R6_pop1000_seed4 <- read.csv("data_pop1000_R6_g0.15_14.csv")
true_R6_pop1000_seed5 <- read.csv("data_pop1000_R6_g0.15_15.csv")

letter_place = 72 
letter_size = 3
axis_size = 2.5
yaxis_size = 2
number_size = 2

# Plot for SIR model - R0 = 6
# plot.new()
par(mfrow = c(3,5), mar=c(5,5,0.2,0.2))
# Pop = 50
N = 50
# Seed 1
plot(x = true_R6_pop50_seed1$time, y = true_R6_pop50_seed1$I, type = "l", col = "red", ylim = c(0,N), xlab = "", ylab = "", cex.axis = number_size)
lines(true_R6_pop50_seed1$S, type = "l", ylim = c(0,N)) # susceptible
lines(true_R6_pop50_seed1$R, type = "l", col = "orange", ylim = c(0,N)) # removed
text(letter_place, 0.95*N, "A", cex = letter_size)
# Seed 2
plot(x = true_R6_pop50_seed2$time, y = true_R6_pop50_seed2$I, type = "l", col = "red", ylim = c(0,N), xlab = "", ylab = "", cex.axis = number_size)
lines(true_R6_pop50_seed2$S, type = "l", ylim = c(0,N)) # susceptible
lines(true_R6_pop50_seed2$R, type = "l", col = "orange", ylim = c(0,N)) # removed
text(letter_place, 0.95*N, "B", cex = letter_size)
# Seed 3
plot(x = true_R6_pop50_seed3$time, y = true_R6_pop50_seed3$I, type = "l", col = "red", ylim = c(0,N), xlab = "", ylab = "", cex.axis = number_size)
lines(true_R6_pop50_seed3$S, type = "l", ylim = c(0,N)) # susceptible
lines(true_R6_pop50_seed3$R, type = "l", col = "orange", ylim = c(0,N)) # removed
text(letter_place, 0.95*N, "C", cex = letter_size)
# Seed 4
plot(x = true_R6_pop50_seed4$time, y = true_R6_pop50_seed4$I, type = "l", col = "red", ylim = c(0,N), xlab = "", ylab = "", cex.axis = number_size)
lines(true_R6_pop50_seed4$S, type = "l", ylim = c(0,N)) # susceptible
lines(true_R6_pop50_seed4$R, type = "l", col = "orange", ylim = c(0,N)) # removed
text(letter_place, 0.95*N, "D", cex = letter_size)
# Seed 5
plot(x = true_R6_pop50_seed5$time, y = true_R6_pop50_seed5$I, type = "l", col = "red", ylim = c(0,N), xlab = "", ylab = "", cex.axis = number_size)
lines(true_R6_pop50_seed5$S, type = "l", ylim = c(0,N)) # susceptible
lines(true_R6_pop50_seed5$R, type = "l", col = "orange", ylim = c(0,N)) # removed
text(letter_place, 0.95*N, "E", cex = letter_size)
# Pop = 200
N = 200
# Seed 1
plot(x = true_R6_pop200_seed1$time, y = true_R6_pop200_seed1$I, type = "l", col = "red", ylim = c(0,N), xlab = "", ylab = "", cex.axis = number_size)
lines(true_R6_pop200_seed1$S, type = "l", ylim = c(0,N)) # susceptible
lines(true_R6_pop200_seed1$R, type = "l", col = "orange", ylim = c(0,N)) # removed
text(letter_place, 0.95*N, "F", cex = letter_size)
mtext("Number susceptible/infected/removed", 2, line = 3, cex = yaxis_size, outer = FALSE)
# Seed 2
plot(x = true_R6_pop200_seed2$time, y = true_R6_pop200_seed2$I, type = "l", col = "red", ylim = c(0,N), xlab = "", ylab = "", cex.axis = number_size)
lines(true_R6_pop200_seed2$S, type = "l", ylim = c(0,N)) # susceptible
lines(true_R6_pop200_seed2$R, type = "l", col = "orange", ylim = c(0,N)) # removed
text(letter_place, 0.95*N, "G", cex = letter_size)
# Seed 3
plot(x = true_R6_pop200_seed3$time, y = true_R6_pop200_seed3$I, type = "l", col = "red", ylim = c(0,N), xlab = "", ylab = "", cex.axis = number_size)
lines(true_R6_pop200_seed3$S, type = "l", ylim = c(0,N)) # susceptible
lines(true_R6_pop200_seed3$R, type = "l", col = "orange", ylim = c(0,N)) # removed
text(letter_place, 0.95*N, "H", cex = letter_size)
# Seed 4
plot(x = true_R6_pop200_seed4$time, y = true_R6_pop200_seed4$I, type = "l", col = "red", ylim = c(0,N), xlab = "", ylab = "", cex.axis = number_size)
lines(true_R6_pop200_seed4$S, type = "l", ylim = c(0,N)) # susceptible
lines(true_R6_pop200_seed4$R, type = "l", col = "orange", ylim = c(0,N)) # removed
text(letter_place, 0.95*N, "I", cex = letter_size)
# Seed 5
plot(x = true_R6_pop200_seed5$time, y = true_R6_pop200_seed5$I, type = "l", col = "red", ylim = c(0,N), xlab = "", ylab = "", cex.axis = number_size)
lines(true_R6_pop200_seed5$S, type = "l", ylim = c(0,N)) # susceptible
lines(true_R6_pop200_seed5$R, type = "l", col = "orange", ylim = c(0,N)) # removed
text(letter_place, 0.95*N, "J", cex = letter_size)
# Pop = 1000
N = 1000
# Seed 1
plot(x = true_R6_pop1000_seed1$time, y = true_R6_pop1000_seed1$I, type = "l", col = "red", ylim = c(0,N), xlab = "", ylab = "", cex.axis = number_size)
lines(true_R6_pop1000_seed1$S, type = "l", ylim = c(0,N)) # susceptible
lines(true_R6_pop1000_seed1$R, type = "l", col = "orange", ylim = c(0,N)) # removed
text(letter_place, 0.95*N, "K", cex = letter_size)
mtext("Time", 1, line = 3, cex = yaxis_size)
# Seed 2
plot(x = true_R6_pop1000_seed2$time, y = true_R6_pop1000_seed2$I, type = "l", col = "red", ylim = c(0,N), xlab = "", ylab = "", cex.axis = number_size)
lines(true_R6_pop1000_seed2$S, type = "l", ylim = c(0,N)) # susceptible
lines(true_R6_pop1000_seed2$R, type = "l", col = "orange", ylim = c(0,N)) # removed
text(letter_place, 0.95*N, "L", cex = letter_size)
mtext("Time", 1, line = 3, cex = yaxis_size)
# Seed 3
plot(x = true_R6_pop1000_seed3$time, y = true_R6_pop1000_seed3$I, type = "l", col = "red", ylim = c(0,N), xlab = "", ylab = "", cex.axis = number_size)
lines(true_R6_pop1000_seed3$S, type = "l", ylim = c(0,N)) # susceptible
lines(true_R6_pop1000_seed3$R, type = "l", col = "orange", ylim = c(0,N)) # removed
text(letter_place, 0.95*N, "M", cex = letter_size)
mtext("Time", 1, line = 3, cex = yaxis_size)
# Seed 4
plot(x = true_R6_pop1000_seed4$time, y = true_R6_pop1000_seed4$I, type = "l", col = "red", ylim = c(0,N), xlab = "", ylab = "", cex.axis = number_size)
lines(true_R6_pop1000_seed4$S, type = "l", ylim = c(0,N)) # susceptible
lines(true_R6_pop1000_seed4$R, type = "l", col = "orange", ylim = c(0,N)) # removed
text(letter_place, 0.95*N, "N", cex = letter_size)
mtext("Time", 1, line = 3, cex = yaxis_size)
# Seed 5
plot(x = true_R6_pop1000_seed5$time, y = true_R6_pop1000_seed5$I, type = "l", col = "red", ylim = c(0,N), xlab = "", ylab = "", cex.axis = number_size)
lines(true_R6_pop1000_seed5$S, type = "l", ylim = c(0,N)) # susceptible
lines(true_R6_pop1000_seed5$R, type = "l", col = "orange", ylim = c(0,N)) # removed
text(letter_place, 0.95*N, "O", cex = letter_size)
mtext("Time", 1, line = 3, cex = yaxis_size)
@
\caption{Panels showing how the number of susceptible (black), infectious (red), and removed (orange) individuals changes with time during an outbreak simulated with a stochastic SIR model with an $R_{0}$ of 6 and $\gamma$, recovery rate, of 0.15. Panels A-E depict a population size of 50, panels F-J depict a population size of 200, and panels K-O depict a population size of 1000.}
\label{outbreak6}
\end{center}
\end{figure}

Not all simulations resulted in a continued outbreak for the lower $R_{0}$, but went extinct early on. For example, over half of the the simulated outbreaks for population size of 50 and $R_{0}$ of 1.5 resulted in fewer than 10\% of the population (5 individuals) becoming infected (Fig. \ref{outbreakhist}). These non-starters were removed from further analysis as the values of $\beta$ and $\gamma$ would not be interesting to infer from outbreaks that went extinct very early on. Stochastic extinction early on during an outbreak became less common with increasing population size (Fig. \ref{outbreakhist}).

\begin{figure}[!h]
\begin{center}
<<outbreakhistogram, cache=TRUE, echo=FALSE, eval=TRUE, dpi=100, height=8, width=6, fig.cap="Data plot", fig=TRUE>>=
par(mfrow = c(3,1), mar=c(5,6.5,0.4,0.2))
dir <- "/home/evelina/OneDrive/MRes_BMR/Project_1/Work_folder/Data/"

letter_size = 1.5

outbreak_size <- read.csv(paste(dir, "pop50_R1.5_outbreaksize.csv", sep=""))
outbreak_size <- unlist(outbreak_size)
hist(outbreak_size, nclass=30, col = "gray", main="", xlab = "Outbreak size", xlim = c(0, 50), ylim = c(0, 500))
box()
text(0.97*50, 0.92*500, "A", cex = letter_size)


outbreak_size <- read.csv(paste(dir, "pop200_R1.5_outbreaksize.csv", sep=""))
outbreak_size <- unlist(outbreak_size)

hist(outbreak_size, nclass=30, col = "gray", main="", xlab = "Outbreak size", xlim = c(0, 200), ylim = c(0, 80))
box()
text(0.97*200, 0.92*80, "B", cex = letter_size)

outbreak_size <- read.csv(paste(dir, "pop1000_R1.5_outbreaksize.csv", sep=""))
outbreak_size <- unlist(outbreak_size)

hist(outbreak_size, nclass=30, col = "gray", main="", xlab = "Outbreak size", xlim = c(0, 1000), ylim = c(0, 170))
box()
text(0.97*1000, 0.92*170, "C", cex = letter_size)
@
\caption{Histograms showing the distribution of total outbreak sizes for 1000 outbreak simulations with an $R_{0}$ of 1.5, $\gamma$ (recovery rate) of 0.15, and a population size of 50 (A), 200 (B), and 1000 (C).}
\label{outbreakhist}
\end{center}
\end{figure}

\FloatBarrier
%%%%%%%%%%
\clearpage
\newpage
\subsection{Residual error}
The RE inference method was used to infer the values that $\beta$ and $\gamma$ might take when given the the recovery curves from the 30 simulated outbreaks. Calculating the point estimate and bootstrapping 1000 times took approximately 20 minutes for each outbreak scenario.
\newline

\begin{figure}[!h]
\begin{center}
<<dataplot3, cache=TRUE, echo=FALSE, eval=TRUE, dpi=100, height=13, width=17, fig.cap="Data plot", fig=TRUE>>=
# RE example plots
library("deSolve")

true_R1.5_pop50_seed1 <- read.csv("data_pop50_R1.5_g0.15_14.csv")
true_R1.5_pop200_seed1 <- read.csv("data_pop200_R1.5_g0.15_81.csv")
true_R1.5_pop1000_seed1 <- read.csv("data_pop1000_R1.5_g0.15_177.csv")
true_R6_pop50_seed1 <- read.csv("data_pop50_R6_g0.15_2.csv")
true_R6_pop200_seed1 <- read.csv("data_pop200_R6_g0.15_6.csv")
true_R6_pop1000_seed1 <- read.csv("data_pop1000_R6_g0.15_11.csv")

dir <- "/home/evelina/OneDrive/MRes_BMR/Project_1/Work_folder/Data/server_data/"
re_R1.5_pop50_seed1 <- read.csv(paste(dir, "re_pop50_R1.5_g0.15_14.csv", sep=""))
re_R1.5_pop200_seed1 <- read.csv(paste(dir,"re_pop200_R1.5_g0.15_81.csv", sep=""))
re_R1.5_pop1000_seed1 <- read.csv(paste(dir,"re_pop1000_R1.5_g0.15_177.csv", sep=""))
re_R6_pop50_seed1 <- read.csv(paste(dir,"re_pop50_R6_g0.15_2.csv", sep=""))
re_R6_pop200_seed1 <- read.csv(paste(dir,"re_pop200_R6_g0.15_6.csv", sep=""))
re_R6_pop1000_seed1 <- read.csv(paste(dir,"re_pop1000_R6_g0.15_11.csv", sep=""))

sir <- function(time, state, param) {
  
  # define model parameters in term of the natural parameters
  beta <- param[1] 
  gamma <- param[2]
  
  with(as.list(c(state, param)), {
    
    dS <- -(beta * S * I) 
    dI <- (beta * S * I) -(gamma * I)
    dR <-  gamma * I
    
    return(list(c(dS, dI, dR)))
  })
}

pre_sir <- function(run_stoch, sse_data){
# Time
timestep <- run_stoch$time[2] - run_stoch$time[1]
end <- max(run_stoch$time)
times <- seq(0, end, by = timestep)

# Initial population: N-1 susceptible, 1 infectious, 0 removed
init.values = c(
  S = run_stoch$S[1],
  I = run_stoch$I[1],
  R = run_stoch$R[1]
)
N = sum(init.values)

det_sir <- as.data.frame(ode(y = init.values, times = times, func = sir, parms = sse_data[1:2]))
}

letter_size = 3
axis_size = 2.5
yaxis_size = 2
number_size = 2.5

par(mfrow = c(2, 3), mar=c(5,6.5,0.4,0.2))
# R = 1.5, Pop = 50
N = 50
plot(true_R1.5_pop50_seed1$I, ylim = c(0, N), type = "l", col = "white", xlab = "", ylab = "", cex.axis = number_size)
  lines(true_R1.5_pop50_seed1$I, ylim = c(0, N), type = "l", col = "red", lwd = 3)
  lines(true_R1.5_pop50_seed1$R, ylim = c(0, N), type = "l", col = "orange", lwd = 3)
for (i in 2:nrow(re_R1.5_pop50_seed1)){
  sir_data <- pre_sir(true_R1.5_pop50_seed1, re_R1.5_pop50_seed1[i,])
  lines(sir_data$I, type = "l", lty = 2, col = "gray60", xlab = " ", ylab = " ", lwd = 0.5)
  lines(sir_data$R, type = "l", lty = 3, col = "gray85", xlab = " ", ylab = " ", lwd = 0.5)
}
for (i in 1){
  sir_data <- pre_sir(true_R1.5_pop50_seed1, re_R1.5_pop50_seed1[i,])
  lines(sir_data$I, type = "l", lty = 2, col = "black", xlab = " ", ylab = " ", lwd = 2)
  lines(sir_data$R, type = "l", lty = 3, col = "black", xlab = " ", ylab = " ", lwd = 2)
}
text(10, 0.95*N, "A", cex = letter_size)
mtext("Number infected/removed", 2, line = 4, cex = yaxis_size, outer = FALSE)
# legend(100, 0.8*N, c("True infected", "Deterministic infected"), pch = 1, col = c("red", "black"), bty = "n")
# R = 1.5, Pop = 200
N = 200
plot(true_R1.5_pop200_seed1$I, ylim = c(0, N), type = "l", col = "white", xlab = "", ylab = "", cex.axis = number_size)
lines(true_R1.5_pop200_seed1$I, ylim = c(0, N), type = "l", col = "red", lwd = 3)
lines(true_R1.5_pop200_seed1$R, ylim = c(0, N), type = "l", col = "orange", lwd = 3)
for (i in 2:nrow(re_R1.5_pop200_seed1)){
  sir_data <- pre_sir(true_R1.5_pop200_seed1, re_R1.5_pop200_seed1[i,])
  lines(sir_data$I, type = "l", lty = 2, col = "gray50", xlab = " ", ylab = " ", lwd = 0.5)
  lines(sir_data$R, type = "l", lty = 3, col = "gray80", xlab = " ", ylab = " ", lwd = 0.5)
}
for (i in 1){
  sir_data <- pre_sir(true_R1.5_pop200_seed1, re_R1.5_pop200_seed1[i,])
  lines(sir_data$I, type = "l", lty = 2, col = "black", xlab = " ", ylab = " ", lwd = 2)
  lines(sir_data$R, type = "l", lty = 3, col = "black", xlab = " ", ylab = " ", lwd = 2)
}
text(10, 0.95*N, "B", cex = letter_size)
# legend(100, 0.8*N, c("True infected", "Deterministic infected"), pch = 1, col = c("red", "black"), bty = "n")
# R = 1.5, Pop = 1000
N = 1000
plot(true_R1.5_pop1000_seed1$I, ylim = c(0, N), type = "l", col = "white", xlab = "", ylab = "", cex.axis = number_size)
lines(true_R1.5_pop1000_seed1$I, ylim = c(0, N), type = "l", col = "red", lwd = 3)
lines(true_R1.5_pop1000_seed1$R, ylim = c(0, N), type = "l", col = "orange", lwd = 3)
for (i in 2:nrow(re_R1.5_pop1000_seed1)){
  sir_data <- pre_sir(true_R1.5_pop1000_seed1, re_R1.5_pop1000_seed1[i,])
  lines(sir_data$I, type = "l", lty = 2, col = "gray50", xlab = " ", ylab = " ", lwd = 0.5)
  lines(sir_data$R, type = "l", lty = 3, col = "gray80", xlab = " ", ylab = " ", lwd = 0.5)
}
for (i in 1){
  sir_data <- pre_sir(true_R1.5_pop1000_seed1, re_R1.5_pop1000_seed1[i,])
  lines(sir_data$I, type = "l", lty = 2, col = "black", xlab = " ", ylab = " ", lwd = 2)
  lines(sir_data$R, type = "l", lty = 3, col = "black", xlab = " ", ylab = " ", lwd = 2)
}
text(10, 0.95*N, "C", cex = letter_size)
# legend(100, 0.8*N, c("True infected", "Deterministic infected"), pch = 1, col = c("red", "black"), bty = "n")
# R = 6, Pop = 50
N = 50
plot(true_R6_pop50_seed1$I, ylim = c(0, N), type = "l", col = "white", xlab = "", ylab = "", cex.axis = number_size)
lines(true_R6_pop50_seed1$I, ylim = c(0, N), type = "l", col = "red", lwd = 3)
lines(true_R6_pop50_seed1$R, ylim = c(0, N), type = "l", col = "orange", lwd = 3)
for (i in 2:nrow(re_R1.5_pop50_seed1)){
  sir_data <- pre_sir(true_R6_pop50_seed1, re_R6_pop50_seed1[i,])
  lines(sir_data$I, type = "l", lty = 2, col = "gray50", xlab = " ", ylab = " ", lwd = 0.5)
  lines(sir_data$R, type = "l", lty = 3, col = "gray80", xlab = " ", ylab = " ", lwd = 0.5)
}
for (i in 1){
  sir_data <- pre_sir(true_R6_pop50_seed1, re_R6_pop50_seed1[i,])
  lines(sir_data$I, type = "l", lty = 2, col = "black", xlab = " ", ylab = " ", lwd = 2)
  lines(sir_data$R, type = "l", lty = 3, col = "black", xlab = " ", ylab = " ", lwd = 2)
}
text(10, 0.95*N, "D", cex = letter_size)
mtext("Timestep", 1, line = 4, cex = yaxis_size)
mtext("Number infected/removed", 2, line = 4, cex = yaxis_size, outer = FALSE)
# legend(100, 0.8*N, c("True infected", "Deterministic infected"), pch = 1, col = c("red", "black"), bty = "n")
# R = 6, Pop = 200
N = 200
plot(true_R6_pop200_seed1$I, ylim = c(0, N), type = "l", col = "white", xlab = "", ylab = "", cex.axis = number_size)
lines(true_R6_pop200_seed1$I, ylim = c(0, N), type = "l", col = "red", lwd = 3)
lines(true_R6_pop200_seed1$R, ylim = c(0, N), type = "l", col = "orange", lwd = 3)
for (i in 2:nrow(re_R1.5_pop200_seed1)){
  sir_data <- pre_sir(true_R6_pop200_seed1, re_R6_pop200_seed1[i,])
  lines(sir_data$I, type = "l", lty = 2, col = "gray50", xlab = " ", ylab = " ", lwd = 0.5)
  lines(sir_data$R, type = "l", lty = 3, col = "gray80", xlab = " ", ylab = " ", lwd = 0.5)
}
for (i in 1){
  sir_data <- pre_sir(true_R6_pop200_seed1, re_R6_pop200_seed1[i,])
  lines(sir_data$I, type = "l", lty = 2, col = "black", xlab = " ", ylab = " ", lwd = 2)
  lines(sir_data$R, type = "l", lty = 3, col = "black", xlab = " ", ylab = " ", lwd = 2)
}
text(10, 0.95*N, "E", cex = letter_size)
mtext("Timestep", 1, line = 4, cex = yaxis_size)
# legend(100, 0.8*N, c("True infected", "Deterministic infected"), pch = 1, col = c("red", "black"), bty = "n")
# R = 6, Pop = 1000
N = 1000
plot(true_R6_pop1000_seed1$I, ylim = c(0, N), type = "l", col = "white", xlab = "", ylab = "", cex.axis = number_size)
lines(true_R6_pop1000_seed1$I, ylim = c(0, N), type = "l", col = "red", lwd = 3)
lines(true_R6_pop1000_seed1$R, ylim = c(0, N), type = "l", col = "orange", lwd = 3)
for (i in 2:nrow(re_R6_pop1000_seed1)){
  sir_data <- pre_sir(true_R6_pop1000_seed1, re_R6_pop1000_seed1[i,])
  lines(sir_data$I, type = "l", lty = 2, col = "gray50", xlab = " ", ylab = " ", lwd = 0.5)
  lines(sir_data$R, type = "l", lty = 3, col = "gray80", xlab = " ", ylab = " ", lwd = 0.5)
}
for (i in 1){
  sir_data <- pre_sir(true_R6_pop1000_seed1, re_R6_pop1000_seed1[i,])
  lines(sir_data$I, type = "l", lty = 2, col = "black", xlab = " ", ylab = " ", lwd = 2)
  lines(sir_data$R, type = "l", lty = 3, col = "black", xlab = " ", ylab = " ", lwd = 2)
}
text(10, 0.95*N, "F", cex = letter_size)
mtext("Timestep", 1, line = 4, cex = yaxis_size)
# legend(100, 0.8*N, c("True infected", "Deterministic infected"), pch = 1, col = c("red", "black"), bty = "n")
@
\caption{Comparing the true simulated infected (red line) and removed curves (orange line) to those constructed from the point estimates and bootstrapped per capita rate of infection ($\beta$) and recovery rate ($\gamma$) of the RE method for the first simulation of each population size and $R_{0}$. The black lines refer to the deterministic infectious (dashed) and removed (dotted) curves based on the $\beta$ and $\gamma$ point estimates. The grey lines represent the deterministic infectious (dark grey) and removed (light grey) curves based on the optimised $\beta$ and $\gamma$ values during the bootstrapping process. Panels A-C and D-F represent outbreaks with an $R_{0}$ of 1.5 and 6, respectively. Panels A and D represent a population size of 50, panels B and E represent a population size of 200, and panels C and F represent a population size of 1000.}
\label{re_inference}
\end{center}
\end{figure}
\clearpage

The exemplifying graphs for the first simulation of every outbreak visualising the estimated numbers of infected and removed individuals over time from the point estimates and results of bootstrapping (Fig. \ref{re_inference}), show that the infectious and recovery curves for outbreaks with the larger $R_{0}$ are more consistent with each other and exhibit less variation in the curves amongst $\beta$ and $\gamma$ values obtained through bootstrapping  than outbreaks with an $R_{0}$ of 1.5. It also seems that the deterministic infectious curves resulting from the point estimate values of $\beta$ and $\gamma$ consistently result in an underestimation of the peak of the outbreak for the scenarios where $R_{0}$ is 6 (Fig. \ref{re_inference} D-F). 

Tables 1, 2, and 3 show that when comparing the output of the residual error method to the parameter values used to generate the simulations that the RE method was being fit to, the RE inference method consistently overestimated the values that $\beta$ might take for all population sizes when $R_{0}$ is 1.5 and consistently underestimated $\beta$ when $R_{0}$ is 6. The true $\beta$ lowers with increasing population size due to $\beta$ referring to the individual-level rate of infection (Tables 1-3). The inferred values of $\gamma$, on the other hand, include the true value used for the generating the simulations more often than for $\beta$, though levels of success vary with $R_{0}$ and population size (Tables 1-3). The difference between the lower and upper bounds of the 95\% interval observed for $\beta$ and $\gamma$ were often wider than the differences for outbreaks with an $R_{0}$ of 6 (Tables 1-3).
\newline

<<RE_result_table_50, cache=FALSE, echo=FALSE, eval=TRUE, results=tex>>=
# Making MCMC table

results_table <- array(NA, dim =c(12,7))

# setwd("/home/evelina/OneDrive/MRes_BMR/Project_1/Work_folder/Data/server_data")
dir <- "/home/evelina/OneDrive/MRes_BMR/Project_1/Work_folder/Data/server_data/"

seed1 <- read.csv(paste(dir, "re_pop50_R1.5_g0.15_14.csv", sep=""))
seed2 <- read.csv(paste(dir, "re_pop50_R1.5_g0.15_22.csv", sep=""))
seed3 <- read.csv(paste(dir, "re_pop50_R1.5_g0.15_40.csv", sep=""))
seed4 <- read.csv(paste(dir, "re_pop50_R1.5_g0.15_43.csv", sep=""))
seed5 <- read.csv(paste(dir, "re_pop50_R1.5_g0.15_95.csv", sep=""))

results_table[2,1] <- 1
results_table[3,1] <- 2
results_table[4,1] <- 3
results_table[5,1] <- 4
results_table[6,1] <- 5

results_table[8,1] <- 1
results_table[9,1] <- 2
results_table[10,1] <- 3
results_table[11,1] <- 4
results_table[12,1] <- 5

# Pop = 1000, R0 = 1.5
results_table[1,2] <- (1.5 * 0.15) / 50
results_table[2,2] <- seed1$beta[1]
intervals <- quantile(seed1$beta[2:1001], c(0.025, 0.975))
results_table[2,3] <- intervals[1]
results_table[2,4] <- intervals[2]
results_table[3,2] <- seed2$beta[1]
intervals <- quantile(seed2$beta[2:1001], c(0.025, 0.975))
results_table[3,3] <- intervals[1]
results_table[3,4] <- intervals[2]
results_table[4,2] <- seed3$beta[1]
intervals <- quantile(seed3$beta[2:1001], c(0.025, 0.975))
results_table[4,3] <- intervals[1]
results_table[4,4] <- intervals[2]
results_table[5,2] <- seed4$beta[1]
intervals <- quantile(seed4$beta[2:1001], c(0.025, 0.975))
results_table[5,3] <- intervals[1]
results_table[5,4] <- intervals[2]
results_table[6,2] <- seed5$beta[1]
intervals <- quantile(seed5$beta[2:1001], c(0.025, 0.975))
results_table[6,3] <- intervals[1]
results_table[6,4] <- intervals[2]

# Pop = 1000, R0 = 1.5
results_table[1,5] <- 0.15
results_table[2,5] <- seed1$gamma[1]
intervals <- quantile(seed1$gamma[2:1001], c(0.025, 0.975))
results_table[2,6] <- intervals[1]
results_table[2,7] <- intervals[2]
results_table[3,5] <- seed2$gamma[1]
intervals <- quantile(seed2$gamma[2:1001], c(0.025, 0.975))
results_table[3,6] <- intervals[1]
results_table[3,7] <- intervals[2]
results_table[4,5] <- seed3$gamma[1]
intervals <- quantile(seed3$gamma[2:1001], c(0.025, 0.975))
results_table[4,6] <- intervals[1]
results_table[4,7] <- intervals[2]
results_table[5,5] <- seed4$gamma[1]
intervals <- quantile(seed4$gamma[2:1001], c(0.025, 0.975))
results_table[5,6] <- intervals[1]
results_table[5,7] <- intervals[2]
results_table[6,5] <- seed5$gamma[1]
intervals <- quantile(seed5$gamma[2:1001], c(0.025, 0.975))
results_table[6,6] <- intervals[1]
results_table[6,7] <- intervals[2]

# R0 = 6 Gamma = 0.15
seed2 <- read.csv(paste(dir, "re_pop50_R6_g0.15_1.csv", sep =""))
seed1 <- read.csv(paste(dir, "re_pop50_R6_g0.15_2.csv", sep =""))
seed3 <- read.csv(paste(dir, "re_pop50_R6_g0.15_3.csv", sep =""))
seed4 <- read.csv(paste(dir, "re_pop50_R6_g0.15_4.csv", sep =""))
seed5 <- read.csv(paste(dir, "re_pop50_R6_g0.15_5.csv", sep =""))

# Pop = 1000, R0 = 6
results_table[7,2] <- (6 * 0.15) / 50
results_table[8,2] <- seed1$beta[1]
intervals <- quantile(seed1$beta[2:1001], c(0.025, 0.975))
results_table[8,3] <- intervals[1]
results_table[8,4] <- intervals[2]
results_table[9,2] <- seed2$beta[1]
intervals <- quantile(seed2$beta[2:1001], c(0.025, 0.975))
results_table[9,3] <- intervals[1]
results_table[9,4] <- intervals[2]
results_table[10,2] <- seed3$beta[1]
intervals <- quantile(seed3$beta[2:1001], c(0.025, 0.975))
results_table[10,3] <- intervals[1]
results_table[10,4] <- intervals[2]
results_table[11,2] <- seed4$beta[1]
intervals <- quantile(seed4$beta[2:1001], c(0.025, 0.975))
results_table[11,3] <- intervals[1]
results_table[11,4] <- intervals[2]
results_table[12,2] <- seed5$beta[1]
intervals <- quantile(seed5$beta[2:1001], c(0.025, 0.975))
results_table[12,3] <- intervals[1]
results_table[12,4] <- intervals[2]

# Pop = 1000, R0 = 6
results_table[7,5] <- 0.15
results_table[8,5] <- seed1$gamma[1]
intervals <- quantile(seed1$gamma[2:1001], c(0.025, 0.975))
results_table[8,6] <- intervals[1]
results_table[8,7] <- intervals[2]
results_table[9,5] <- seed2$gamma[1]
intervals <- quantile(seed2$gamma[2:1001], c(0.025, 0.975))
results_table[9,6] <- intervals[1]
results_table[9,7] <- intervals[2]
results_table[10,5] <- seed3$gamma[1]
intervals <- quantile(seed3$gamma[2:1001], c(0.025, 0.975))
results_table[10,6] <- intervals[1]
results_table[10,7] <- intervals[2]
results_table[11,5] <- seed4$gamma[1]
intervals <- quantile(seed4$gamma[2:1001], c(0.025, 0.975))
results_table[11,6] <- intervals[1]
results_table[11,7] <- intervals[2]
results_table[12,5] <- seed5$gamma[1]
intervals <- quantile(seed5$gamma[2:1001], c(0.025, 0.975))
results_table[12,6] <- intervals[1]
results_table[12,7] <- intervals[2]

##############################
# Making Residual Error table
library('xtable')
# options(xtable.floating = FALSE)
# options(xtable.timestamp = "")

rownames(results_table) <- c("1.5", "\\textcolor{white}{a}","\\textcolor{white}{b}","\\textcolor{white}{c}","\\textcolor{white}{d}","\\textcolor{white}{e}","6","\\textcolor{white}{f}","\\textcolor{white}{g}","\\textcolor{white}{h}","\\textcolor{white}{i}","\\textcolor{white}{j}")

addtorow <- list()
addtorow$pos <- list(0, 0)
addtorow$command <- c("& \\multicolumn{1}{X}{} & \\multicolumn{3}{X}{$\\beta$}  & \\multicolumn{3}{X}{$\\gamma$} \\\\\n", "$R_{0}$ & simulation & PE & min & max & PE & min & max \\\\\n")

tab <- xtable(results_table, digits= c(1, 0, -1, -1, -1, 2, 2, 2), caption = "Values for $\\beta$, the rate of infection, and $\\gamma$, the recovery rate, for the RE inference method  $R_{0}$ of 1.5 or 6 for a population size of 50. The numbers presented without 95\\% intervals are the true $\\beta$ and $\\gamma$ for each $R_{0}$. PE: point estimate, min: lower bound of 95\\% interval, max: upper bound of 95\\% interval", label = "re_pop50") 
align(tab) <- "lXXXXXXX"
print(tab, hline.after=c(-1, 0, 12), comment = FALSE, math.style.exponents = FALSE, caption.placement = "top", add.to.row = addtorow, include.colnames = FALSE, type="latex", sanitize.rownames.function = identity, tabular.environment = "tabularx", width = "\\textwidth")
@
\clearpage
For population size 50, the 95\% interval overestimated the true $\beta$ value for all five simulations and one out of five simulations when $R_{0}$ is 1.5 and 6 respectively (Table \ref{re_pop50}). The remaining four simulations for the higher $R_{0}$, on the other hand, underestimated $\beta$. Simulations 1 and 4 for the outbreaks where $R_{0}$ of 1.5 overestimated $\gamma$, with the remaining three simulations underestimating $\gamma$. $\gamma$ was included in the 95\% intervals for simulations 1 and 4 for the outbreaks with a $R_{0}$ of 6, while simulations 1 and 5 overestimated $\gamma$ and simulation 3 underestimated it.

For outbreaks with a population size of 200, the 95\% interval again overerestimated $\beta$ for four out of five outbreaks with an $R_{0}$ of 1.5, though simulation 1 included the true value in its interval (Table \ref{re_pop200}). For outbreaks with an $R_{0}$ of 6, $\beta$ was underestimated for all five simulations (Table \ref{re_pop200}). $\gamma$ was underestimated for two simulations, overestimated for one simulation, and is included in the 95\% interval for two simulations for the outbreaks where $R_{0}$ is 1.5. For outbreaks where $R_{0}$ is 6, $\gamma$ was included in the 95\% interval for one simulation while the remaining four simulations' intervals overestimated $\gamma$.
\newline

<<RE_result_table_200, cache=FALSE, echo=FALSE, eval=TRUE, results=tex>>=
# Making MCMC table

results_table <- array(NA, dim =c(12,7))

# setwd("/home/evelina/OneDrive/MRes_BMR/Project_1/Work_folder/Data/server_data")
dir <- "/home/evelina/OneDrive/MRes_BMR/Project_1/Work_folder/Data/server_data/"

seed1 <- read.csv(paste(dir, "re_pop200_R1.5_g0.15_81.csv", sep=""))
seed2 <- read.csv(paste(dir, "re_pop200_R1.5_g0.15_93.csv", sep=""))
seed3 <- read.csv(paste(dir, "re_pop200_R1.5_g0.15_102.csv", sep=""))
seed4 <- read.csv(paste(dir, "re_pop200_R1.5_g0.15_202.csv", sep=""))
seed5 <- read.csv(paste(dir, "re_pop200_R1.5_g0.15_246.csv", sep=""))

results_table[2,1] <- 1
results_table[3,1] <- 2
results_table[4,1] <- 3
results_table[5,1] <- 4
results_table[6,1] <- 5

results_table[8,1] <- 1
results_table[9,1] <- 2
results_table[10,1] <- 3
results_table[11,1] <- 4
results_table[12,1] <- 5

# Pop = 1000, R0 = 1.5
results_table[1,2] <- (1.5 * 0.15) / 200
results_table[2,2] <- seed1$beta[1]
intervals <- quantile(seed1$beta[2:1001], c(0.025, 0.975))
results_table[2,3] <- intervals[1]
results_table[2,4] <- intervals[2]
results_table[3,2] <- seed2$beta[1]
intervals <- quantile(seed2$beta[2:1001], c(0.025, 0.975))
results_table[3,3] <- intervals[1]
results_table[3,4] <- intervals[2]
results_table[4,2] <- seed3$beta[1]
intervals <- quantile(seed3$beta[2:1001], c(0.025, 0.975))
results_table[4,3] <- intervals[1]
results_table[4,4] <- intervals[2]
results_table[5,2] <- seed4$beta[1]
intervals <- quantile(seed4$beta[2:1001], c(0.025, 0.975))
results_table[5,3] <- intervals[1]
results_table[5,4] <- intervals[2]
results_table[6,2] <- seed5$beta[1]
intervals <- quantile(seed5$beta[2:1001], c(0.025, 0.975))
results_table[6,3] <- intervals[1]
results_table[6,4] <- intervals[2]

# Pop = 1000, R0 = 1.5
results_table[1,5] <- 0.15
results_table[2,5] <- seed1$gamma[1]
intervals <- quantile(seed1$gamma[2:1001], c(0.025, 0.975))
results_table[2,6] <- intervals[1]
results_table[2,7] <- intervals[2]
results_table[3,5] <- seed2$gamma[1]
intervals <- quantile(seed2$gamma[2:1001], c(0.025, 0.975))
results_table[3,6] <- intervals[1]
results_table[3,7] <- intervals[2]
results_table[4,5] <- seed3$gamma[1]
intervals <- quantile(seed3$gamma[2:1001], c(0.025, 0.975))
results_table[4,6] <- intervals[1]
results_table[4,7] <- intervals[2]
results_table[5,5] <- seed4$gamma[1]
intervals <- quantile(seed4$gamma[2:1001], c(0.025, 0.975))
results_table[5,6] <- intervals[1]
results_table[5,7] <- intervals[2]
results_table[6,5] <- seed5$gamma[1]
intervals <- quantile(seed5$gamma[2:1001], c(0.025, 0.975))
results_table[6,6] <- intervals[1]
results_table[6,7] <- intervals[2]

# R0 = 6 Gamma = 0.15
seed1 <- read.csv(paste(dir, "re_pop200_R6_g0.15_6.csv", sep =""))
seed2 <- read.csv(paste(dir, "re_pop200_R6_g0.15_7.csv", sep =""))
seed3 <- read.csv(paste(dir, "re_pop200_R6_g0.15_8.csv", sep =""))
seed4 <- read.csv(paste(dir, "re_pop200_R6_g0.15_9.csv", sep =""))
seed5 <- read.csv(paste(dir, "re_pop200_R6_g0.15_10.csv", sep =""))

# Pop = 1000, R0 = 6
results_table[7,2] <- (6 * 0.15) / 200
results_table[8,2] <- seed1$beta[1]
intervals <- quantile(seed1$beta[2:1001], c(0.025, 0.975))
results_table[8,3] <- intervals[1]
results_table[8,4] <- intervals[2]
results_table[9,2] <- seed2$beta[1]
intervals <- quantile(seed2$beta[2:1001], c(0.025, 0.975))
results_table[9,3] <- intervals[1]
results_table[9,4] <- intervals[2]
results_table[10,2] <- seed3$beta[1]
intervals <- quantile(seed3$beta[2:1001], c(0.025, 0.975))
results_table[10,3] <- intervals[1]
results_table[10,4] <- intervals[2]
results_table[11,2] <- seed4$beta[1]
intervals <- quantile(seed4$beta[2:1001], c(0.025, 0.975))
results_table[11,3] <- intervals[1]
results_table[11,4] <- intervals[2]
results_table[12,2] <- seed5$beta[1]
intervals <- quantile(seed5$beta[2:1001], c(0.025, 0.975))
results_table[12,3] <- intervals[1]
results_table[12,4] <- intervals[2]

# Pop = 1000, R0 = 6
results_table[7,5] <- 0.15
results_table[8,5] <- seed1$gamma[1]
intervals <- quantile(seed1$gamma[2:1001], c(0.025, 0.975))
results_table[8,6] <- intervals[1]
results_table[8,7] <- intervals[2]
results_table[9,5] <- seed2$gamma[1]
intervals <- quantile(seed2$gamma[2:1001], c(0.025, 0.975))
results_table[9,6] <- intervals[1]
results_table[9,7] <- intervals[2]
results_table[10,5] <- seed3$gamma[1]
intervals <- quantile(seed3$gamma[2:1001], c(0.025, 0.975))
results_table[10,6] <- intervals[1]
results_table[10,7] <- intervals[2]
results_table[11,5] <- seed4$gamma[1]
intervals <- quantile(seed4$gamma[2:1001], c(0.025, 0.975))
results_table[11,6] <- intervals[1]
results_table[11,7] <- intervals[2]
results_table[12,5] <- seed5$gamma[1]
intervals <- quantile(seed5$gamma[2:1001], c(0.025, 0.975))
results_table[12,6] <- intervals[1]
results_table[12,7] <- intervals[2]

##############################
# Making Residual Error table
library('xtable')
# options(xtable.floating = FALSE)
# options(xtable.timestamp = "")

rownames(results_table) <- c("1.5", "\\textcolor{white}{a}","\\textcolor{white}{b}","\\textcolor{white}{c}","\\textcolor{white}{d}","\\textcolor{white}{e}","6","\\textcolor{white}{f}","\\textcolor{white}{g}","\\textcolor{white}{h}","\\textcolor{white}{i}","\\textcolor{white}{j}")

addtorow <- list()
addtorow$pos <- list(0, 0)
addtorow$command <- c("& \\multicolumn{1}{l}{} & \\multicolumn{3}{l}{$\\beta$}  & \\multicolumn{3}{l}{$\\gamma$} \\\\\n", "$R_{0}$ & simulation & PE & min & max & PE & min & max \\\\\n")

tab <- xtable(results_table, digits= c(1, 0, -1, -1, -1, 2, 2, 2), caption = "Values for $\\beta$, the rate of infection, and $\\gamma$, the recovery rate, for the RE inference method  $R_{0}$ of 1.5 or 6 for a population size of 200. The numbers presented without 95\\% intervals are the true $\\beta$ and $\\gamma$ for each $R_{0}$. PE: point estimate, min: lower bound of 95\\% interval, max: upper bound of 95\\% interval", label = "re_pop200") 
align(tab) <- "lXXXXXXX"
print(tab, hline.after=c(-1, 0, 12), comment = FALSE, math.style.exponents = FALSE, caption.placement = "top", add.to.row = addtorow, include.colnames = FALSE, type="latex", sanitize.rownames.function = identity, tabular.environment = "tabularx", width = "\\textwidth")
@

\newpage
$\beta$ was also overestimated for most of the outbreaks where $R_{0}$ was 1.5 and the population size was 1000. Only simulation 1's 95\% interval included the true $\beta$ value (Table \ref{re_pop1000}). Three out of the five simulations included the true $\gamma$ value in their 95\% intervals. When $R_{0}$ was 6, $\beta$ was underestimated for all five simulations, though all of them did include the true $\gamma$ value in their 95\% intervals (Table \ref{re_pop1000}).  
\newline

<<RE_result_table_1000, cache=FALSE, echo=FALSE, eval=TRUE, results=tex>>=
# Making MCMC table

results_table <- array(NA, dim =c(12,7))

# setwd("/home/evelina/OneDrive/MRes_BMR/Project_1/Work_folder/Data/server_data")
dir <- "/home/evelina/OneDrive/MRes_BMR/Project_1/Work_folder/Data/server_data/"

seed1 <- read.csv(paste(dir, "re_pop1000_R1.5_g0.15_177.csv", sep=""))
seed2 <- read.csv(paste(dir, "re_pop1000_R1.5_g0.15_246.csv", sep=""))
seed3 <- read.csv(paste(dir, "re_pop1000_R1.5_g0.15_469.csv", sep=""))
seed4 <- read.csv(paste(dir, "re_pop1000_R1.5_g0.15_520.csv", sep=""))
seed5 <- read.csv(paste(dir, "re_pop1000_R1.5_g0.15_784.csv", sep=""))

results_table[2,1] <- 1
results_table[3,1] <- 2
results_table[4,1] <- 3
results_table[5,1] <- 4
results_table[6,1] <- 5

results_table[8,1] <- 1
results_table[9,1] <- 2
results_table[10,1] <- 3
results_table[11,1] <- 4
results_table[12,1] <- 5

# Pop = 1000, R0 = 1.5
results_table[1,2] <- (1.5 * 0.15) / 1000
results_table[2,2] <- seed1$beta[1]
intervals <- quantile(seed1$beta[2:1001], c(0.025, 0.975))
results_table[2,3] <- intervals[1]
results_table[2,4] <- intervals[2]
results_table[3,2] <- seed2$beta[1]
intervals <- quantile(seed2$beta[2:1001], c(0.025, 0.975))
results_table[3,3] <- intervals[1]
results_table[3,4] <- intervals[2]
results_table[4,2] <- seed3$beta[1]
intervals <- quantile(seed3$beta[2:1001], c(0.025, 0.975))
results_table[4,3] <- intervals[1]
results_table[4,4] <- intervals[2]
results_table[5,2] <- seed4$beta[1]
intervals <- quantile(seed4$beta[2:1001], c(0.025, 0.975))
results_table[5,3] <- intervals[1]
results_table[5,4] <- intervals[2]
results_table[6,2] <- seed5$beta[1]
intervals <- quantile(seed5$beta[2:1001], c(0.025, 0.975))
results_table[6,3] <- intervals[1]
results_table[6,4] <- intervals[2]

# Pop = 1000, R0 = 1.5
results_table[1,5] <- 0.15
results_table[2,5] <- seed1$gamma[1]
intervals <- quantile(seed1$gamma[2:1001], c(0.025, 0.975))
results_table[2,6] <- intervals[1]
results_table[2,7] <- intervals[2]
results_table[3,5] <- seed2$gamma[1]
intervals <- quantile(seed2$gamma[2:1001], c(0.025, 0.975))
results_table[3,6] <- intervals[1]
results_table[3,7] <- intervals[2]
results_table[4,5] <- seed3$gamma[1]
intervals <- quantile(seed3$gamma[2:1001], c(0.025, 0.975))
results_table[4,6] <- intervals[1]
results_table[4,7] <- intervals[2]
results_table[5,5] <- seed4$gamma[1]
intervals <- quantile(seed4$gamma[2:1001], c(0.025, 0.975))
results_table[5,6] <- intervals[1]
results_table[5,7] <- intervals[2]
results_table[6,5] <- seed5$gamma[1]
intervals <- quantile(seed5$gamma[2:1001], c(0.025, 0.975))
results_table[6,6] <- intervals[1]
results_table[6,7] <- intervals[2]

# R0 = 6 Gamma = 0.15
seed1 <- read.csv(paste(dir, "re_pop1000_R6_g0.15_11.csv", sep =""))
seed2 <- read.csv(paste(dir, "re_pop1000_R6_g0.15_12.csv", sep =""))
seed3 <- read.csv(paste(dir, "re_pop1000_R6_g0.15_13.csv", sep =""))
seed4 <- read.csv(paste(dir, "re_pop1000_R6_g0.15_14.csv", sep =""))
seed5 <- read.csv(paste(dir, "re_pop1000_R6_g0.15_15.csv", sep =""))

# Pop = 1000, R0 = 6
results_table[7,2] <- (6 * 0.15) / 1000
results_table[8,2] <- seed1$beta[1]
intervals <- quantile(seed1$beta[2:1001], c(0.025, 0.975))
results_table[8,3] <- intervals[1]
results_table[8,4] <- intervals[2]
results_table[9,2] <- seed2$beta[1]
intervals <- quantile(seed2$beta[2:1001], c(0.025, 0.975))
results_table[9,3] <- intervals[1]
results_table[9,4] <- intervals[2]
results_table[10,2] <- seed3$beta[1]
intervals <- quantile(seed3$beta[2:1001], c(0.025, 0.975))
results_table[10,3] <- intervals[1]
results_table[10,4] <- intervals[2]
results_table[11,2] <- seed4$beta[1]
intervals <- quantile(seed4$beta[2:1001], c(0.025, 0.975))
results_table[11,3] <- intervals[1]
results_table[11,4] <- intervals[2]
results_table[12,2] <- seed5$beta[1]
intervals <- quantile(seed5$beta[2:1001], c(0.025, 0.975))
results_table[12,3] <- intervals[1]
results_table[12,4] <- intervals[2]

# Pop = 1000, R0 = 6
results_table[7,5] <- 0.15
results_table[8,5] <- seed1$gamma[1]
intervals <- quantile(seed1$gamma[2:1001], c(0.025, 0.975))
results_table[8,6] <- intervals[1]
results_table[8,7] <- intervals[2]
results_table[9,5] <- seed2$gamma[1]
intervals <- quantile(seed2$gamma[2:1001], c(0.025, 0.975))
results_table[9,6] <- intervals[1]
results_table[9,7] <- intervals[2]
results_table[10,5] <- seed3$gamma[1]
intervals <- quantile(seed3$gamma[2:1001], c(0.025, 0.975))
results_table[10,6] <- intervals[1]
results_table[10,7] <- intervals[2]
results_table[11,5] <- seed4$gamma[1]
intervals <- quantile(seed4$gamma[2:1001], c(0.025, 0.975))
results_table[11,6] <- intervals[1]
results_table[11,7] <- intervals[2]
results_table[12,5] <- seed5$gamma[1]
intervals <- quantile(seed5$gamma[2:1001], c(0.025, 0.975))
results_table[12,6] <- intervals[1]
results_table[12,7] <- intervals[2]

##############################
# Making Residual Error table
library('xtable')
# options(xtable.floating = FALSE)
# options(xtable.timestamp = "")

rownames(results_table) <- c("1.5", "\\textcolor{white}{a}","\\textcolor{white}{b}","\\textcolor{white}{c}","\\textcolor{white}{d}","\\textcolor{white}{e}","6","\\textcolor{white}{f}","\\textcolor{white}{g}","\\textcolor{white}{h}","\\textcolor{white}{i}","\\textcolor{white}{j}")

addtorow <- list()
addtorow$pos <- list(0, 0)
addtorow$command <- c("& \\multicolumn{1}{l}{} & \\multicolumn{3}{l}{$\\beta$}  & \\multicolumn{3}{l}{$\\gamma$} \\\\\n", "$R_{0}$ & simulation & PE & min & max & PE & min & max \\\\\n")

tab <- xtable(results_table, digits= c(1, 0, -1, -1, -1, 2, 2, 2), caption = "Values for $\\beta$, the per capita rate of infection, and $\\gamma$, the recovery rate, for the RE inference method  $R_{0}$ of 1.5 or 6 for a population size of 1000. The numbers presented without 95\\% intervals are the true $\\beta$ and $\\gamma$ for each $R_{0}$. PE: point estimate, min: lower bound of 95\\% interval, max: upper bound of 95\\% interval", label = "re_pop1000") 
align(tab) <- "lXXXXXXX"
print(tab, hline.after=c(-1, 0, 12), comment = FALSE, math.style.exponents = FALSE, caption.placement = "top", add.to.row = addtorow, include.colnames = FALSE, type="latex", sanitize.rownames.function = identity, tabular.environment = "tabularx", width = "\\textwidth")
@
\newline
\newline
<<RE_result_table_RE, cache=FALSE, echo=FALSE, eval=TRUE, results=tex>>=
# Making Residual Error table
RE_results_table <- array(NA, dim =c(10,4))

# setwd("/home/evelina/OneDrive/MRes_BMR/Project_1/Work_folder/Data/server_data")
dir <- "/home/evelina/OneDrive/MRes_BMR/Project_1/Work_folder/Data/server_data/"

re_pop50_seed1 <- read.csv(paste(dir, "re_pop50_R1.5_g0.15_14.csv", sep=""))
re_pop50_seed2 <- read.csv(paste(dir, "re_pop50_R1.5_g0.15_22.csv", sep=""))
re_pop50_seed3 <- read.csv(paste(dir, "re_pop50_R1.5_g0.15_40.csv", sep=""))
re_pop50_seed4 <- read.csv(paste(dir, "re_pop50_R1.5_g0.15_43.csv", sep=""))
re_pop50_seed5 <- read.csv(paste(dir, "re_pop50_R1.5_g0.15_95.csv", sep=""))
re_pop200_seed1 <- read.csv(paste(dir, "re_pop200_R1.5_g0.15_81.csv", sep=""))
re_pop200_seed2 <- read.csv(paste(dir, "re_pop200_R1.5_g0.15_93.csv", sep=""))
re_pop200_seed3 <- read.csv(paste(dir, "re_pop200_R1.5_g0.15_102.csv", sep=""))
re_pop200_seed4 <- read.csv(paste(dir, "re_pop200_R1.5_g0.15_202.csv", sep =""))
re_pop200_seed5 <- read.csv(paste(dir, "re_pop200_R1.5_g0.15_246.csv", sep =""))
re_pop1000_seed1 <- read.csv(paste(dir, "re_pop1000_R1.5_g0.15_177.csv", sep =""))
re_pop1000_seed2 <- read.csv(paste(dir, "re_pop1000_R1.5_g0.15_246.csv", sep =""))
re_pop1000_seed3 <- read.csv(paste(dir, "re_pop1000_R1.5_g0.15_469.csv", sep =""))
re_pop1000_seed4 <- read.csv(paste(dir, "re_pop1000_R1.5_g0.15_520.csv", sep =""))
re_pop1000_seed5 <- read.csv(paste(dir,"re_pop1000_R1.5_g0.15_784.csv", sep =""))

# Pop = 50, R0 = 1.5
RE_results_table[1,1] <- 1
RE_results_table[2,1] <- 2
RE_results_table[3,1] <- 3
RE_results_table[4,1] <- 4
RE_results_table[5,1] <- 5
RE_results_table[1,2] <- re_pop50_seed1$RE[1]
RE_results_table[2,2] <- re_pop50_seed2$RE[1]
RE_results_table[3,2] <- re_pop50_seed3$RE[1]
RE_results_table[4,2] <- re_pop50_seed4$RE[1]
RE_results_table[5,2] <- re_pop50_seed5$RE[1]
# Pop = 200, R0 = 1.5
RE_results_table[1,3] <- re_pop200_seed1$RE[1]
RE_results_table[2,3] <- re_pop200_seed2$RE[1]
RE_results_table[3,3] <- re_pop200_seed3$RE[1]
RE_results_table[4,3] <- re_pop200_seed4$RE[1]
RE_results_table[5,3] <- re_pop200_seed5$RE[1]
# Pop = 1000, R0 = 1.5
RE_results_table[1,4] <- re_pop1000_seed1$RE[1]
RE_results_table[2,4] <- re_pop1000_seed2$RE[1]
RE_results_table[3,4] <- re_pop1000_seed3$RE[1]
RE_results_table[4,4] <- re_pop1000_seed4$RE[1]
RE_results_table[5,4] <- re_pop1000_seed5$RE[1]

# R0 = 6 Gamma = 0.15
re_pop50_seed2 <- read.csv(paste(dir, "re_pop50_R6_g0.15_1.csv", sep =""))
re_pop50_seed1 <- read.csv(paste(dir, "re_pop50_R6_g0.15_2.csv", sep =""))
re_pop50_seed3 <- read.csv(paste(dir, "re_pop50_R6_g0.15_3.csv", sep =""))
re_pop50_seed4 <- read.csv(paste(dir, "re_pop50_R6_g0.15_4.csv", sep =""))
re_pop50_seed5 <- read.csv(paste(dir, "re_pop50_R6_g0.15_5.csv", sep =""))
re_pop200_seed1 <- read.csv(paste(dir, "re_pop200_R6_g0.15_6.csv", sep =""))
re_pop200_seed2 <- read.csv(paste(dir, "re_pop200_R6_g0.15_7.csv", sep =""))
re_pop200_seed3 <- read.csv(paste(dir, "re_pop200_R6_g0.15_8.csv", sep =""))
re_pop200_seed4 <- read.csv(paste(dir, "re_pop200_R6_g0.15_9.csv", sep =""))
re_pop200_seed5 <- read.csv(paste(dir, "re_pop200_R6_g0.15_10.csv", sep =""))
re_pop1000_seed1 <- read.csv(paste(dir, "re_pop1000_R6_g0.15_11.csv", sep =""))
re_pop1000_seed2 <- read.csv(paste(dir, "re_pop1000_R6_g0.15_12.csv", sep =""))
re_pop1000_seed3 <- read.csv(paste(dir, "re_pop1000_R6_g0.15_13.csv", sep =""))
re_pop1000_seed4 <- read.csv(paste(dir, "re_pop1000_R6_g0.15_14.csv", sep =""))
re_pop1000_seed5 <- read.csv(paste(dir, "re_pop1000_R6_g0.15_15.csv", sep =""))

RE_results_table[6,1] <- 1
RE_results_table[7,1] <- 2
RE_results_table[8,1] <- 3
RE_results_table[9,1] <- 4
RE_results_table[10,1] <- 5
# Pop = 50, R0 = 1.5
RE_results_table[6,2] <- re_pop50_seed1$RE[1]
RE_results_table[7,2] <- re_pop50_seed2$RE[1]
RE_results_table[8,2] <- re_pop50_seed3$RE[1]
RE_results_table[9,2] <- re_pop50_seed4$RE[1]
RE_results_table[10,2] <- re_pop50_seed5$RE[1]
# Pop = 200, R0 = 1.5
RE_results_table[6,3] <- re_pop200_seed1$RE[1]
RE_results_table[7,3] <- re_pop200_seed2$RE[1]
RE_results_table[8,3] <- re_pop200_seed3$RE[1]
RE_results_table[9,3] <- re_pop200_seed4$RE[1]
RE_results_table[10,3] <- re_pop200_seed5$RE[1]
# Pop = 1000, R0 = 1.5
RE_results_table[6,4] <- re_pop1000_seed1$RE[1]
RE_results_table[7,4] <- re_pop1000_seed2$RE[1]
RE_results_table[8,4] <- re_pop1000_seed3$RE[1]
RE_results_table[9,4] <- re_pop1000_seed4$RE[1]
RE_results_table[10,4] <- re_pop1000_seed5$RE[1]

##############################
# Making Residual Error table
library('xtable')
# options(xtable.floating = FALSE)
# options(xtable.timestamp = "")

rownames(RE_results_table) <- c("1.5","\\textcolor{white}{b}","\\textcolor{white}{c}","\\textcolor{white}{d}","\\textcolor{white}{e}","6","\\textcolor{white}{g}","\\textcolor{white}{h}","\\textcolor{white}{i}","\\textcolor{white}{j}")

addtorow <- list()
addtorow$pos <- list(0, 0)
addtorow$command <- c("& \\multicolumn{1}{X}{}  & \\multicolumn{1}{X}{N = 50}  & \\multicolumn{1}{X}{N = 200} & \\multicolumn{1}{X}{N = 1000} \\\\\n", "$R_{0}$ & simulation & PE & PE & PE \\\\\n")

tab <- xtable(RE_results_table, digits = 0, caption = "Residual error values for the RE inference method for each outbreak of a given population size N and $R_{0}$ of 1.5 or 6. PE: point estimate", label = "re_pe") 
align(tab) <- "lXXXX"
print(tab, hline.after=c(-1, 0, 10), comment = FALSE, math.style.exponents = FALSE, caption.placement = "top", add.to.row = addtorow, include.colnames = FALSE, type="latex", sanitize.rownames.function = identity, tabular.environment = "tabularx", width = "\\textwidth")
@

\FloatBarrier
\clearpage

The sum of squared differences between the model and the true recovery curve increased with population size (Table \ref{re_pe}). Additionally, the REs for the point estimates tended to be lower for the higher $R_{0}$ than the lower $R_{0}$ for the populations (Table \ref{re_pe}). 

% Discarded beta and gamma tables
<<RE_result_table_beta, cache=FALSE, echo=FALSE, eval=FALSE, results=tex>>=
# Making Residual Error table
RE_beta_results_table <- array(NA, dim =c(12,10))

# setwd("/home/evelina/OneDrive/MRes_BMR/Project_1/Work_folder/Data/server_data")
dir <- "/home/evelina/OneDrive/MRes_BMR/Project_1/Work_folder/Data/server_data/"

re_pop50_seed1 <- read.csv(paste(dir, "re_pop50_R1.5_g0.15_14.csv", sep=""))
re_pop50_seed2 <- read.csv(paste(dir, "re_pop50_R1.5_g0.15_22.csv", sep=""))
re_pop50_seed3 <- read.csv(paste(dir, "re_pop50_R1.5_g0.15_40.csv", sep=""))
re_pop50_seed4 <- read.csv(paste(dir, "re_pop50_R1.5_g0.15_43.csv", sep=""))
re_pop50_seed5 <- read.csv(paste(dir, "re_pop50_R1.5_g0.15_95.csv", sep=""))
re_pop200_seed1 <- read.csv(paste(dir, "re_pop200_R1.5_g0.15_81.csv", sep=""))
re_pop200_seed2 <- read.csv(paste(dir, "re_pop200_R1.5_g0.15_93.csv", sep=""))
re_pop200_seed3 <- read.csv(paste(dir, "re_pop200_R1.5_g0.15_102.csv", sep=""))
re_pop200_seed4 <- read.csv(paste(dir, "re_pop200_R1.5_g0.15_202.csv", sep =""))
re_pop200_seed5 <- read.csv(paste(dir, "re_pop200_R1.5_g0.15_246.csv", sep =""))
re_pop1000_seed1 <- read.csv(paste(dir, "re_pop1000_R1.5_g0.15_177.csv", sep =""))
re_pop1000_seed2 <- read.csv(paste(dir, "re_pop1000_R1.5_g0.15_246.csv", sep =""))
re_pop1000_seed3 <- read.csv(paste(dir, "re_pop1000_R1.5_g0.15_469.csv", sep =""))
re_pop1000_seed4 <- read.csv(paste(dir, "re_pop1000_R1.5_g0.15_520.csv", sep =""))
re_pop1000_seed5 <- read.csv(paste(dir,"re_pop1000_R1.5_g0.15_784.csv", sep =""))

# Pop = 50, R0 = 1.5
RE_beta_results_table[2,1] <- 1
RE_beta_results_table[3,1] <- 2
RE_beta_results_table[4,1] <- 3
RE_beta_results_table[5,1] <- 4
RE_beta_results_table[6,1] <- 5
RE_beta_results_table[1,2] <- (1.5 * 0.15) / 50
RE_beta_results_table[2,2] <- re_pop50_seed1$beta[1]
sorted <- sort(re_pop50_seed1$beta[2:1001], decreasing=FALSE)
RE_beta_results_table[2,3] <- sorted[25]
RE_beta_results_table[2,4] <- sorted[975]
RE_beta_results_table[3,2] <- re_pop50_seed2$beta[1]
sorted <- sort(re_pop50_seed2$beta[2:1001], decreasing=FALSE)
RE_beta_results_table[3,3] <- sorted[25]
RE_beta_results_table[3,4] <- sorted[975]
RE_beta_results_table[4,2] <- re_pop50_seed3$beta[1]
sorted <- sort(re_pop50_seed3$beta[2:1001], decreasing=FALSE)
RE_beta_results_table[4,3] <- sorted[25]
RE_beta_results_table[4,4] <- sorted[975]
RE_beta_results_table[5,2] <- re_pop50_seed4$beta[1]
sorted <- sort(re_pop50_seed4$beta[2:1001], decreasing=FALSE)
RE_beta_results_table[5,3] <- sorted[25]
RE_beta_results_table[5,4] <- sorted[975]
RE_beta_results_table[6,2] <- re_pop50_seed5$beta[1]
sorted <- sort(re_pop50_seed1$beta[2:1001], decreasing=FALSE)
RE_beta_results_table[6,3] <- sorted[25]
RE_beta_results_table[6,4] <- sorted[975]
# Pop = 200, R0 = 1.5
RE_beta_results_table[1,5] <- (1.5 * 0.15) / 200
RE_beta_results_table[2,5] <- re_pop200_seed1$beta[1]
sorted <- sort(re_pop200_seed1$beta[2:1001], decreasing=FALSE)
RE_beta_results_table[2,6] <- sorted[25]
RE_beta_results_table[2,7] <- sorted[975]
RE_beta_results_table[3,5] <- re_pop200_seed2$beta[1]
sorted <- sort(re_pop200_seed2$beta[2:1001], decreasing=FALSE)
RE_beta_results_table[3,6] <- sorted[25]
RE_beta_results_table[3,7] <- sorted[975]
RE_beta_results_table[4,5] <- re_pop200_seed3$beta[1]
sorted <- sort(re_pop200_seed3$beta[2:1001], decreasing=FALSE)
RE_beta_results_table[4,6] <- sorted[25]
RE_beta_results_table[4,7] <- sorted[975]
RE_beta_results_table[5,5] <- re_pop200_seed4$beta[1]
sorted <- sort(re_pop200_seed4$beta[2:1001], decreasing=FALSE)
RE_beta_results_table[5,6] <- sorted[25]
RE_beta_results_table[5,7] <- sorted[975]
RE_beta_results_table[6,5] <- re_pop200_seed5$beta[1]
sorted <- sort(re_pop200_seed5$beta[2:1001], decreasing=FALSE)
RE_beta_results_table[6,6] <- sorted[25]
RE_beta_results_table[6,7] <- sorted[975]
# Pop = 1000, R0 = 1.5
RE_beta_results_table[1,8] <- (1.5 * 0.15) / 1000
RE_beta_results_table[2,8] <- re_pop1000_seed1$beta[1]
sorted <- sort(re_pop1000_seed1$beta[2:1001], decreasing=FALSE)
RE_beta_results_table[2,9] <- sorted[25]
RE_beta_results_table[2,10] <- sorted[975]
RE_beta_results_table[3,8] <- re_pop1000_seed2$beta[1]
sorted <- sort(re_pop1000_seed2$beta[2:1001], decreasing=FALSE)
RE_beta_results_table[3,9] <- sorted[25]
RE_beta_results_table[3,10] <- sorted[975]
RE_beta_results_table[4,8] <- re_pop1000_seed3$beta[1]
sorted <- sort(re_pop1000_seed3$beta[2:1001], decreasing=FALSE)
RE_beta_results_table[4,9] <- sorted[25]
RE_beta_results_table[4,10] <- sorted[975]
RE_beta_results_table[5,8] <- re_pop1000_seed4$beta[1]
sorted <- sort(re_pop1000_seed4$beta[2:1001], decreasing=FALSE)
RE_beta_results_table[5,9] <- sorted[25]
RE_beta_results_table[5,10] <- sorted[975]
RE_beta_results_table[6,8] <- re_pop1000_seed5$beta[1]
sorted <- sort(re_pop1000_seed5$beta[2:1001], decreasing=FALSE)
RE_beta_results_table[6,9] <- sorted[25]
RE_beta_results_table[6,10] <- sorted[975]

# R0 = 6 Gamma = 0.15
re_pop50_seed1 <- read.csv(paste(dir, "re_pop50_R6_g0.15_1.csv", sep =""))
re_pop50_seed2 <- read.csv(paste(dir, "re_pop50_R6_g0.15_2.csv", sep =""))
re_pop50_seed3 <- read.csv(paste(dir, "re_pop50_R6_g0.15_3.csv", sep =""))
re_pop50_seed4 <- read.csv(paste(dir, "re_pop50_R6_g0.15_4.csv", sep =""))
re_pop50_seed5 <- read.csv(paste(dir, "re_pop50_R6_g0.15_5.csv", sep =""))
re_pop200_seed1 <- read.csv(paste(dir, "re_pop200_R6_g0.15_6.csv", sep =""))
re_pop200_seed2 <- read.csv(paste(dir, "re_pop200_R6_g0.15_7.csv", sep =""))
re_pop200_seed3 <- read.csv(paste(dir, "re_pop200_R6_g0.15_8.csv", sep =""))
re_pop200_seed4 <- read.csv(paste(dir, "re_pop200_R6_g0.15_9.csv", sep =""))
re_pop200_seed5 <- read.csv(paste(dir, "re_pop200_R6_g0.15_10.csv", sep =""))
re_pop1000_seed1 <- read.csv(paste(dir, "re_pop1000_R6_g0.15_11.csv", sep =""))
re_pop1000_seed2 <- read.csv(paste(dir, "re_pop1000_R6_g0.15_12.csv", sep =""))
re_pop1000_seed3 <- read.csv(paste(dir, "re_pop1000_R6_g0.15_13.csv", sep =""))
re_pop1000_seed4 <- read.csv(paste(dir, "re_pop1000_R6_g0.15_14.csv", sep =""))
re_pop1000_seed5 <- read.csv(paste(dir, "re_pop1000_R6_g0.15_15.csv", sep =""))

# Pop = 50, R0 = 6
RE_beta_results_table[8,1] <- 1
RE_beta_results_table[9,1] <- 2
RE_beta_results_table[10,1] <- 3
RE_beta_results_table[11,1] <- 4
RE_beta_results_table[12,1] <- 5
RE_beta_results_table[7,2] <- (6 * 0.15) / 50
RE_beta_results_table[8,2] <- re_pop50_seed1$beta[1]
sorted <- sort(re_pop50_seed1$beta[2:1001], decreasing=FALSE)
RE_beta_results_table[8,3] <- sorted[25]
RE_beta_results_table[8,4] <- sorted[975]
RE_beta_results_table[9,2] <- re_pop50_seed2$beta[1]
sorted <- sort(re_pop50_seed2$beta[2:1001], decreasing=FALSE)
RE_beta_results_table[9,3] <- sorted[25]
RE_beta_results_table[9,4] <- sorted[975]
RE_beta_results_table[10,2] <- re_pop50_seed3$beta[1]
sorted <- sort(re_pop50_seed3$beta[2:1001], decreasing=FALSE)
RE_beta_results_table[10,3] <- sorted[25]
RE_beta_results_table[10,4] <- sorted[975]
RE_beta_results_table[11,2] <- re_pop50_seed4$beta[1]
sorted <- sort(re_pop50_seed4$beta[2:1001], decreasing=FALSE)
RE_beta_results_table[11,3] <- sorted[25]
RE_beta_results_table[11,4] <- sorted[975]
RE_beta_results_table[12,2] <- re_pop50_seed5$beta[1]
sorted <- sort(re_pop50_seed5$beta[2:1001], decreasing=FALSE)
RE_beta_results_table[12,3] <- sorted[25]
RE_beta_results_table[12,4] <- sorted[975]
# Pop = 200, R0 = 6
RE_beta_results_table[7,5] <- (6 * 0.15) / 200
RE_beta_results_table[8,5] <- re_pop200_seed1$beta[1]
sorted <- sort(re_pop200_seed1$beta[2:1001], decreasing=FALSE)
RE_beta_results_table[8,6] <- sorted[25]
RE_beta_results_table[8,7] <- sorted[975]
RE_beta_results_table[9,5] <- re_pop200_seed2$beta[1]
sorted <- sort(re_pop200_seed2$beta[2:1001], decreasing=FALSE)
RE_beta_results_table[9,6] <- sorted[25]
RE_beta_results_table[9,7] <- sorted[975]
RE_beta_results_table[10,5] <- re_pop200_seed3$beta[1]
sorted <- sort(re_pop200_seed3$beta[2:1001], decreasing=FALSE)
RE_beta_results_table[10,6] <- sorted[25]
RE_beta_results_table[10,7] <- sorted[975]
RE_beta_results_table[11,5] <- re_pop200_seed4$beta[1]
sorted <- sort(re_pop200_seed4$beta[2:1001], decreasing=FALSE)
RE_beta_results_table[11,6] <- sorted[25]
RE_beta_results_table[11,7] <- sorted[975]
RE_beta_results_table[12,5] <- re_pop200_seed5$beta[1]
sorted <- sort(re_pop200_seed5$beta[2:1001], decreasing=FALSE)
RE_beta_results_table[12,6] <- sorted[25]
RE_beta_results_table[12,7] <- sorted[975]
# Pop = 1000, R0 = 6
RE_beta_results_table[7,8] <- (6 * 0.15) / 1000
RE_beta_results_table[8,8] <- re_pop1000_seed1$beta[1]
sorted <- sort(re_pop1000_seed1$beta[2:1001], decreasing=FALSE)
RE_beta_results_table[8,9] <- sorted[25]
RE_beta_results_table[8,10] <- sorted[975]
RE_beta_results_table[9,8] <- re_pop1000_seed2$beta[1]
sorted <- sort(re_pop1000_seed2$beta[2:1001], decreasing=FALSE)
RE_beta_results_table[9,9] <- sorted[25]
RE_beta_results_table[9,10] <- sorted[975]
RE_beta_results_table[10,8] <- re_pop1000_seed3$beta[1]
sorted <- sort(re_pop1000_seed3$beta[2:1001], decreasing=FALSE)
RE_beta_results_table[10,9] <- sorted[25]
RE_beta_results_table[10,10] <- sorted[975]
RE_beta_results_table[11,8] <- re_pop1000_seed4$beta[1]
sorted <- sort(re_pop1000_seed4$beta[2:1001], decreasing=FALSE)
RE_beta_results_table[11,9] <- sorted[25]
RE_beta_results_table[11,10] <- sorted[975]
RE_beta_results_table[12,8] <- re_pop1000_seed5$beta[1]
sorted <- sort(re_pop1000_seed5$beta[2:1001], decreasing=FALSE)
RE_beta_results_table[12,9] <- sorted[25]
RE_beta_results_table[12,10] <- sorted[975]

##############################
# Making Residual Error table
library('xtable')
# options(xtable.floating = FALSE)
# options(xtable.timestamp = "")

rownames(RE_beta_results_table) <- c("1.5", ".", "..", "...", "....", ".....","6", "-", "--", "---", "----", "-----")

addtorow <- list()
addtorow$pos <- list(0, 0)
addtorow$command <- c("& \\multicolumn{1}{l}{ }  & \\multicolumn{3}{l}{N = 50}  & \\multicolumn{3}{l}{N = 200} & \\multicolumn{3}{l}{N = 1000} \\\\\n", "$R_{0}$ & sim  & PE & min & max & PE & min & max & PE & min & max \\\\\n")

tab <- xtable(RE_beta_results_table, digits= c(1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1), table.placement="!h", caption = "$\\beta$ values for the RE inference method for each outbreak of a given population size N and $R_{0}$ of 1.5 or 6. PE: point estimate; min: lower bound of 95\\% interval obtained through bootstrapping, max: upper bound of 95\\% interval obtained through bootstrapping") 
align(tab) <- "lllllllllll"
print(tab, hline.after=c(-1,0,12), comment = FALSE, math.style.exponents = FALSE, caption.placement = "top", add.to.row = addtorow, include.colnames = FALSE, type="latex")
@
<<RE_result_table_gamma, cache=FALSE, echo=FALSE, eval=FALSE, results=tex>>=
# Making Residual Error table
RE_results_table <- array(NA, dim =c(12,10))

# setwd("/home/evelina/OneDrive/MRes_BMR/Project_1/Work_folder/Data/server_data")
dir <- "/home/evelina/OneDrive/MRes_BMR/Project_1/Work_folder/Data/server_data/"

re_pop50_seed1 <- read.csv(paste(dir, "re_pop50_R1.5_g0.15_14.csv", sep=""))
re_pop50_seed2 <- read.csv(paste(dir, "re_pop50_R1.5_g0.15_22.csv", sep=""))
re_pop50_seed3 <- read.csv(paste(dir, "re_pop50_R1.5_g0.15_40.csv", sep=""))
re_pop50_seed4 <- read.csv(paste(dir, "re_pop50_R1.5_g0.15_43.csv", sep=""))
re_pop50_seed5 <- read.csv(paste(dir, "re_pop50_R1.5_g0.15_95.csv", sep=""))
re_pop200_seed1 <- read.csv(paste(dir, "re_pop200_R1.5_g0.15_81.csv", sep=""))
re_pop200_seed2 <- read.csv(paste(dir, "re_pop200_R1.5_g0.15_93.csv", sep=""))
re_pop200_seed3 <- read.csv(paste(dir, "re_pop200_R1.5_g0.15_102.csv", sep=""))
re_pop200_seed4 <- read.csv(paste(dir, "re_pop200_R1.5_g0.15_202.csv", sep =""))
re_pop200_seed5 <- read.csv(paste(dir, "re_pop200_R1.5_g0.15_246.csv", sep =""))
re_pop1000_seed1 <- read.csv(paste(dir, "re_pop1000_R1.5_g0.15_177.csv", sep =""))
re_pop1000_seed2 <- read.csv(paste(dir, "re_pop1000_R1.5_g0.15_246.csv", sep =""))
re_pop1000_seed3 <- read.csv(paste(dir, "re_pop1000_R1.5_g0.15_469.csv", sep =""))
re_pop1000_seed4 <- read.csv(paste(dir, "re_pop1000_R1.5_g0.15_520.csv", sep =""))
re_pop1000_seed5 <- read.csv(paste(dir,"re_pop1000_R1.5_g0.15_784.csv", sep =""))
gamma <- 0.15

# Pop = 50, R0 = 1.5
RE_results_table[1,1] <- 0
RE_results_table[2,1] <- 1
RE_results_table[3,1] <- 2
RE_results_table[4,1] <- 3
RE_results_table[5,1] <- 4
RE_results_table[6,1] <- 5
RE_results_table[1,2] <- gamma
RE_results_table[2,2] <- re_pop50_seed1$gamma[1]
sorted <- sort(re_pop50_seed1$gamma[2:1001], decreasing=FALSE)
RE_results_table[2,3] <- sorted[25]
RE_results_table[2,4] <- sorted[975]
RE_results_table[3,2] <- re_pop50_seed2$gamma[1]
sorted <- sort(re_pop50_seed2$gamma[2:1001], decreasing=FALSE)
RE_results_table[3,3] <- sorted[25]
RE_results_table[3,4] <- sorted[975]
RE_results_table[4,2] <- re_pop50_seed3$gamma[1]
sorted <- sort(re_pop50_seed3$gamma[2:1001], decreasing=FALSE)
RE_results_table[4,3] <- sorted[25]
RE_results_table[4,4] <- sorted[975]
RE_results_table[5,2] <- re_pop50_seed4$gamma[1]
sorted <- sort(re_pop50_seed4$gamma[2:1001], decreasing=FALSE)
RE_results_table[5,3] <- sorted[25]
RE_results_table[5,4] <- sorted[975]
RE_results_table[6,2] <- re_pop50_seed5$gamma[1]
sorted <- sort(re_pop50_seed5$gamma[2:1001], decreasing=FALSE)
RE_results_table[6,3] <- sorted[25]
RE_results_table[6,4] <- sorted[975]
# Pop = 200, R0 = 1.5
RE_results_table[1,5] <- gamma
RE_results_table[2,5] <- re_pop200_seed1$gamma[1]
sorted <- sort(re_pop200_seed1$gamma[2:1001], decreasing=FALSE)
RE_results_table[2,6] <- sorted[25]
RE_results_table[2,7] <- sorted[975]
RE_results_table[3,5] <- re_pop200_seed2$gamma[1]
sorted <- sort(re_pop200_seed2$gamma[2:1001], decreasing=FALSE)
RE_results_table[3,6] <- sorted[25]
RE_results_table[3,7] <- sorted[975]
RE_results_table[4,5] <- re_pop200_seed3$gamma[1]
sorted <- sort(re_pop200_seed3$gamma[2:1001], decreasing=FALSE)
RE_results_table[4,6] <- sorted[25]
RE_results_table[4,7] <- sorted[975]
RE_results_table[5,5] <- re_pop200_seed4$gamma[1]
sorted <- sort(re_pop200_seed4$gamma[2:1001], decreasing=FALSE)
RE_results_table[5,6] <- sorted[25]
RE_results_table[5,7] <- sorted[975]
RE_results_table[6,5] <- re_pop200_seed5$gamma[1]
sorted <- sort(re_pop200_seed5$gamma[2:1001], decreasing=FALSE)
RE_results_table[6,6] <- sorted[25]
RE_results_table[6,7] <- sorted[975]
# Pop = 1000, R0 = 1.5
RE_results_table[1,8] <- gamma
RE_results_table[2,8] <- re_pop1000_seed1$gamma[1]
sorted <- sort(re_pop1000_seed1$gamma[2:1001], decreasing=FALSE)
RE_results_table[2,9] <- sorted[25]
RE_results_table[2,10] <- sorted[975]
RE_results_table[3,8] <- re_pop1000_seed2$gamma[1]
sorted <- sort(re_pop1000_seed2$gamma[2:1001], decreasing=FALSE)
RE_results_table[3,9] <- sorted[25]
RE_results_table[3,10] <- sorted[975]
RE_results_table[4,8] <- re_pop1000_seed3$gamma[1]
sorted <- sort(re_pop1000_seed3$gamma[2:1001], decreasing=FALSE)
RE_results_table[4,9] <- sorted[25]
RE_results_table[4,10] <- sorted[975]
RE_results_table[5,8] <- re_pop1000_seed4$gamma[1]
sorted <- sort(re_pop1000_seed4$gamma[2:1001], decreasing=FALSE)
RE_results_table[5,9] <- sorted[25]
RE_results_table[5,10] <- sorted[975]
RE_results_table[6,8] <- re_pop1000_seed5$gamma[1]
sorted <- sort(re_pop1000_seed5$gamma[2:1001], decreasing=FALSE)
RE_results_table[6,9] <- sorted[25]
RE_results_table[6,10] <- sorted[975]

# R0 = 6 Gamma = 0.15
re_pop50_seed1 <- read.csv(paste(dir, "re_pop50_R6_g0.15_1.csv", sep =""))
re_pop50_seed2 <- read.csv(paste(dir, "re_pop50_R6_g0.15_2.csv", sep =""))
re_pop50_seed3 <- read.csv(paste(dir, "re_pop50_R6_g0.15_3.csv", sep =""))
re_pop50_seed4 <- read.csv(paste(dir, "re_pop50_R6_g0.15_4.csv", sep =""))
re_pop50_seed5 <- read.csv(paste(dir, "re_pop50_R6_g0.15_5.csv", sep =""))
re_pop200_seed1 <- read.csv(paste(dir, "re_pop200_R6_g0.15_6.csv", sep =""))
re_pop200_seed2 <- read.csv(paste(dir, "re_pop200_R6_g0.15_7.csv", sep =""))
re_pop200_seed3 <- read.csv(paste(dir, "re_pop200_R6_g0.15_8.csv", sep =""))
re_pop200_seed4 <- read.csv(paste(dir, "re_pop200_R6_g0.15_9.csv", sep =""))
re_pop200_seed5 <- read.csv(paste(dir, "re_pop200_R6_g0.15_10.csv", sep =""))
re_pop1000_seed1 <- read.csv(paste(dir, "re_pop1000_R6_g0.15_11.csv", sep =""))
re_pop1000_seed2 <- read.csv(paste(dir, "re_pop1000_R6_g0.15_12.csv", sep =""))
re_pop1000_seed3 <- read.csv(paste(dir, "re_pop1000_R6_g0.15_13.csv", sep =""))
re_pop1000_seed4 <- read.csv(paste(dir, "re_pop1000_R6_g0.15_14.csv", sep =""))
re_pop1000_seed5 <- read.csv(paste(dir, "re_pop1000_R6_g0.15_15.csv", sep =""))

# Pop = 50, R0 = 6
RE_results_table[7,1] <- 0
RE_results_table[8,1] <- 1
RE_results_table[9,1] <- 2
RE_results_table[10,1] <- 3
RE_results_table[11,1] <- 4
RE_results_table[12,1] <- 5
RE_results_table[7,2] <- gamma
RE_results_table[8,2] <- re_pop50_seed1$gamma[1]
sorted <- sort(re_pop50_seed1$gamma[2:1001], decreasing=FALSE)
RE_results_table[8,3] <- sorted[25]
RE_results_table[8,4] <- sorted[975]
RE_results_table[9,2] <- re_pop50_seed2$gamma[1]
sorted <- sort(re_pop50_seed2$gamma[2:1001], decreasing=FALSE)
RE_results_table[9,3] <- sorted[25]
RE_results_table[9,4] <- sorted[975]
RE_results_table[10,2] <- re_pop50_seed3$gamma[1]
sorted <- sort(re_pop50_seed3$gamma[2:1001], decreasing=FALSE)
RE_results_table[10,3] <- sorted[25]
RE_results_table[10,4] <- sorted[975]
RE_results_table[11,2] <- re_pop50_seed4$gamma[1]
sorted <- sort(re_pop50_seed4$gamma[2:1001], decreasing=FALSE)
RE_results_table[11,3] <- sorted[25]
RE_results_table[11,4] <- sorted[975]
RE_results_table[12,2] <- re_pop50_seed5$gamma[1]
sorted <- sort(re_pop50_seed5$gamma[2:1001], decreasing=FALSE)
RE_results_table[12,3] <- sorted[25]
RE_results_table[12,4] <- sorted[975]
# Pop = 200, R0 = 1.5
RE_results_table[7,5] <- gamma
RE_results_table[8,5] <- re_pop200_seed1$gamma[1]
sorted <- sort(re_pop200_seed1$gamma[2:1001], decreasing=FALSE)
RE_results_table[8,6] <- sorted[25]
RE_results_table[8,7] <- sorted[975]
RE_results_table[9,5] <- re_pop200_seed2$gamma[1]
sorted <- sort(re_pop200_seed2$gamma[2:1001], decreasing=FALSE)
RE_results_table[9,6] <- sorted[25]
RE_results_table[9,7] <- sorted[975]
RE_results_table[10,5] <- re_pop200_seed3$gamma[1]
sorted <- sort(re_pop200_seed3$gamma[2:1001], decreasing=FALSE)
RE_results_table[10,6] <- sorted[25]
RE_results_table[10,7] <- sorted[975]
RE_results_table[11,5] <- re_pop200_seed4$gamma[1]
sorted <- sort(re_pop200_seed4$gamma[2:1001], decreasing=FALSE)
RE_results_table[11,6] <- sorted[25]
RE_results_table[11,7] <- sorted[975]
RE_results_table[12,5] <- re_pop200_seed5$gamma[1]
sorted <- sort(re_pop200_seed5$gamma[2:1001], decreasing=FALSE)
RE_results_table[12,6] <- sorted[25]
RE_results_table[12,7] <- sorted[975]
# Pop = 1000, R0 = 1.5
RE_results_table[7,8] <- gamma
RE_results_table[8,8] <- re_pop1000_seed1$gamma[1]
sorted <- sort(re_pop1000_seed1$gamma[2:1001], decreasing=FALSE)
RE_results_table[8,9] <- sorted[25]
RE_results_table[8,10] <- sorted[975]
RE_results_table[9,8] <- re_pop1000_seed2$gamma[1]
sorted <- sort(re_pop1000_seed2$gamma[2:1001], decreasing=FALSE)
RE_results_table[9,9] <- sorted[25]
RE_results_table[9,10] <- sorted[975]
RE_results_table[10,8] <- re_pop1000_seed3$gamma[1]
sorted <- sort(re_pop1000_seed3$gamma[2:1001], decreasing=FALSE)
RE_results_table[10,9] <- sorted[25]
RE_results_table[10,10] <- sorted[975]
RE_results_table[11,8] <- re_pop1000_seed4$gamma[1]
sorted <- sort(re_pop1000_seed4$gamma[2:1001], decreasing=FALSE)
RE_results_table[11,9] <- sorted[25]
RE_results_table[11,10] <- sorted[975]
RE_results_table[12,8] <- re_pop1000_seed5$gamma[1]
sorted <- sort(re_pop1000_seed5$gamma[2:1001], decreasing=FALSE)
RE_results_table[12,9] <- sorted[25]
RE_results_table[12,10] <- sorted[975]

##############################
# Making Residual Error table
library('xtable')
# options(xtable.floating = FALSE)
# options(xtable.timestamp = "")

rownames(RE_results_table) <- c("1.5", ".","..","...","....", ".....","6","-","--","---","----", "-----")

addtorow <- list()
addtorow$pos <- list(0, 0)
addtorow$command <- c("& \\multicolumn{1}{c}{ }  & \\multicolumn{3}{l}{N = 50}  & \\multicolumn{3}{l}{N = 200} & \\multicolumn{3}{l}{N = 1000} \\\\\n", "$R_{0}$ & sim & PE & min & max & PE & min & max & PE & min & max \\\\\n")

tab <- xtable(RE_results_table, table.placement="H", digits = c(1, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2), caption = "$\\gamma$ values for the RE inference method for each outbreak of a given population size N and $R_{0}$ of 1.5 or 6. PE: point estimate; min: minimum value obtained through bootstrapping, max: maximum obtained through bootstrapping") 

align(tab) <- "lllllllllll"
print(tab, hline.after=c(-1, 0, 12), comment = FALSE, caption.placement = "top", add.to.row = addtorow, include.colnames = FALSE, type="latex")
@
% When $R_{0}$ is 6, the inferred 95\% interval for $\gamma$ includes the true value of 0.15 for all five simulations for the population size of 1000, and twice respectively for the population sizes of 50 and 200 (Table \ref{re_pop200}). For the smaller $R_{0}$ of 1.5, the true $\gamma$ value is included in the 95\% interval three times for the population size of 1000, once for the 200 member population, and zero times for the population size of 50 (Table \ref{re_pop200}). Whether the $\gamma$ ranges that did not include the true $\gamma$ value are over- or underestimating can vary even within a given $R_{0}$ and population size. For the population size of 50 for both $R_{0}$s, the 95\% intervals both over- and underestimate the ranges of values $\gamma$ may take. For the population size of 200, $\gamma$ is always overestimated when $R_{0}$ is 6, but is both over- and underestimated when $R_{0}$ is 1.5. When $R_{0}$ is 1.5 and the population size is 1000, the $\gamma$ values are overestimated.   

The heatmap in Figure \ref{re_heatmap} shows the REs for different combinations of $\beta$ and $\gamma$ when optimising to the recovery curve of simulation 1 with population size 200 and $R_{0}$ of 1.5. The trend suggests that $\beta$ and $\gamma$ are positively correlated, implying that the REs remain lower when both $\beta$ and $\gamma$ values are either lowered or increased together. Similar trends in the relationship between $\beta$ and $\gamma$ were observed for the other 29 outbreaks.
\newline

\begin{figure}[!h]
\begin{center}
\includegraphics[width=\textwidth]{RE_heatmap_pop200}
\caption{Heatmap showing the residual error for different $\beta$, per capita rate of infection, and $\gamma$, recovery rate, combinations when evaluated for simulation 1 of population size 200 and $R_{0}$ of 1.5.}
\label{re_heatmap}
\end{center}
\end{figure}
<<heatmap, cache=TRUE, echo=FALSE, eval=FALSE, dpi=100, height=13, width=17, fig.cap="Data plot", fig=TRUE>>=
library("plotly") #package for solving differential equations

dir <- "/home/evelina/OneDrive/MRes_BMR/Project_1/Work_folder/Data/"

heatmap <- read.csv(paste(dir, "re_pop200_R1.5_g0.15_81_heatmap.csv", sep=""))
for (i in 1:nrow(heatmap)){
  if (heatmap[i,3] > 5000){
    heatmap[i,3] = 5000
  }
}

matrix_heatmap <- xtabs(RE~beta+gamma, data=heatmap)

beta <- seq(min(heatmap$beta), max(heatmap$beta), by = ((max(heatmap$beta)-min(heatmap$beta))/nrow(matrix_heatmap)))
gamma <- seq(min(heatmap$gamma), max(heatmap$gamma), by = ((max(heatmap$gamma)-min(heatmap$gamma))/nrow(matrix_heatmap)))

tick_beta <- list(
  autotick = FALSE,
  ticks = "outside",
  tick0 = min(heatmap$beta),
  dtick = ((max(heatmap$beta)-min(heatmap$beta))/nrow(matrix_heatmap)),
  ticklen = 5,
  tickwidth = 2,
  tickcolor = toRGB("blue")
)

tick_gamma <- list(
  autotick = FALSE,
  ticks = "outside",
  tick0 = min(heatmap$gamma),
  dtick = ((max(heatmap$gamma)-min(heatmap$gamma))/nrow(matrix_heatmap)),
  tickangle = 45,
  ticklen = 5,
  tickwidth = 2,
  tickcolor = toRGB("blue")
)

m <- list(
  l = 100,
  r = 5,
  b = 80,
  t = 5,
  pad = 4
)

plot_heatmap <- plot_ly(z = matrix_heatmap, x = ~gamma, y = ~beta, colors = colorRamp(c("yellow","darkorange2","orangered","red","maroon","magenta4","blue", "navy", "midnightblue")), type = "heatmap") %>%
  layout(xaxis = tick_gamma, yaxis = tick_beta, margin = m)
# plot_heatmap
heatmap_png <- export(plot_heatmap, file = "plot_heatmap.png")
heatmap_png
@

\FloatBarrier
%%%%%%%%%%
\clearpage
\newpage
\subsection{Markov Chain Monte Carlo}

A MCMC approach was also taken to infer the values that $\beta$ and $\gamma$ might take when given the recovery curve from the simulated outbreaks. The MCMC with 3.5 million iterations took between 20 and 25 hours to run to completion while the MCMC with 17.5 iterations took between 120 and 168 hours (5 to 7 days) to run to completion, with outbreaks with an $R_{0}$ of 1.5 taking longer to finish than outbreaks with an $R_{0}$ of 6. The time taken for the MCMC to finish also increased with population size.

For the outbreaks where population size was 50 and the MCMC was run for 3.5 million iterations, the post-burn-in $\beta$ 95\% credible intervals include the true value for all outbreaks, excepting for simulation 5 from the outbreaks with an $R_{0}$ of 1.5 remained below the true $\gamma$ value (Table \ref{mcmc_pop50}). The 95\% credible intervals for $\beta$ and $\gamma$ for outbreaks where $R_{0}$ is 6 all include the true values. 
\newline

<<MCMC_result_table_50, cache=FALSE, echo=FALSE, eval=TRUE, results=tex>>=
# Making MCMC table
burnIn <- 3500 * 0.23
results_table <- array(NA, dim =c(12,7))

# setwd("/home/evelina/OneDrive/MRes_BMR/Project_1/Work_folder/Data/server_data")
dir <- "/home/evelina/OneDrive/MRes_BMR/Project_1/Work_folder/Data/server_data/"

seed1 <- read.csv(paste(dir, "mcmc_pop50_R1.5_g0.15_14_loglik.csv", sep=""))
seed2 <- read.csv(paste(dir, "mcmc_pop50_R1.5_g0.15_22_loglik.csv", sep=""))
seed3 <- read.csv(paste(dir, "mcmc_pop50_R1.5_g0.15_40_loglik.csv", sep=""))
seed4 <- read.csv(paste(dir, "mcmc_pop50_R1.5_g0.15_43_loglik.csv", sep=""))
seed5 <- read.csv(paste(dir, "mcmc_pop50_R1.5_g0.15_95_loglik.csv", sep=""))

# results_table[1,1] <- "ref"
results_table[2,1] <- 1
results_table[3,1] <- 2
results_table[4,1] <- 3
results_table[5,1] <- 4
results_table[6,1] <- 5

# results_table[7,1] <- "ref"
results_table[8,1] <- 1
results_table[9,1] <- 2
results_table[10,1] <- 3
results_table[11,1] <- 4
results_table[12,1] <- 5

# Pop = 1000, R0 = 1.5
results_table[1,2] <- (1.5 * 0.15) / 50
results_table[2,2] <- mean(seed1$beta)
intervals <- quantile(seed1$beta[-(1:burnIn)], c(0.025, 0.975))
results_table[2,3] <- intervals[1]
results_table[2,4] <- intervals[2]
results_table[3,2] <- mean(seed2$beta[1])
intervals <- quantile(seed2$beta[-(1:burnIn)], c(0.025, 0.975))
results_table[3,3] <- intervals[1]
results_table[3,4] <- intervals[2]
results_table[4,2] <- mean(seed3$beta[1])
intervals <- quantile(seed3$beta[-(1:burnIn)], c(0.025, 0.975))
results_table[4,3] <- intervals[1]
results_table[4,4] <- intervals[2]
results_table[5,2] <- mean(seed4$beta[1])
intervals <- quantile(seed4$beta[-(1:burnIn)], c(0.025, 0.975))
results_table[5,3] <- intervals[1]
results_table[5,4] <- intervals[2]
results_table[6,2] <- mean(seed5$beta[1])
intervals <- quantile(seed5$beta[-(1:burnIn)], c(0.025, 0.975))
results_table[6,3] <- intervals[1]
results_table[6,4] <- intervals[2]

# Pop = 1000, R0 = 1.5
results_table[1,5] <- 0.15
results_table[2,5] <- mean(seed1$gamma)
intervals <- quantile(seed1$gamma[-(1:burnIn)], c(0.025, 0.975))
results_table[2,6] <- intervals[1]
results_table[2,7] <- intervals[2]
results_table[3,5] <- mean(seed2$gamma)
intervals <- quantile(seed2$gamma[-(1:burnIn)], c(0.025, 0.975))
results_table[3,6] <- intervals[1]
results_table[3,7] <- intervals[2]
results_table[4,5] <- mean(seed3$gamma)
intervals <- quantile(seed3$gamma[-(1:burnIn)], c(0.025, 0.975))
results_table[4,6] <- intervals[1]
results_table[4,7] <- intervals[2]
results_table[5,5] <- mean(seed4$gamma)
intervals <- quantile(seed4$gamma[-(1:burnIn)], c(0.025, 0.975))
results_table[5,6] <- intervals[1]
results_table[5,7] <- intervals[2]
results_table[6,5] <- mean(seed5$gamma)
intervals <- quantile(seed5$gamma[-(1:burnIn)], c(0.025, 0.975))
results_table[6,6] <- intervals[1]
results_table[6,7] <- intervals[2]

# R0 = 6 Gamma = 0.15
seed2 <- read.csv(paste(dir, "mcmc_pop50_R6_g0.15_1_loglik.csv", sep =""))
seed1 <- read.csv(paste(dir, "mcmc_pop50_R6_g0.15_2_loglik.csv", sep =""))
seed3 <- read.csv(paste(dir, "mcmc_pop50_R6_g0.15_3_loglik.csv", sep =""))
seed4 <- read.csv(paste(dir, "mcmc_pop50_R6_g0.15_4_loglik.csv", sep =""))
seed5 <- read.csv(paste(dir, "mcmc_pop50_R6_g0.15_5_loglik.csv", sep =""))

# Pop = 1000, R0 = 6
results_table[7,2] <- (6 * 0.15) / 50
results_table[8,2] <- mean(seed1$beta)
intervals <- quantile(seed1$beta[-(1:burnIn)], c(0.025, 0.975))
results_table[8,3] <- intervals[1]
results_table[8,4] <- intervals[2]
results_table[9,2] <- mean(seed2$beta)
intervals <- quantile(seed2$beta[-(1:burnIn)], c(0.025, 0.975))
results_table[9,3] <- intervals[1]
results_table[9,4] <- intervals[2]
results_table[10,2] <- mean(seed3$beta)
intervals <- quantile(seed3$beta[-(1:burnIn)], c(0.025, 0.975))
results_table[10,3] <- intervals[1]
results_table[10,4] <- intervals[2]
results_table[11,2] <- mean(seed4$beta)
intervals <- quantile(seed4$beta[-(1:burnIn)], c(0.025, 0.975))
results_table[11,3] <- intervals[1]
results_table[11,4] <- intervals[2]
results_table[12,2] <- mean(seed5$beta)
intervals <- quantile(seed5$beta[-(1:burnIn)], c(0.025, 0.975))
results_table[12,3] <- intervals[1]
results_table[12,4] <- intervals[2]

# Pop = 1000, R0 = 6
results_table[7,5] <- 0.15
results_table[8,5] <- mean(seed1$gamma)
intervals <- quantile(seed1$gamma[-(1:burnIn)], c(0.025, 0.975))
results_table[8,6] <- intervals[1]
results_table[8,7] <- intervals[2]
results_table[9,5] <- mean(seed2$gamma)
intervals <- quantile(seed2$gamma[-(1:burnIn)], c(0.025, 0.975))
results_table[9,6] <- intervals[1]
results_table[9,7] <- intervals[2]
results_table[10,5] <- mean(seed3$gamma)
intervals <- quantile(seed3$gamma[-(1:burnIn)], c(0.025, 0.975))
results_table[10,6] <- intervals[1]
results_table[10,7] <- intervals[2]
results_table[11,5] <- mean(seed4$gamma)
intervals <- quantile(seed4$gamma[-(1:burnIn)], c(0.025, 0.975))
results_table[11,6] <- intervals[1]
results_table[11,7] <- intervals[2]
results_table[12,5] <- mean(seed5$gamma)
intervals <- quantile(seed5$gamma[-(1:burnIn)], c(0.025, 0.975))
results_table[12,6] <- intervals[1]
results_table[12,7] <- intervals[2]

##############################
# Making Residual Error table
library('xtable')
# options(xtable.floating = FALSE)
# options(xtable.timestamp = "")

rownames(results_table) <- c("1.5", "\\textcolor{white}{a}","\\textcolor{white}{b}","\\textcolor{white}{c}","\\textcolor{white}{d}","\\textcolor{white}{e}","6","\\textcolor{white}{f}","\\textcolor{white}{g}","\\textcolor{white}{h}","\\textcolor{white}{i}","\\textcolor{white}{j}")

addtorow <- list()
addtorow$pos <- list(0, 0)
addtorow$command <- c("& \\multicolumn{1}{l}{} & \\multicolumn{3}{l}{$\\beta$}  & \\multicolumn{3}{l}{$\\gamma$} \\\\\n", "$R_{0}$ & simulation & mean & min & max & mean & min & max \\\\\n")

tab <- xtable(results_table, digits= c(1, 0, -1, -1, -1, 2, 2, 2), caption = "$\\beta$ and $\\gamma$ values for the 3.5 million iteration MCMC inference method  $R_{0}$ of 1.5 or 6 for a population size of 50. The numbers presented without 95\\% credible intervals are the true $\\beta$ and $\\gamma$ for each $R_{0}$. min: lower bound of 95\\% credible interval, max: upper bound of 95\\% credible interval", label = "mcmc_pop50") 
align(tab) <- "lXXXXXXX"
print(tab, hline.after=c(-1, 0, 12), comment = FALSE, math.style.exponents = FALSE, caption.placement = "top", add.to.row = addtorow, include.colnames = FALSE, type="latex", sanitize.rownames.function = identity, tabular.environment = "tabularx", width = "\\textwidth")
@
\FloatBarrier
\clearpage
The 95\% credible intervals for $\beta$ and $\gamma$ for outbreaks with a population size of 200 and 3.5 million MCMC iterations all include the true values, except for the interval for $\beta$ in simulation 4 for the outbreaks where $R_{0}$ is 1.5 (Table \ref{mcmc_pop200}).
\newline

<<MCMC_result_table_200, cache=FALSE, echo=FALSE, eval=TRUE, results=tex>>=
# Making MCMC table
burnIn <- 3500 * 0.23
results_table <- array(NA, dim =c(12,7))

# setwd("/home/evelina/OneDrive/MRes_BMR/Project_1/Work_folder/Data/server_data")
dir <- "/home/evelina/OneDrive/MRes_BMR/Project_1/Work_folder/Data/server_data/"

seed1 <- read.csv(paste(dir, "mcmc_pop200_R1.5_g0.15_81_loglik.csv", sep=""))
seed2 <- read.csv(paste(dir, "mcmc_pop200_R1.5_g0.15_93_loglik.csv", sep=""))
seed3 <- read.csv(paste(dir, "mcmc_pop200_R1.5_g0.15_102_loglik.csv", sep=""))
seed4 <- read.csv(paste(dir, "mcmc_pop200_R1.5_g0.15_202_loglik.csv", sep=""))
seed5 <- read.csv(paste(dir, "mcmc_pop200_R1.5_g0.15_246_loglik.csv", sep=""))

# results_table[1,1] <- "ref"
results_table[2,1] <- 1
results_table[3,1] <- 2
results_table[4,1] <- 3
results_table[5,1] <- 4
results_table[6,1] <- 5

# results_table[7,1] <- "ref"
results_table[8,1] <- 1
results_table[9,1] <- 2
results_table[10,1] <- 3
results_table[11,1] <- 4
results_table[12,1] <- 5

# Pop = 1000, R0 = 1.5
results_table[1,2] <- (1.5 * 0.15) / 200
results_table[2,2] <- mean(seed1$beta)
intervals <- quantile(seed1$beta[-(1:burnIn)], c(0.025, 0.975))
results_table[2,3] <- intervals[1]
results_table[2,4] <- intervals[2]
results_table[3,2] <- mean(seed2$beta)
intervals <- quantile(seed2$beta[-(1:burnIn)], c(0.025, 0.975))
results_table[3,3] <- intervals[1]
results_table[3,4] <- intervals[2]
results_table[4,2] <- mean(seed3$beta)
intervals <- quantile(seed3$beta[-(1:burnIn)], c(0.025, 0.975))
results_table[4,3] <- intervals[1]
results_table[4,4] <- intervals[2]
results_table[5,2] <- mean(seed4$beta)
intervals <- quantile(seed4$beta[-(1:burnIn)], c(0.025, 0.975))
results_table[5,3] <- intervals[1]
results_table[5,4] <- intervals[2]
results_table[6,2] <- mean(seed5$beta)
intervals <- quantile(seed5$beta[-(1:burnIn)], c(0.025, 0.975))
results_table[6,3] <- intervals[1]
results_table[6,4] <- intervals[2]

# Pop = 1000, R0 = 1.5
results_table[1,5] <- 0.15
results_table[2,5] <- mean(seed1$gamma)
intervals <- quantile(seed1$gamma[-(1:burnIn)], c(0.025, 0.975))
results_table[2,6] <- intervals[1]
results_table[2,7] <- intervals[2]
results_table[3,5] <- mean(seed2$gamma)
intervals <- quantile(seed2$gamma[-(1:burnIn)], c(0.025, 0.975))
results_table[3,6] <- intervals[1]
results_table[3,7] <- intervals[2]
results_table[4,5] <- mean(seed3$gamma)
intervals <- quantile(seed3$gamma[-(1:burnIn)], c(0.025, 0.975))
results_table[4,6] <- intervals[1]
results_table[4,7] <- intervals[2]
results_table[5,5] <- mean(seed4$gamma)
intervals <- quantile(seed4$gamma[-(1:burnIn)], c(0.025, 0.975))
results_table[5,6] <- intervals[1]
results_table[5,7] <- intervals[2]
results_table[6,5] <- mean(seed5$gamma)
intervals <- quantile(seed5$gamma[-(1:burnIn)], c(0.025, 0.975))
results_table[6,6] <- intervals[1]
results_table[6,7] <- intervals[2]

# R0 = 6 Gamma = 0.15
seed1 <- read.csv(paste(dir, "mcmc_pop200_R6_g0.15_6_loglik.csv", sep =""))
seed2 <- read.csv(paste(dir, "mcmc_pop200_R6_g0.15_7_loglik.csv", sep =""))
seed3 <- read.csv(paste(dir, "mcmc_pop200_R6_g0.15_8_loglik.csv", sep =""))
seed4 <- read.csv(paste(dir, "mcmc_pop200_R6_g0.15_9_loglik.csv", sep =""))
seed5 <- read.csv(paste(dir, "mcmc_pop200_R6_g0.15_10_loglik.csv", sep =""))

# Pop = 1000, R0 = 6
results_table[7,2] <- (6 * 0.15) / 200
results_table[8,2] <- mean(seed1$beta)
intervals <- quantile(seed1$beta[-(1:burnIn)], c(0.025, 0.975))
results_table[8,3] <- intervals[1]
results_table[8,4] <- intervals[2]
results_table[9,2] <- mean(seed2$beta)
intervals <- quantile(seed2$beta[-(1:burnIn)], c(0.025, 0.975))
results_table[9,3] <- intervals[1]
results_table[9,4] <- intervals[2]
results_table[10,2] <- mean(seed3$beta)
intervals <- quantile(seed3$beta[-(1:burnIn)], c(0.025, 0.975))
results_table[10,3] <- intervals[1]
results_table[10,4] <- intervals[2]
results_table[11,2] <- mean(seed4$beta)
intervals <- quantile(seed4$beta[-(1:burnIn)], c(0.025, 0.975))
results_table[11,3] <- intervals[1]
results_table[11,4] <- intervals[2]
results_table[12,2] <- mean(seed5$beta)
intervals <- quantile(seed5$beta[-(1:burnIn)], c(0.025, 0.975))
results_table[12,3] <- intervals[1]
results_table[12,4] <- intervals[2]

# Pop = 1000, R0 = 6
results_table[7,5] <- 0.15
results_table[8,5] <- mean(seed1$gamma)
intervals <- quantile(seed1$gamma[-(1:burnIn)], c(0.025, 0.975))
results_table[8,6] <- intervals[1]
results_table[8,7] <- intervals[2]
results_table[9,5] <- mean(seed2$gamma)
intervals <- quantile(seed2$gamma[-(1:burnIn)], c(0.025, 0.975))
results_table[9,6] <- intervals[1]
results_table[9,7] <- intervals[2]
results_table[10,5] <- mean(seed3$gamma)
intervals <- quantile(seed3$gamma[-(1:burnIn)], c(0.025, 0.975))
results_table[10,6] <- intervals[1]
results_table[10,7] <- intervals[2]
results_table[11,5] <- mean(seed4$gamma)
intervals <- quantile(seed4$gamma[-(1:burnIn)], c(0.025, 0.975))
results_table[11,6] <- intervals[1]
results_table[11,7] <- intervals[2]
results_table[12,5] <- mean(seed5$gamma)
intervals <- quantile(seed5$gamma[-(1:burnIn)], c(0.025, 0.975))
results_table[12,6] <- intervals[1]
results_table[12,7] <- intervals[2]

##############################
# Making Residual Error table
library('xtable')
# options(xtable.floating = FALSE)
# options(xtable.timestamp = "")

rownames(results_table) <- c("1.5", "\\textcolor{white}{a}","\\textcolor{white}{b}","\\textcolor{white}{c}","\\textcolor{white}{d}","\\textcolor{white}{e}","6","\\textcolor{white}{f}","\\textcolor{white}{g}","\\textcolor{white}{h}","\\textcolor{white}{i}","\\textcolor{white}{j}")

addtorow <- list()
addtorow$pos <- list(0, 0)
addtorow$command <- c("& \\multicolumn{1}{l}{} & \\multicolumn{3}{l}{$\\beta$}  & \\multicolumn{3}{l}{$\\gamma$} \\\\\n", "$R_{0}$ & simulation & mean & min & max & mean & min & max \\\\\n")

tab <- xtable(results_table, digits= c(1, 0, -1, -1, -1, 2, 2, 2), caption = "$\\beta$ and $\\gamma$ values for the 3.5 million iteration MCMC inference method  $R_{0}$ of 1.5 or 6 for a population size of 200. The numbers presented without 95\\% credible intervals are the true $\\beta$ and $\\gamma$ for each $R_{0}$. min: lower bound of 95\\% credible interval, max: upper bound of 95\\% credible interval", label = "mcmc_pop200") 
align(tab) <- "lXXXXXXX"
print(tab, hline.after=c(-1, 0, 12), comment = FALSE, math.style.exponents = FALSE, caption.placement = "top", add.to.row = addtorow, include.colnames = FALSE, type="latex", sanitize.rownames.function = identity, tabular.environment = "tabularx", width = "\\textwidth")
@

\FloatBarrier
\clearpage

For outbreaks with an $R_{0}$ of 1.5 and a population size of 1000 for the 3.5 million iteration MCMC, the 95\% credible interval for $\beta$ included the true value for all five simulations, but for $\gamma$, simulations 1 and 5 had intervals that were below the true value (Table \ref{mcmc_pop1000}). For outbreaks with an $R_{0}$ of 6, the 95\% credible intervals for $\gamma$ were all below the true value, while the credible intervals for $\beta$ were below the true value for simulations 1, 4, and 5, with the remaining two simulations' credible intervals including the true value (Table \ref{mcmc_pop1000}).
\newline

<<MCMC_result_table_1000, cache=FALSE, echo=FALSE, eval=TRUE, results=tex>>=
# Making MCMC table
burnIn <- 3500 * 0.23
results_table <- array(NA, dim =c(12,7))

# setwd("/home/evelina/OneDrive/MRes_BMR/Project_1/Work_folder/Data/server_data")
dir <- "/home/evelina/OneDrive/MRes_BMR/Project_1/Work_folder/Data/server_data/"

seed1 <- read.csv(paste(dir, "mcmc_pop1000_R1.5_g0.15_177_loglik.csv", sep=""))
seed2 <- read.csv(paste(dir, "mcmc_pop1000_R1.5_g0.15_246_loglik.csv", sep=""))
seed3 <- read.csv(paste(dir, "mcmc_pop1000_R1.5_g0.15_469_loglik.csv", sep=""))
seed4 <- read.csv(paste(dir, "mcmc_pop1000_R1.5_g0.15_520_loglik.csv", sep=""))
seed5 <- read.csv(paste(dir, "mcmc_pop1000_R1.5_g0.15_784_loglik.csv", sep=""))

# results_table[1,1] <- "ref"
results_table[2,1] <- 1
results_table[3,1] <- 2
results_table[4,1] <- 3
results_table[5,1] <- 4
results_table[6,1] <- 5

# results_table[7,1] <- "ref"
results_table[8,1] <- 1
results_table[9,1] <- 2
results_table[10,1] <- 3
results_table[11,1] <- 4
results_table[12,1] <- 5

# Pop = 1000, R0 = 1.5
results_table[1,2] <- (1.5 * 0.15) / 1000
results_table[2,2] <- mean(seed1$beta)
intervals <- quantile(seed1$beta[-(1:burnIn)], c(0.025, 0.975))
results_table[2,3] <- intervals[1]
results_table[2,4] <- intervals[2]
results_table[3,2] <- mean(seed2$beta)
intervals <- quantile(seed2$beta[-(1:burnIn)], c(0.025, 0.975))
results_table[3,3] <- intervals[1]
results_table[3,4] <- intervals[2]
results_table[4,2] <- mean(seed3$beta)
intervals <- quantile(seed3$beta[-(1:burnIn)], c(0.025, 0.975))
results_table[4,3] <- intervals[1]
results_table[4,4] <- intervals[2]
results_table[5,2] <- mean(seed4$beta)
intervals <- quantile(seed4$beta[-(1:burnIn)], c(0.025, 0.975))
results_table[5,3] <- intervals[1]
results_table[5,4] <- intervals[2]
results_table[6,2] <- mean(seed5$beta)
intervals <- quantile(seed5$beta[-(1:burnIn)], c(0.025, 0.975))
results_table[6,3] <- intervals[1]
results_table[6,4] <- intervals[2]

# Pop = 1000, R0 = 1.5
results_table[1,5] <- 0.15
results_table[2,5] <- mean(seed1$gamma)
intervals <- quantile(seed1$gamma[-(1:burnIn)], c(0.025, 0.975))
results_table[2,6] <- intervals[1]
results_table[2,7] <- intervals[2]
results_table[3,5] <- mean(seed2$gamma)
intervals <- quantile(seed2$gamma[-(1:burnIn)], c(0.025, 0.975))
results_table[3,6] <- intervals[1]
results_table[3,7] <- intervals[2]
results_table[4,5] <- mean(seed3$gamma)
intervals <- quantile(seed3$gamma[-(1:burnIn)], c(0.025, 0.975))
results_table[4,6] <- intervals[1]
results_table[4,7] <- intervals[2]
results_table[5,5] <- mean(seed4$gamma)
intervals <- quantile(seed4$gamma[-(1:burnIn)], c(0.025, 0.975))
results_table[5,6] <- intervals[1]
results_table[5,7] <- intervals[2]
results_table[6,5] <- mean(seed5$gamma)
intervals <- quantile(seed5$gamma[-(1:burnIn)], c(0.025, 0.975))
results_table[6,6] <- intervals[1]
results_table[6,7] <- intervals[2]

# R0 = 6 Gamma = 0.15
seed1 <- read.csv(paste(dir, "mcmc_pop1000_R6_g0.15_11_loglik.csv", sep =""))
seed2 <- read.csv(paste(dir, "mcmc_pop1000_R6_g0.15_12_loglik.csv", sep =""))
seed3 <- read.csv(paste(dir, "mcmc_pop1000_R6_g0.15_13_loglik.csv", sep =""))
seed4 <- read.csv(paste(dir, "mcmc_pop1000_R6_g0.15_14_loglik.csv", sep =""))
seed5 <- read.csv(paste(dir, "mcmc_pop1000_R6_g0.15_15_loglik.csv", sep =""))

# Pop = 1000, R0 = 6
results_table[7,2] <- (6 * 0.15) / 1000
results_table[8,2] <- mean(seed1$beta)
intervals <- quantile(seed1$beta[-(1:burnIn)], c(0.025, 0.975))
results_table[8,3] <- intervals[1]
results_table[8,4] <- intervals[2]
results_table[9,2] <- mean(seed2$beta)
intervals <- quantile(seed2$beta[-(1:burnIn)], c(0.025, 0.975))
results_table[9,3] <- intervals[1]
results_table[9,4] <- intervals[2]
results_table[10,2] <- mean(seed3$beta)
intervals <- quantile(seed3$beta[-(1:burnIn)], c(0.025, 0.975))
results_table[10,3] <- intervals[1]
results_table[10,4] <- intervals[2]
results_table[11,2] <- mean(seed4$beta)
intervals <- quantile(seed4$beta[-(1:burnIn)], c(0.025, 0.975))
results_table[11,3] <- intervals[1]
results_table[11,4] <- intervals[2]
results_table[12,2] <- mean(seed5$beta)
intervals <- quantile(seed5$beta[-(1:burnIn)], c(0.025, 0.975))
results_table[12,3] <- intervals[1]
results_table[12,4] <- intervals[2]

# Pop = 1000, R0 = 6
results_table[7,5] <- 0.15
results_table[8,5] <- mean(seed1$gamma)
intervals <- quantile(seed1$gamma[-(1:burnIn)], c(0.025, 0.975))
results_table[8,6] <- intervals[1]
results_table[8,7] <- intervals[2]
results_table[9,5] <- mean(seed2$gamma)
intervals <- quantile(seed2$gamma[-(1:burnIn)], c(0.025, 0.975))
results_table[9,6] <- intervals[1]
results_table[9,7] <- intervals[2]
results_table[10,5] <- mean(seed3$gamma)
intervals <- quantile(seed3$gamma[-(1:burnIn)], c(0.025, 0.975))
results_table[10,6] <- intervals[1]
results_table[10,7] <- intervals[2]
results_table[11,5] <- mean(seed4$gamma)
intervals <- quantile(seed4$gamma[-(1:burnIn)], c(0.025, 0.975))
results_table[11,6] <- intervals[1]
results_table[11,7] <- intervals[2]
results_table[12,5] <- mean(seed5$gamma)
intervals <- quantile(seed5$gamma[-(1:burnIn)], c(0.025, 0.975))
results_table[12,6] <- intervals[1]
results_table[12,7] <- intervals[2]

##############################
# Making Residual Error table
library('xtable')
# options(xtable.floating = FALSE)
# options(xtable.timestamp = "")

rownames(results_table) <- c("1.5", "\\textcolor{white}{a}","\\textcolor{white}{b}","\\textcolor{white}{c}","\\textcolor{white}{d}","\\textcolor{white}{e}","6","\\textcolor{white}{f}","\\textcolor{white}{g}","\\textcolor{white}{h}","\\textcolor{white}{i}","\\textcolor{white}{j}")

addtorow <- list()
addtorow$pos <- list(0, 0)
addtorow$command <- c("& \\multicolumn{1}{l}{} & \\multicolumn{3}{l}{$\\beta$}  & \\multicolumn{3}{l}{$\\gamma$} \\\\\n", "$R_{0}$ & simulation & mean & min & max & mean & min & max \\\\\n")

tab <- xtable(results_table, digits= c(1, 0, -1, -1, -1, 2, 2, 2), caption = "$\\beta$ and $\\gamma$ values for the 3.5 million iteration MCMC inference method  $R_{0}$ of 1.5 or 6 for a population size of 1000. The numbers presented without 95\\% credible intervals are the true $\\beta$ and $\\gamma$ for each $R_{0}$. min: lower bound of 95\\% credible interval, max: upper bound of 95\\% credible interval", label = "mcmc_pop1000") 
align(tab) <- "lXXXXXXX"
print(tab, hline.after=c(-1, 0, 12), comment = FALSE, math.style.exponents = FALSE, caption.placement = "top", add.to.row = addtorow, include.colnames = FALSE, type="latex", sanitize.rownames.function = identity, tabular.environment = "tabularx", width = "\\textwidth")
@

\FloatBarrier
\clearpage

The traces for $\beta$ and $\gamma$ for the first simulation of each population size and $R_{0}$ combination suggest that the MCMC had not always fully converged during the 3.5 million iterations, epecially for the larger population sizes (Fig. \ref{mcmc_beta_trace}, Fig. \ref{mcmc_gamma_trace}). The $\beta$ trace for population size 50 and $R_{0}$ of 6 looks the closest to having reached convergence (Fig. \ref{mcmc_beta_trace}D), while the traces for the largest population size (Fig. \ref{mcmc_beta_trace}C, F) look to be the furthest from being converged, which is in concordance with the $\beta$ 95\% credible intervals for population size 1000 often excluding the true $\beta$ value in Table \ref{mcmc_pop1000}. 
\newline

\begin{figure}[!h]
\begin{center}
<<dataplot4, cache=FALSE, echo=FALSE, eval=TRUE, dpi=100, height=13, width=19, fig.cap="Data plot", fig=TRUE>>=
# Beta posterior plots
dir <- "/home/evelina/OneDrive/MRes_BMR/Project_1/Work_folder/Data/server_data/"
mcmc_R1.5_pop50_seed1 <- read.csv(paste(dir, "mcmc_pop50_R1.5_g0.15_14_loglik.csv", sep=""))
mcmc_R1.5_pop200_seed1 <- read.csv(paste(dir,"mcmc_pop200_R1.5_g0.15_81_loglik.csv", sep=""))
mcmc_R1.5_pop1000_seed1 <- read.csv(paste(dir,"mcmc_pop1000_R1.5_g0.15_177_loglik.csv", sep=""))
mcmc_R6_pop50_seed1 <- read.csv(paste(dir,"mcmc_pop50_R6_g0.15_2_loglik.csv", sep=""))
mcmc_R6_pop200_seed1 <- read.csv(paste(dir,"mcmc_pop200_R6_g0.15_6_loglik.csv", sep=""))
mcmc_R6_pop1000_seed1 <- read.csv(paste(dir,"mcmc_pop1000_R6_g0.15_11_loglik.csv", sep=""))

letter_size = 3
iteration_place = 3400
axis_size = 2.5
yaxis_size = 2
number_size = 2.5

par(mfrow = c(2,3), mar=c(5,6.5,1.5,1))

burnIn = ((3500000/1000)/100)*23 # 23% burn-in
beta_yaxis50_1 = c(0, 0.013)
beta_yaxis200_1 = c(0, 0.0022)
beta_yaxis1000_1 = c(0.0001, 0.0003)
beta_yaxis50_2 = c(0, 0.030)
beta_yaxis200_2 = c(0.001, 0.007)
beta_yaxis1000_2 = c(0.0002, 0.001)
# Seed 1, Pop size 50
plot(mcmc_R1.5_pop50_seed1$beta, type = "l", main = "", xlab = "", ylab = "", ylim = beta_yaxis50_1, cex.axis = number_size)
  # abline(v = burnIn, untf = FALSE)
text(iteration_place, min(beta_yaxis50_1)+(0.95*max(beta_yaxis50_1)-min(beta_yaxis50_1)), "A", cex = letter_size)
mtext(expression(beta), 2, line = 3, cex = yaxis_size, outer = FALSE)
# Seed 1, Pop size 200
plot(mcmc_R1.5_pop200_seed1$beta, type = "l", main = "", xlab = "", ylab = "", ylim = beta_yaxis200_1, cex.axis = number_size)
  # abline(v = burnIn, untf = FALSE)
text(iteration_place, min(beta_yaxis200_1)+(0.95*max(beta_yaxis200_1)-min(beta_yaxis200_1)), "B", cex = letter_size)
# Seed 1, Pop size 1000
plot(mcmc_R1.5_pop1000_seed1$beta, type = "l", main = "", xlab = "", ylab = "", ylim = beta_yaxis1000_1, cex.axis = number_size)
  # abline(v = burnIn, untf = FALSE)
text(iteration_place, min(beta_yaxis1000_1)+(0.96*max(beta_yaxis1000_1)-min(beta_yaxis1000_1)), "C", cex = letter_size)
# Seed 1, Pop size 50
plot(mcmc_R6_pop50_seed1$beta, type = "l", main = "", xlab = "", ylab = "", ylim = beta_yaxis50_2, cex.axis = number_size)
  # abline(v = burnIn, untf = FALSE)
text(iteration_place, min(beta_yaxis50_2)+(0.95*max(beta_yaxis50_2)-min(beta_yaxis50_2)), "D", cex = letter_size)
mtext(expression(beta), 2, line = 3, cex = yaxis_size, outer = FALSE)
mtext("Iteration (x1000)", 1, line = 4, cex = yaxis_size)
# Seed 1, Pop size 200
plot(mcmc_R6_pop200_seed1$beta, type = "l", main = "", xlab = "", ylab = "", ylim = beta_yaxis200_2, cex.axis = number_size)
  # abline(v = burnIn, untf = FALSE)
text(iteration_place, min(beta_yaxis200_2)+(0.95*max(beta_yaxis200_2)-min(beta_yaxis200_2)), "E", cex = letter_size)
mtext("Iteration (x1000)", 1, line = 4, cex = yaxis_size)
# Seed 1, Pop size 1000
plot(mcmc_R6_pop1000_seed1$beta, type = "l", main = "", xlab = "", ylab = "", ylim = beta_yaxis1000_2, cex.axis = number_size)
  # abline(v = burnIn, untf = FALSE)
text(iteration_place, min(beta_yaxis1000_2)+(0.95*max(beta_yaxis1000_2)-min(beta_yaxis1000_2)), "F", cex = letter_size)
mtext("Iteration (x1000)", 1, line = 4, cex = yaxis_size)
@
\caption{Traces of posterior $\beta$, per capita rate of infection, values throughout a 3.5 million iteration MCMC for the first simulation of each population size and $R_{0}$. Panels A-C are for outbreaks with an $R_{0}$ of 1.5, while panels D-F are for outbreaks with an $R_{0}$ of 6. Panels A and D have a population size of 50, panels B and E have a population size of 200, and panels C and F have a population size of 1000.}
\label{mcmc_beta_trace}
\end{center}
\end{figure}

\clearpage
A similar situation for the $\gamma$ traces can be seen as for the $\beta$ traces. Here again the trace for population size 50 and $R_{0}$ of 6 looks the most converged (Fig. \ref{mcmc_gamma_trace}D). None of the $\gamma$ traces for population size 200 (Fig. \ref{mcmc_gamma_trace}B, E) or population size 1000 (Fig. \ref{mcmc_gamma_trace}C, F) look fully converged. Additionally, in Figure \ref{mcmc_gamma_trace} seems that the fluctuation in $\gamma$ decreases with increasing population size.
\newline

\begin{figure}[!h]
\begin{center}
<<dataplot6, cache=FALSE, echo=FALSE, eval=TRUE, dpi=100, fig.pos="H", height=13, width=19, fig.cap="Data plot", fig=TRUE>>=
# Beta and gamma posterior plots
dir <- "/home/evelina/OneDrive/MRes_BMR/Project_1/Work_folder/Data/server_data/"
mcmc_R1.5_pop50_seed1 <- read.csv(paste(dir, "mcmc_pop50_R1.5_g0.15_14_loglik.csv", sep=""))
mcmc_R1.5_pop200_seed1 <- read.csv(paste(dir,"mcmc_pop200_R1.5_g0.15_81_loglik.csv", sep=""))
mcmc_R1.5_pop1000_seed1 <- read.csv(paste(dir,"mcmc_pop1000_R1.5_g0.15_177_loglik.csv", sep=""))
mcmc_R6_pop50_seed1 <- read.csv(paste(dir,"mcmc_pop50_R6_g0.15_2_loglik.csv", sep=""))
mcmc_R6_pop200_seed1 <- read.csv(paste(dir,"mcmc_pop200_R6_g0.15_6_loglik.csv", sep=""))
mcmc_R6_pop1000_seed1 <- read.csv(paste(dir,"mcmc_pop1000_R6_g0.15_11_loglik.csv", sep=""))

letter_size = 3
iteration_place = 3400
axis_size = 2.5
yaxis_size = 2
number_size = 2.5

par(mfrow = c(2,3), mar=c(5,6.5,1.5,1))

burnIn = ((3500000/1000)/100)*23 # 23% burn-in
gamma_yaxis50_1 = c(0, 0.40)
gamma_yaxis200_1 = c(0.05, 0.25)
gamma_yaxis1000_1 = c(0.08, 0.17)
gamma_yaxis50_2 = c(0, 0.30)
gamma_yaxis200_2 = c(0.05, 0.25)
gamma_yaxis1000_2 = c(0.08, 0.15)

# Seed 1, Pop size 50
plot(mcmc_R1.5_pop50_seed1$gamma, type = "l", main = "", xlab = "", ylab = "", ylim = gamma_yaxis50_1, cex.axis=number_size)
  # abline(v = burnIn, untf = FALSE)
text(iteration_place, min(gamma_yaxis50_1)+(0.95*max(gamma_yaxis50_1)-min(gamma_yaxis50_1)), "A", cex = letter_size)
mtext(expression(gamma), 2, line = 3, cex = yaxis_size, outer = FALSE)
# Seed 1, Pop size 200
plot(mcmc_R1.5_pop200_seed1$gamma, type = "l", main = "", xlab = "", ylab = "", ylim = gamma_yaxis200_1, cex.axis=number_size)
  # abline(v = burnIn, untf = FALSE)
text(iteration_place, min(gamma_yaxis200_1)+(0.95*max(gamma_yaxis200_1)-min(gamma_yaxis200_1)), "B", cex = letter_size)
# Seed 1, Pop size 1000
plot(mcmc_R1.5_pop1000_seed1$gamma, type = "l", main = "", xlab = "", ylab = "", ylim = gamma_yaxis1000_1, cex.axis=number_size)
  # abline(v = burnIn, untf = FALSE)
text(iteration_place, min(gamma_yaxis1000_1)+(0.95*max(gamma_yaxis1000_1)-min(gamma_yaxis1000_1)), "C", cex = letter_size)
# Seed 1, Pop size 50
plot(mcmc_R6_pop50_seed1$gamma, type = "l", main = "", xlab = "", ylab = "", ylim = gamma_yaxis50_2, cex.axis=number_size)
  # abline(v = burnIn, untf = FALSE)
text(iteration_place, min(gamma_yaxis50_2)+(0.95*max(gamma_yaxis50_2)-min(gamma_yaxis50_2)), "D", cex = letter_size)
mtext("Iteration (x1000)", 1, line = 4, cex = yaxis_size)
mtext(expression(gamma), 2, line = 3, cex = yaxis_size, outer = FALSE)
# Seed 1, Pop size 200
plot(mcmc_R6_pop200_seed1$gamma, type = "l", main = "", xlab = "", ylab = "", ylim = gamma_yaxis200_2, cex.axis=number_size)
  # abline(v = burnIn, untf = FALSE)
text(iteration_place, min(gamma_yaxis200_2)+(0.95*max(gamma_yaxis200_2)-min(gamma_yaxis200_2)), "E", cex = letter_size)
mtext("Iteration (x1000)", 1, line = 4, cex = yaxis_size)
# Seed 1, Pop size 1000
plot(mcmc_R6_pop1000_seed1$gamma, type = "l", main = "", xlab = "", ylab = "", ylim = gamma_yaxis1000_2, cex.axis=number_size)
  # abline(v = burnIn, untf = FALSE)
text(iteration_place, min(gamma_yaxis1000_2)+(0.97*max(gamma_yaxis1000_2)-min(gamma_yaxis1000_2)), "F", cex = letter_size)
mtext("Iteration (x1000)", 1, line = 4, cex = yaxis_size)
@
\caption{Traces of posterior $\gamma$, recovery rate, values throughout a 3.5 million MCMC for the first simulation of each population size and $R_{0}$. Plots A-C are for outbreaks with an $R_{0}$ of 1.5, while plots D-F are for outbreaks with an $R_{0}$ of 6. Plots A and D have a population size of 50, plots B and E have a population size of 200, and plots C and F have a population size of 1000. }
\label{mcmc_gamma_trace}
\end{center}
\end{figure}

\clearpage

The range of suggested infectious curves after the burn-in period overlaps with the true simulated infectious curve in general (Fig. \ref{mcmc_infcurve}). The unconverged MCMCs in Figure \ref{mcmc_infcurve}C and F are an exception to this, their the MCMC's augmented infectious curves are not yet overlapping with the true infectious curve. The range of infectious curves produced is widest for the smallest population size and $R_{0}$ (Fig. \ref{mcmc_infcurve}A). For each of the $R_{0}$s, the range of infectious curves produced narrows with an increasing population size (Fig. \ref{mcmc_infcurve}A-C and Fig. \ref{mcmc_infcurve}D-F).
\newline

\begin{figure}[!h]
\begin{center}
<<dataplot5, cache=FALSE, echo=FALSE, eval=TRUE, dpi=100, height=13, width=17, fig.cap="Data plot", fig=TRUE>>=
# Infectious curve plots
dir <- "/home/evelina/OneDrive/MRes_BMR/Project_1/Work_folder/Data/server_data/"
mcmc_R1.5_pop50_seed1 <- read.csv(paste(dir, "mcmc_pop50_R1.5_g0.15_14_infectious.csv", sep=""))
mcmc_R1.5_pop200_seed1 <- read.csv(paste(dir,"mcmc_pop200_R1.5_g0.15_81_infectious.csv", sep=""))
mcmc_R1.5_pop1000_seed1 <- read.csv(paste(dir,"mcmc_pop1000_R1.5_g0.15_177_infectious.csv", sep=""))
mcmc_R6_pop50_seed1 <- read.csv(paste(dir,"mcmc_pop50_R6_g0.15_2_infectious.csv", sep=""))
mcmc_R6_pop200_seed1 <- read.csv(paste(dir,"mcmc_pop200_R6_g0.15_6_infectious.csv", sep=""))
mcmc_R6_pop1000_seed1 <- read.csv(paste(dir,"mcmc_pop1000_R6_g0.15_11_infectious.csv", sep=""))

true_R1.5_pop50_seed1 <- read.csv("data_pop50_R1.5_g0.15_14.csv")
true_R1.5_pop200_seed1 <- read.csv("data_pop200_R1.5_g0.15_81.csv")
true_R1.5_pop1000_seed1 <- read.csv("data_pop1000_R1.5_g0.15_177.csv")
true_R6_pop50_seed1 <- read.csv("data_pop50_R6_g0.15_2.csv")
true_R6_pop200_seed1 <- read.csv("data_pop200_R6_g0.15_6.csv")
true_R6_pop1000_seed1 <- read.csv("data_pop1000_R6_g0.15_11.csv")

par(mfrow = c(2,3))

burnIn = ((3500000/1000)/100)*23 # 23% burn-in

letter_size = 3
axis_size = 2.5
yaxis_size = 2
number_size = 2.5

par(mfrow = c(2, 3), mar=c(5,6.5,0.4,0.2))

# Pop size = 50
N = 50
timestep_diff <- nrow(mcmc_R1.5_pop50_seed1) - nrow(true_R1.5_pop50_seed1)
zero_array <- array(0, dim = c(timestep_diff, ncol(true_R1.5_pop50_seed1)))
colnames(zero_array) <- c("time","S", "I", "R", "new_I", "new_R")
true_R1.5_pop50_seed1 <- rbind(zero_array, true_R1.5_pop50_seed1)
true_R1.5_pop50_seed1$time <- mcmc_R1.5_pop50_seed1[,1]

plot(true_R1.5_pop50_seed1$I, ylim = c(0, N), type = "l", col = "white", xlab = "", ylab = "", main = "", cex.axis = number_size)
for (i in burnIn:ncol(mcmc_R1.5_pop50_seed1)){
  lines(mcmc_R1.5_pop50_seed1[,i], type = "l", lty = 2, col = "grey", xlab = " ", ylab = " ", lwd = 0.5)
}
lines(true_R1.5_pop50_seed1$I, ylim = c(0, N), type = "l", col = "red", lwd = 3)
text(150, 0.95*N, "A", cex = letter_size)
mtext("Number infected", 2, line = 4, cex = yaxis_size, outer = FALSE)
# Pop size 200
N = 200
timestep_diff <- nrow(mcmc_R1.5_pop200_seed1) - nrow(true_R1.5_pop200_seed1)
zero_array <- array(0, dim = c(timestep_diff, ncol(true_R1.5_pop200_seed1)))
colnames(zero_array) <- c("time","S", "I", "R", "new_I", "new_R")
true_R1.5_pop200_seed1 <- rbind(zero_array, true_R1.5_pop200_seed1)
true_R1.5_pop200_seed1$time <- mcmc_R1.5_pop200_seed1[,1]

plot(true_R1.5_pop200_seed1$I, ylim = c(0, N), type = "l", col = "white", xlab = "", ylab = "", main = "", cex.axis = number_size)
for (i in burnIn:ncol(mcmc_R1.5_pop200_seed1)){
  lines(mcmc_R1.5_pop200_seed1[,i], type = "l", lty = 2, col = "grey", xlab = " ", ylab = " ", lwd = 0.5)
}
lines(true_R1.5_pop200_seed1$I, ylim = c(0, N), type = "l", col = "red", lwd = 3)
text(150, 0.95*N, "B", cex = letter_size)
# Pop size 1000
N = 1000
timestep_diff <- nrow(mcmc_R1.5_pop1000_seed1) - nrow(true_R1.5_pop1000_seed1)
zero_array <- array(0, dim = c(timestep_diff, ncol(true_R1.5_pop1000_seed1)))
colnames(zero_array) <- c("time","S", "I", "R", "new_I", "new_R")
true_R1.5_pop1000_seed1 <- rbind(zero_array, true_R1.5_pop1000_seed1)
true_R1.5_pop1000_seed1$time <- mcmc_R1.5_pop1000_seed1[,1]

plot(true_R1.5_pop1000_seed1$I, ylim = c(0, N), type = "l", col = "white", xlab = "", ylab = "", main = "", cex.axis = number_size)
for (i in burnIn:ncol(mcmc_R1.5_pop1000_seed1)){
  lines(mcmc_R1.5_pop1000_seed1[,i], type = "l", lty = 2, col = "grey", xlab = " ", ylab = " ", lwd = 0.5)
}
lines(true_R1.5_pop1000_seed1$I, ylim = c(0, N), type = "l", col = "red", lwd = 3)
text(150, 0.95*N, "C", cex = letter_size)
# Pop size 50
N = 50
timestep_diff <- nrow(mcmc_R6_pop50_seed1) - nrow(true_R6_pop50_seed1)
zero_array <- array(0, dim = c(timestep_diff, ncol(true_R6_pop50_seed1)))
colnames(zero_array) <- c("time","S", "I", "R", "new_I", "new_R")
true_R6_pop50_seed1 <- rbind(zero_array, true_R6_pop50_seed1)
true_R6_pop50_seed1$time <- mcmc_R6_pop50_seed1[,1]

plot(true_R6_pop50_seed1$I, ylim = c(0, N), type = "l", col = "white", xlab = "", ylab = "", main = "", cex.axis = number_size)
for (i in burnIn:ncol(mcmc_R6_pop50_seed1)){
  lines(mcmc_R6_pop50_seed1[,i], type = "l", lty = 2, col = "grey", xlab = " ", ylab = " ", lwd = 0.5)
}
lines(true_R6_pop50_seed1$I, ylim = c(0, N), type = "l", col = "red", lwd = 3)
text(150, 0.95*N, "D", cex = letter_size)
mtext("Timestep", 1, line = 4, cex = yaxis_size)
mtext("Number infected", 2, line = 4, cex = yaxis_size, outer = FALSE)
# Pop size 200
N = 200
timestep_diff <- nrow(mcmc_R6_pop200_seed1) - nrow(true_R6_pop200_seed1)
zero_array <- array(0, dim = c(timestep_diff, ncol(true_R6_pop200_seed1)))
colnames(zero_array) <- c("time","S", "I", "R", "new_I", "new_R")
true_R6_pop200_seed1 <- rbind(zero_array, true_R6_pop200_seed1)
true_R6_pop200_seed1$time <- mcmc_R6_pop200_seed1[,1]

plot(true_R6_pop200_seed1$I, ylim = c(0, N), type = "l", col = "white", xlab = "", ylab = "", main = "", cex.axis = number_size)
for (i in burnIn:ncol(mcmc_R6_pop200_seed1)){
  lines(mcmc_R6_pop200_seed1[,i], type = "l", lty = 2, col = "grey", xlab = " ", ylab = " ", lwd = 0.5)
}
lines(true_R6_pop200_seed1$I, ylim = c(0, N), type = "l", col = "red", lwd = 3)
text(150, 0.95*N, "E", cex = letter_size)
mtext("Timestep", 1, line = 4, cex = yaxis_size)
# Pop size 1000
N = 1000
timestep_diff <- nrow(mcmc_R6_pop1000_seed1) - nrow(true_R6_pop1000_seed1)
zero_array <- array(0, dim = c(timestep_diff, ncol(true_R6_pop1000_seed1)))
colnames(zero_array) <- c("time","S", "I", "R", "new_I", "new_R")
true_R6_pop1000_seed1 <- rbind(zero_array, true_R6_pop1000_seed1)
true_R6_pop1000_seed1$time <- mcmc_R6_pop1000_seed1[,1]

plot(true_R6_pop1000_seed1$I, ylim = c(0, N), type = "l", col = "white", xlab = "", ylab = "", main = "", cex.axis = number_size)
for (i in burnIn:ncol(mcmc_R6_pop1000_seed1)){
  lines(mcmc_R6_pop1000_seed1[,i], type = "l", lty = 2, col = "grey", xlab = " ", ylab = " ", lwd = 0.5)
}
lines(true_R6_pop1000_seed1$I, ylim = c(0, N), type = "l", col = "red", lwd = 3)
text(150, 0.95*N, "F", cex = letter_size)
mtext("Timestep", 1, line = 4, cex = yaxis_size)
@
\caption{Comparing the infectious curves of the 3.5 million MCMC to each outbreak's true simulated infectious curve for the first simulation of each population size and $R_{0}$. The red line depicts the number of individuals in a population infected at a given timepoint in the true dataset. The grey dotted lines refer to the post-burn-in period infectious curves used by the MCMC. Panels A-C and D-F represent outbreaks with an $R_{0}$ of 1.5 and 6, respectively. Panels A and D represent a population size of 50, panels B and E represent a population size of 200, and panels C and F represent a population size of 1000.}
\label{mcmc_infcurve}
\end{center}
\end{figure}

\FloatBarrier
\newpage
For the 17.5 million iteration MCMCs conducted for the outbreaks with a population size 1000 and an $R_{0}$ of 1.5, only simulations 1 and 5 included the true $\beta$, while the other three simulations overestimated the value (Table \ref{mcmc_pop1000_long}). For $\gamma$, simulations 1, 4, and 5 include the true $\gamma$ while the other two simulations overestimate it (Table \ref{mcmc_pop1000_long}). For outbreaks with and $R_{0}$ of 6, the true $\beta$ is included in the 95\% credible interval for all simulations except simulation 3 (Table \ref{mcmc_pop1000_long}). The true $\gamma$ is not included in the credible intervals of simulations 3 or 4, but is included in the intervals of the other three (Table \ref{mcmc_pop1000_long}).
\newline

<<MCMC_result_table_long, cache=FALSE, echo=FALSE, eval=TRUE, results=tex>>=
# Making MCMC table

mcmc_beta_results_table <- array(NA, dim =c(12,7))
burnIn = ((17500000/1000)/100)*23 # 23% burn-in

# setwd("/home/evelina/OneDrive/MRes_BMR/Project_1/Work_folder/Data/server_data")
dir <- "/home/evelina/OneDrive/MRes_BMR/Project_1/Work_folder/Data/server_data/"

mcmc_pop1000_seed1 <- read.csv(paste(dir, "mcmc_pop1000_R1.5_g0.15_177_loglik_long.csv", sep =""))
mcmc_pop1000_seed2 <- read.csv(paste(dir, "mcmc_pop1000_R1.5_g0.15_246_loglik_long.csv", sep =""))
mcmc_pop1000_seed3 <- read.csv(paste(dir, "mcmc_pop1000_R1.5_g0.15_469_loglik_long.csv", sep =""))
mcmc_pop1000_seed4 <- read.csv(paste(dir, "mcmc_pop1000_R1.5_g0.15_520_loglik_long.csv", sep =""))
mcmc_pop1000_seed5 <- read.csv(paste(dir,"mcmc_pop1000_R1.5_g0.15_784_loglik_long.csv", sep =""))

mcmc_beta_results_table[2,1] <- 1
mcmc_beta_results_table[3,1] <- 2
mcmc_beta_results_table[4,1] <- 3
mcmc_beta_results_table[5,1] <- 4
mcmc_beta_results_table[6,1] <- 5

mcmc_beta_results_table[8,1] <- 1
mcmc_beta_results_table[9,1] <- 2
mcmc_beta_results_table[10,1] <- 3
mcmc_beta_results_table[11,1] <- 4
mcmc_beta_results_table[12,1] <- 5

# Pop = 1000, R0 = 1.5
mcmc_beta_results_table[1,2] <- (1.5 * 0.15) / 1000
mcmc_beta_results_table[2,2] <- mean(mcmc_pop1000_seed1$beta[-(1:burnIn)])
intervals <- quantile(mcmc_pop1000_seed1$beta[-(1:burnIn)], c(0.025, 0.975))
mcmc_beta_results_table[2,3] <- intervals[1]
mcmc_beta_results_table[2,4] <- intervals[2]
mcmc_beta_results_table[3,2] <- mean(mcmc_pop1000_seed2$beta[-(1:burnIn)])
intervals <- quantile(mcmc_pop1000_seed2$beta[-(1:burnIn)], c(0.025, 0.975))
mcmc_beta_results_table[3,3] <- intervals[1]
mcmc_beta_results_table[3,4] <- intervals[2]
mcmc_beta_results_table[4,2] <- mean(mcmc_pop1000_seed3$beta[-(1:burnIn)])
intervals <- quantile(mcmc_pop1000_seed3$beta[-(1:burnIn)], c(0.025, 0.975))
mcmc_beta_results_table[4,3] <- intervals[1]
mcmc_beta_results_table[4,4] <- intervals[2]
mcmc_beta_results_table[5,2] <- mean(mcmc_pop1000_seed4$beta[-(1:burnIn)])
intervals <- quantile(mcmc_pop1000_seed4$beta[-(1:burnIn)], c(0.025, 0.975))
mcmc_beta_results_table[5,3] <- intervals[1]
mcmc_beta_results_table[5,4] <- intervals[2]
mcmc_beta_results_table[6,2] <- mean(mcmc_pop1000_seed5$beta[-(1:burnIn)])
intervals <- quantile(mcmc_pop1000_seed5$beta[-(1:burnIn)], c(0.025, 0.975))
mcmc_beta_results_table[6,3] <- intervals[1]
mcmc_beta_results_table[6,4] <- intervals[2]

# Pop = 1000, R0 = 1.5
mcmc_beta_results_table[1,5] <- 0.15
mcmc_beta_results_table[2,5] <- mean(mcmc_pop1000_seed1$gamma[-(1:burnIn)])
intervals <- quantile(mcmc_pop1000_seed1$gamma[-(1:burnIn)], c(0.025, 0.975))
mcmc_beta_results_table[2,6] <- intervals[1]
mcmc_beta_results_table[2,7] <- intervals[2]
mcmc_beta_results_table[3,5] <- mean(mcmc_pop1000_seed2$gamma[-(1:burnIn)])
intervals <- quantile(mcmc_pop1000_seed2$gamma[-(1:burnIn)], c(0.025, 0.975))
mcmc_beta_results_table[3,6] <- intervals[1]
mcmc_beta_results_table[3,7] <- intervals[2]
mcmc_beta_results_table[4,5] <- mean(mcmc_pop1000_seed3$gamma[-(1:burnIn)])
intervals <- quantile(mcmc_pop1000_seed3$gamma[-(1:burnIn)], c(0.025, 0.975))
mcmc_beta_results_table[4,6] <- intervals[1]
mcmc_beta_results_table[4,7] <- intervals[2]
mcmc_beta_results_table[5,5] <- mean(mcmc_pop1000_seed4$gamma[-(1:burnIn)])
intervals <- quantile(mcmc_pop1000_seed4$gamma[-(1:burnIn)], c(0.025, 0.975))
mcmc_beta_results_table[5,6] <- intervals[1]
mcmc_beta_results_table[5,7] <- intervals[2]
mcmc_beta_results_table[6,5] <- mean(mcmc_pop1000_seed5$gamma[-(1:burnIn)])
intervals <- quantile(mcmc_pop1000_seed5$gamma[-(1:burnIn)], c(0.025, 0.975))
mcmc_beta_results_table[6,6] <- intervals[1]
mcmc_beta_results_table[6,7] <- intervals[2]

# R0 = 6 Gamma = 0.15
mcmc_pop1000_seed1 <- read.csv(paste(dir, "mcmc_pop1000_R6_g0.15_11_loglik_long.csv", sep =""))
mcmc_pop1000_seed2 <- read.csv(paste(dir, "mcmc_pop1000_R6_g0.15_12_loglik_long.csv", sep =""))
mcmc_pop1000_seed3 <- read.csv(paste(dir, "mcmc_pop1000_R6_g0.15_13_loglik_long.csv", sep =""))
mcmc_pop1000_seed4 <- read.csv(paste(dir, "mcmc_pop1000_R6_g0.15_14_loglik_long.csv", sep =""))
mcmc_pop1000_seed5 <- read.csv(paste(dir, "mcmc_pop1000_R6_g0.15_15_loglik_long.csv", sep =""))

# Pop = 1000, R0 = 6
mcmc_beta_results_table[7,2] <- (6 * 0.15) / 1000
mcmc_beta_results_table[8,2] <- mean(mcmc_pop1000_seed1$beta[-(1:burnIn)])
intervals <- quantile(mcmc_pop1000_seed1$beta[-(1:burnIn)], c(0.025, 0.975))
mcmc_beta_results_table[8,3] <- intervals[1]
mcmc_beta_results_table[8,4] <- intervals[2]
mcmc_beta_results_table[9,2] <- mean(mcmc_pop1000_seed2$beta[-(1:burnIn)])
intervals <- quantile(mcmc_pop1000_seed2$beta[-(1:burnIn)], c(0.025, 0.975))
mcmc_beta_results_table[9,3] <- intervals[1]
mcmc_beta_results_table[9,4] <- intervals[2]
mcmc_beta_results_table[10,2] <- mean(mcmc_pop1000_seed3$beta[-(1:burnIn)])
intervals <- quantile(mcmc_pop1000_seed3$beta[-(1:burnIn)], c(0.025, 0.975))
mcmc_beta_results_table[10,3] <- intervals[1]
mcmc_beta_results_table[10,4] <- intervals[2]
mcmc_beta_results_table[11,2] <- mean(mcmc_pop1000_seed4$beta[-(1:burnIn)])
intervals <- quantile(mcmc_pop1000_seed4$beta[-(1:burnIn)], c(0.025, 0.975))
mcmc_beta_results_table[11,3] <- intervals[1]
mcmc_beta_results_table[11,4] <- intervals[2]
mcmc_beta_results_table[12,2] <- mean(mcmc_pop1000_seed5$beta[-(1:burnIn)])
intervals <- quantile(mcmc_pop1000_seed5$beta[-(1:burnIn)], c(0.025, 0.975))
mcmc_beta_results_table[12,3] <- intervals[1]
mcmc_beta_results_table[12,4] <- intervals[2]

# Pop = 1000, R0 = 6
mcmc_beta_results_table[7,5] <- 0.15
mcmc_beta_results_table[8,5] <- mean(mcmc_pop1000_seed1$gamma[-(1:burnIn)])
intervals <- quantile(mcmc_pop1000_seed1$gamma[-(1:burnIn)], c(0.025, 0.975))
mcmc_beta_results_table[8,6] <- intervals[1]
mcmc_beta_results_table[8,7] <- intervals[2]
mcmc_beta_results_table[9,5] <- mean(mcmc_pop1000_seed2$gamma[-(1:burnIn)])
intervals <- quantile(mcmc_pop1000_seed2$gamma[-(1:burnIn)], c(0.025, 0.975))
mcmc_beta_results_table[9,6] <- intervals[1]
mcmc_beta_results_table[9,7] <- intervals[2]
mcmc_beta_results_table[10,5] <- mean(mcmc_pop1000_seed3$gamma[-(1:burnIn)])
intervals <- quantile(mcmc_pop1000_seed3$gamma[-(1:burnIn)], c(0.025, 0.975))
mcmc_beta_results_table[10,6] <- intervals[1]
mcmc_beta_results_table[10,7] <- intervals[2]
mcmc_beta_results_table[11,5] <- mean(mcmc_pop1000_seed4$gamma[-(1:burnIn)])
intervals <- quantile(mcmc_pop1000_seed4$gamma[-(1:burnIn)], c(0.025, 0.975))
mcmc_beta_results_table[11,6] <- intervals[1]
mcmc_beta_results_table[11,7] <- intervals[2]
mcmc_beta_results_table[12,5] <- mean(mcmc_pop1000_seed5$gamma[-(1:burnIn)])
intervals <- quantile(mcmc_pop1000_seed5$gamma[-(1:burnIn)], c(0.025, 0.975))
mcmc_beta_results_table[12,6] <- intervals[1]
mcmc_beta_results_table[12,7] <- intervals[2]

##############################
# Making Residual Error table
library('xtable')
# options(xtable.floating = FALSE)
# options(xtable.timestamp = "")

rownames(mcmc_beta_results_table) <- c("1.5", "\\textcolor{white}{a}","\\textcolor{white}{b}","\\textcolor{white}{c}","\\textcolor{white}{d}","\\textcolor{white}{e}","6","\\textcolor{white}{f}","\\textcolor{white}{g}","\\textcolor{white}{h}","\\textcolor{white}{i}","\\textcolor{white}{j}")

addtorow <- list()
addtorow$pos <- list(0, 0)
addtorow$command <- c("& \\multicolumn{1}{l}{} & \\multicolumn{3}{l}{$\\beta$}  & \\multicolumn{3}{l}{$\\gamma$} \\\\\n", "$R_{0}$ & simulation & mean & min & max & mean & min & max \\\\\n")

tab <- xtable(mcmc_beta_results_table, digits= c(1, 0, -1, -1, -1, 2, 2, 2), caption = "$\\beta$ and $\\gamma$ values for the 17.5 million iteration MCMC inference method  $R_{0}$ of 1.5 or 6 for a population size of 1000. The numbers presented without 95\\% credible intervals are the true $\\beta$ and $\\gamma$ for each $R_{0}$. min: lower bound of 95\\% credible interval, max: upper bound of 95\\% credible interval", label = "mcmc_pop1000_long") 
align(tab) <- "lXXXXXXX"
print(tab, hline.after=c(-1, 0, 12), comment = FALSE, math.style.exponents = FALSE, caption.placement = "top", add.to.row = addtorow, include.colnames = FALSE, type="latex", sanitize.rownames.function = identity, tabular.environment = "tabularx", width = "\\textwidth")
@

\clearpage
\FloatBarrier
For the 17.5 million iteration run for the largest population size, the MCMC did come close to converging for the outbreak with an $R_{0}$ of 6 and population size of 1000 (Fig. \ref{mcmc_long}D-E). The outbreak with an $R_{0}$ of 1.5 though has not converged (Fig. \ref{mcmc_long}A-B). The infectious curves estimated during the post-burn-in period of the MCMC now do coincide with the true simulated infectious curves (Fig. \ref{mcmc_long}C, F).
\newline

\begin{figure}[!h]
\begin{center}
<<longmcmcplot, cache=FALSE, echo=FALSE, eval=TRUE, dpi=100, height=13, width=17, fig.cap="Data plot", fig=TRUE>>=
# Infectious curve plots
dir <- "/home/evelina/OneDrive/MRes_BMR/Project_1/Work_folder/Data/server_data/"
mcmc_R1.5_pop1000_seed1 <- read.csv(paste(dir,"mcmc_pop1000_R1.5_g0.15_177_loglik_long.csv", sep=""))
mcmc_R6_pop1000_seed1 <- read.csv(paste(dir,"mcmc_pop1000_R6_g0.15_11_loglik_long.csv", sep=""))
mcmc_R1.5_pop1000_inf <- read.csv(paste(dir,"mcmc_pop1000_R1.5_g0.15_177_infectious_long.csv", sep=""))
mcmc_R6_pop1000_inf <- read.csv(paste(dir,"mcmc_pop1000_R6_g0.15_11_infectious_long.csv", sep=""))

true_R1.5_pop1000_seed1 <- read.csv("data_pop1000_R1.5_g0.15_177.csv")
true_R6_pop1000_seed1 <- read.csv("data_pop1000_R6_g0.15_11.csv")

burnIn = ((17500000/1000)/100)*23 # 23% burn-in
burnIn_1.5 = ((17500000/1000)/100)*23 # ((17500000/1000)/100)*23

letter_size = 3
axis_size = 2.5
yaxis_size = 2
number_size = 2.5
beta_yaxis1000_1.5 = c(0, 0.0005)
beta_yaxis1000_6 = c(0, 0.0015)
gamma_yaxis1000_1.5 = c(0, 0.3)
gamma_yaxis1000_6 = c(0, 0.2)
iteration_place = 17500*0.9

par(mfrow = c(2, 3), mar=c(5.5,6.5,1.5,0.2))

# Seed 1, Pop size 1000
plot(mcmc_R1.5_pop1000_seed1$beta, type = "l", main = "", xlab = "", ylab = "", ylim = beta_yaxis1000_1.5, cex.axis = number_size)
  # abline(v = burnIn_1.5, untf = FALSE)
text(iteration_place, 0.95*max(beta_yaxis1000_1.5), "A", cex = letter_size)
mtext("Iteration (x1000)", 1, line = 4, cex = yaxis_size)
mtext(expression(beta), 2, line = 3, cex = yaxis_size, outer = FALSE)

# Seed 1, Pop size 1000
plot(mcmc_R1.5_pop1000_seed1$gamma, type = "l", main = "", xlab = "", ylab = "", ylim = gamma_yaxis1000_1.5, cex.axis = number_size)
  # abline(v = burnIn_1.5, untf = FALSE)
text(iteration_place, 0.95*max(gamma_yaxis1000_1.5), "B", cex = letter_size)
mtext("Iteration (x1000)", 1, line = 4, cex = yaxis_size)
mtext(expression(gamma), 2, line = 3, cex = yaxis_size, outer = FALSE)

# Pop size 1000
N = 1000
timestep_diff <- nrow(mcmc_R1.5_pop1000_inf) - nrow(true_R1.5_pop1000_seed1)
zero_array <- array(0, dim = c(timestep_diff, ncol(true_R1.5_pop1000_seed1)))
colnames(zero_array) <- c("time","S", "I", "R", "new_I", "new_R")
true_R1.5_pop1000_seed1 <- rbind(zero_array, true_R1.5_pop1000_seed1)
true_R1.5_pop1000_seed1$time <- mcmc_R1.5_pop1000_inf[,1]

plot(true_R1.5_pop1000_seed1$I, ylim = c(0, N), type = "l", col = "white", xlab = "", ylab = "", main = "", cex.axis = number_size)
for (i in burnIn_1.5:ncol(mcmc_R1.5_pop1000_inf)){
  lines(mcmc_R1.5_pop1000_inf[,i], type = "l", lty = 2, col = "grey", xlab = " ", ylab = " ", lwd = 0.5)
}
lines(true_R1.5_pop1000_seed1$I, ylim = c(0, N), type = "l", col = "red", lwd = 3)
text(160, 0.95*N, "C", cex = letter_size)
mtext("Timestep", 1, line = 4, cex = yaxis_size)
mtext("Number infected", 2, line = 3, cex = yaxis_size, outer = FALSE)

# Seed 1, Pop size 1000
plot(mcmc_R6_pop1000_seed1$beta, type = "l", main = "", xlab = "", ylab = "", ylim = beta_yaxis1000_6, cex.axis = number_size)
  # abline(v = burnIn, untf = FALSE)
text(iteration_place, 0.95*max(beta_yaxis1000_6), "D", cex = letter_size)
mtext("Iteration (x1000)", 1, line = 4, cex = yaxis_size)
mtext(expression(beta), 2, line = 3, cex = yaxis_size, outer = FALSE)

# Seed 1, Pop size 1000
plot(mcmc_R6_pop1000_seed1$gamma, type = "l", main = "", xlab = "", ylab = "", ylim = gamma_yaxis1000_6, cex.axis = number_size)
  # abline(v = burnIn, untf = FALSE)
text(iteration_place, 0.95*max(gamma_yaxis1000_6), "E", cex = letter_size)
mtext("Iteration (x1000)", 1, line = 4, cex = yaxis_size)
mtext(expression(gamma), 2, line = 3, cex = yaxis_size, outer = FALSE)

# Pop size 1000
N = 1000
timestep_diff <- nrow(mcmc_R6_pop1000_inf) - nrow(true_R6_pop1000_seed1)
zero_array <- array(0, dim = c(timestep_diff, ncol(true_R6_pop1000_seed1)))
colnames(zero_array) <- c("time","S", "I", "R", "new_I", "new_R")
true_R6_pop1000_seed1 <- rbind(zero_array, true_R6_pop1000_seed1)
true_R6_pop1000_seed1$time <- mcmc_R6_pop1000_inf[,1]

plot(true_R6_pop1000_seed1$I, ylim = c(0, N), type = "l", col = "white", xlab = "", ylab = "", main = "", cex.axis = number_size)
for (i in burnIn:ncol(mcmc_R6_pop1000_inf)){
  lines(mcmc_R6_pop1000_inf[,i], type = "l", lty = 2, col = "grey", xlab = " ", ylab = " ", lwd = 0.5)
}
lines(true_R6_pop1000_seed1$I, ylim = c(0, N), type = "l", col = "red", lwd = 3)
text(160, 0.95*N, "F", cex = letter_size)
mtext("Timestep", 1, line = 4, cex = yaxis_size)
mtext("Number infected", 2, line = 3, cex = yaxis_size, outer = FALSE)
@
\caption{The $\beta$ (per capita rate of infection) and $\gamma$ (recovery rate) posterior traces, and infectious curves for 17.5 million iteration MCMCs for the first simulation of each $R_{0}$ with a population size of 1000. Panels A and B show the $\beta$ and $\gamma$ traces respectively of an outbreak with a population size of 1000 and $R_{0}$ of 1.5. Panels D and E show the $\beta$ and $\gamma$ traces for an outbreak with a population size of 1000 and $R_{0}$ of 6. Panels C and F show the infectious curves for the outbreaks with $R_{0}$ of 1.5 and 6, respectively. The red line depicts the number of infectious individuals per timepoint for the true simulated outbreaks, while the grey dotted lines represent the post-burn-in infectious curves for the MCMC.}
\label{mcmc_long}
\end{center}
\end{figure}

\clearpage

The contour plot in Figure \ref{mcmc_contourplot} shows how $\beta$ and $\gamma$ correlate for the first simulation of population size 200 and $R_{0}$ of 1.5. The most commonly occurring $\beta$ and $\gamma$ combination is approximately 0.0013 and 0.13 respectively. There is a positive correlation between $\beta$ and $\gamma$ as combinations with both a lower $\beta$ and $\gamma$ appear at a similar density as combinations with a higher $\beta$ and $\gamma$ as is the case for, for example, a combination where $\beta$ is 0.0012 and $\gamma$ is 0.12 and a combination where $\beta$ is 0.0015 and $\gamma$ is 0.16. A similar positive correlation is seen for the other 29 outbreaks.

\begin{figure}
\begin{center}
<<mcmcdensityplot, cache=FALSE, echo=FALSE, eval=TRUE, dpi=100, height=10, width=14, fig.cap="Data plot", fig=TRUE>>=
dir <- "/home/evelina/OneDrive/MRes_BMR/Project_1/Work_folder/Data/server_data/"
mcmc_R1.5_pop200 <- read.csv(paste(dir,"mcmc_pop200_R1.5_g0.15_81_loglik.csv", sep=""))

burnIn = 3500 * 0.23
# Plot beta vs. gamma
par(mfrow = c(1,1), mar = c(5, 5, 1, 5))
library(RColorBrewer)
library(MASS)

k <- 11
my.cols <- rev(brewer.pal(k, "RdYlBu"))
z <- kde2d(mcmc_R1.5_pop200$gamma[-(1:burnIn)], mcmc_R1.5_pop200$beta[-(1:burnIn)], n=50)
filled.contour(z, nlevels=k, col=my.cols, xlab = "", ylab = "", cex.axis = 1.5)
mtext(expression(gamma), 1, line = 4, cex = 1.5)
mtext(expression(beta), 2, line = 3, cex = 1.5, outer = FALSE)
@
\caption{Contour plot for the first simulation of population size 200 and $R_{0}$ of 1.5 showing the densities of different combinations of $\beta$, per capita rate of infection, and $\gamma$, recovery rate. The higher the number on the scale on the right, the higher the density of $\beta$ and $\gamma$ combinations in the coloured contoured area corresponding to the score.}
\label{mcmc_contourplot}
\end{center}
\end{figure}


% Make histogram comparisons of beta and gamma ranges that compares RE and MCMC for example outbreaks? 
% Plot: Histograms comparing RE and MCMC beta and gamma for 6 example scenarios
<<dataplot7, cache=FALSE, echo=FALSE, eval=FALSE, dpi=100, fig.pos="H", fig.height=4, fig.width=6, fig.cap="Data plot", fig=TRUE>>=
# Beta Histogram
dir <- "/home/evelina/OneDrive/MRes_BMR/Project_1/Work_folder/Data/server_data/"
re_R1.5_pop50_seed1 <- read.csv(paste(dir, "re_pop50_R1.5_g0.15_14.csv", sep=""))
re_R1.5_pop200_seed1 <- read.csv(paste(dir,"re_pop200_R1.5_g0.15_81.csv", sep=""))
re_R1.5_pop1000_seed1 <- read.csv(paste(dir,"re_pop1000_R1.5_g0.15_177.csv", sep=""))
re_R6_pop50_seed1 <- read.csv(paste(dir,"re_pop50_R6_g0.15_2.csv", sep=""))
re_R6_pop200_seed1 <- read.csv(paste(dir,"re_pop200_R6_g0.15_6.csv", sep=""))
re_R6_pop1000_seed1 <- read.csv(paste(dir,"re_pop1000_R6_g0.15_11.csv", sep=""))

mcmc_R1.5_pop50_seed1 <- read.csv(paste(dir, "mcmc_pop50_R1.5_g0.15_14_loglik.csv", sep=""))
mcmc_R1.5_pop200_seed1 <- read.csv(paste(dir,"mcmc_pop200_R1.5_g0.15_81_loglik.csv", sep=""))
mcmc_R1.5_pop1000_seed1 <- read.csv(paste(dir,"mcmc_pop1000_R1.5_g0.15_177_loglik.csv", sep=""))
mcmc_R6_pop50_seed1 <- read.csv(paste(dir,"mcmc_pop50_R6_g0.15_2_loglik.csv", sep=""))
mcmc_R6_pop200_seed1 <- read.csv(paste(dir,"mcmc_pop200_R6_g0.15_6_loglik.csv", sep=""))
mcmc_R6_pop1000_seed1 <- read.csv(paste(dir,"mcmc_pop1000_R6_g0.15_11_loglik.csv", sep=""))

letter_size = 1

# Pop. size = 50
par(mfrow = c(2,6), mar = c(4,3.9,1,0.5))
# beta_xaxis <- c(0.002, 0.010)
# gamma_xaxis <- c(0.05, 0.14)
beta_xaxis1.5 <- c(0.0, 0.013)
gamma_xaxis1.5 <-c(0.0, 0.40)

# RE
# RE seed 1
hist(re_R1.5_pop50_seed1$beta[2:nrow(re_R1.5_pop50_seed1)],nclass=30, col="gray", main="RE beta seed 1", xlab="", xlim = beta_xaxis1.5)
abline(v = re_R1.5_pop50_seed1$beta[1], col = "red")
box()
# Pop = 200 & R0 = 1.5
# RE seed 1
hist(re_R1.5_pop200_seed1$beta[2:nrow(re_R1.5_pop200_seed1)],nclass=30, col="gray", main="RE beta seed 1", xlab="", xlim = beta_xaxis1.5)
abline(v = re_R1.5_pop200_seed1$beta[1], col = "red")
box()
# Pop = 1000 & R0 = 1.5
# RE seed 1
hist(re_R1.5_pop1000_seed1$beta[2:nrow(re_R1.5_pop1000_seed1)],nclass=30, col="gray", main="RE beta seed 1", xlab="", xlim = beta_xaxis1.5)
abline(v = re_R1.5_pop1000_seed1$beta[1], col = "red")
box()

# MCMC seed 1
hist(mcmc_R1.5_pop50_seed1$beta[-(1:burnIn)], nclass=30, col = "gray", main="MCMC beta seed 1", xlab = "Beta", xlim = beta_xaxis1.5)
abline(v = mean(mcmc_R1.5_pop50_seed1$beta[-(1:burnIn)]), col = "red")
box()
# MCMC seed 1
hist(mcmc_R1.5_pop200_seed1$beta[-(1:burnIn)], nclass=30, col = "gray", main="MCMC beta seed 1", xlab = "Beta", xlim = beta_xaxis1.5)
abline(v = mean(mcmc_R1.5_pop200_seed1$beta[-(1:burnIn)]), col = "red")
box()
# MCMC seed 1
hist(mcmc_R1.5_pop1000_seed1$beta[-(1:burnIn)], nclass=30, col = "gray", main="MCMC beta seed 1", xlab = "Beta", xlim = beta_xaxis1.5)
abline(v = mean(mcmc_R1.5_pop1000_seed1$beta[-(1:burnIn)]), col = "red")
box()
@

\FloatBarrier
%%%%%%%%%%
\clearpage
\newpage
\subsection{Markov Chain Monte Carlo with a partly deterministic process}

Running the MCMC with a partly deterministic process for 2.5 million iterations took approximately 20 hours for each of the six studied outbreaks.

The $\beta$ posterior traces for the MCMC with a deterministic process were increasingly sticky with increasing population size, tending to stay in place rather than explore other parameter values, (Fig. \ref{det_mcmc_beta}). Relating to this, the $\beta$ estimates did not change drastically over the course of the 2.5 million iterations for some outbreaks, such as that with a population size of 50 and $R_{0}$ of 6 or population size of 1000 and $R_{0}$ of 1.5 (Figure \ref{det_mcmc_beta}C, D).
\newline

\begin{figure}[!h]
\begin{center}
<<detmcmcbeta, cache=FALSE, echo=FALSE, eval=TRUE, dpi=100, height=13, width=19, fig.cap="Data plot", fig=TRUE>>=
# Beta posterior plots
dir <- "/home/evelina/OneDrive/MRes_BMR/Project_1/Work_folder/Data/server_data/"
mcmc_R1.5_pop50_seed1 <- read.csv(paste(dir, "det_mcmc_pop50_R1.5_g0.15_14_loglik.csv", sep=""))
mcmc_R1.5_pop200_seed1 <- read.csv(paste(dir,"det_mcmc_pop200_R1.5_g0.15_81_loglik.csv", sep=""))
mcmc_R1.5_pop1000_seed1 <- read.csv(paste(dir,"det_mcmc_pop1000_R1.5_g0.15_177_loglik.csv", sep=""))
mcmc_R6_pop50_seed1 <- read.csv(paste(dir,"det_mcmc_pop50_R6_g0.15_2_loglik.csv", sep=""))
mcmc_R6_pop200_seed1 <- read.csv(paste(dir,"det_mcmc_pop200_R6_g0.15_6_loglik.csv", sep=""))
mcmc_R6_pop1000_seed1 <- read.csv(paste(dir,"det_mcmc_pop1000_R6_g0.15_11_loglik.csv", sep=""))

letter_size = 3
iteration_place = 2400
axis_size = 2.5
yaxis_size = 2
number_size = 2.5

par(mfrow = c(2,3), mar=c(5,6.5,1.5,1))

burnIn = ((2500000/1000)/100)*23 # 23% burn-in
beta_yaxis50_1 = c(0.002, 0.005)
beta_yaxis200_1 = c(0.0005, 0.0015)
beta_yaxis1000_1 = c(0.00009, 0.00011)
beta_yaxis50_2 = c(0.013, 0.0145)
beta_yaxis200_2 = c(0.0025, 0.004)
beta_yaxis1000_2 = c(0.00105, 0.0012)
# Seed 1, Pop size 50
plot(mcmc_R1.5_pop50_seed1$beta, type = "l", main = "", xlab = "", ylab = "", ylim = beta_yaxis50_1, cex.axis = number_size)
  # abline(v = burnIn, untf = FALSE)
text(iteration_place, (0.95*(max(beta_yaxis50_1)-min(beta_yaxis50_1)))+min(beta_yaxis50_1), "A", cex = letter_size)
mtext(expression(beta), 2, line = 3, cex = yaxis_size, outer = FALSE)
# Seed 1, Pop size 200
plot(mcmc_R1.5_pop200_seed1$beta, type = "l", main = "", xlab = "", ylab = "", ylim = beta_yaxis200_1, cex.axis = number_size)
  # abline(v = burnIn, untf = FALSE)
text(iteration_place, (0.95*(max(beta_yaxis200_1)-min(beta_yaxis200_1)))+min(beta_yaxis200_1), "B", cex = letter_size)
# Seed 1, Pop size 1000
plot(mcmc_R1.5_pop1000_seed1$beta, type = "l", main = "", xlab = "", ylab = "", ylim = beta_yaxis1000_1, cex.axis = number_size)
  # abline(v = burnIn, untf = FALSE)
text(iteration_place, (0.95*(max(beta_yaxis1000_1)-min(beta_yaxis1000_1)))+min(beta_yaxis1000_1), "C", cex = letter_size)
# Seed 1, Pop size 50
plot(mcmc_R6_pop50_seed1$beta, type = "l", main = "", xlab = "", ylab = "", ylim = beta_yaxis50_2, cex.axis = number_size)
  # abline(v = burnIn, untf = FALSE)
text(iteration_place, (0.95*(max(beta_yaxis50_2)-min(beta_yaxis50_2)))+min(beta_yaxis50_2), "D", cex = letter_size)
mtext(expression(beta), 2, line = 3, cex = yaxis_size, outer = FALSE)
mtext("Iteration (x1000)", 1, line = 4, cex = yaxis_size)
# Seed 1, Pop size 200
plot(mcmc_R6_pop200_seed1$beta, type = "l", main = "", xlab = "", ylab = "", ylim = beta_yaxis200_2, cex.axis = number_size)
  # abline(v = burnIn, untf = FALSE)
text(iteration_place, (0.95*(max(beta_yaxis200_2)-min(beta_yaxis200_2)))+min(beta_yaxis200_2), "E", cex = letter_size)
mtext("Iteration (x1000)", 1, line = 4, cex = yaxis_size)
# Seed 1, Pop size 1000
plot(mcmc_R6_pop1000_seed1$beta, type = "l", main = "", xlab = "", ylab = "", ylim = beta_yaxis1000_2, cex.axis = number_size)
  # abline(v = burnIn, untf = FALSE)
text(iteration_place, (0.95*(max(beta_yaxis1000_2)-min(beta_yaxis1000_2)))+min(beta_yaxis1000_2), "F", cex = letter_size)
mtext("Iteration (x1000)", 1, line = 4, cex = yaxis_size)
@
\caption{Traces of posterior $\beta$, per capita rate of infection, values throughout a 2.5 million iteration MCMC with a deterministic process. Panels A-C are for outbreaks with an $R_{0}$ of 1.5, while panels D-F are for outbreaks with an $R_{0}$ of 6. Panels A and D have a population size of 50, panels B and E have a population size of 200, and panels C and F have a population size of 1000.}
\label{det_mcmc_beta}
\end{center}
\end{figure}

\clearpage
A similar situation is seen for the $\gamma$ posterior traces (Fig. \ref{det_mcmc_gamma}), the traces stayed mostly in place for for the largest population sizes and for the outbreak with an $R_{0}$ of 6 and population size of 50 (Fig. \ref{det_mcmc_gamma}C, D, F). The other two population sizes for outbreaks with an $R_{0}$ of 1.5 and for the outbreak where $R_{0}$ was 6 and population size 200 saw increases in $\gamma$ over the course of the 2.5 million iterations (Fig. \ref{det_mcmc_gamma}A-B, E). Also, as with the $\beta$ posteriors, the $\gamma$ posteriors' stickiness increased with increasing population size.
\newline

\begin{figure}[!h]
\begin{center}
<<detmcmcgamma, cache=FALSE, echo=FALSE, eval=TRUE, dpi=100, height=13, width=19, fig.cap="Data plot", fig=TRUE>>=
# Beta posterior plots
dir <- "/home/evelina/OneDrive/MRes_BMR/Project_1/Work_folder/Data/server_data/"
mcmc_R1.5_pop50_seed1 <- read.csv(paste(dir, "det_mcmc_pop50_R1.5_g0.15_14_loglik.csv", sep=""))
mcmc_R1.5_pop200_seed1 <- read.csv(paste(dir,"det_mcmc_pop200_R1.5_g0.15_81_loglik.csv", sep=""))
mcmc_R1.5_pop1000_seed1 <- read.csv(paste(dir,"det_mcmc_pop1000_R1.5_g0.15_177_loglik.csv", sep=""))
mcmc_R6_pop50_seed1 <- read.csv(paste(dir,"det_mcmc_pop50_R6_g0.15_2_loglik.csv", sep=""))
mcmc_R6_pop200_seed1 <- read.csv(paste(dir,"det_mcmc_pop200_R6_g0.15_6_loglik.csv", sep=""))
mcmc_R6_pop1000_seed1 <- read.csv(paste(dir,"det_mcmc_pop1000_R6_g0.15_11_loglik.csv", sep=""))

letter_size = 3
iteration_place = 2400
axis_size = 2.5
yaxis_size = 2
number_size = 2.5

par(mfrow = c(2,3), mar=c(5,6.5,1.5,1))

burnIn = ((2500000/1000)/100)*23 # 23% burn-in
gamma_yaxis50_1 = c(0.05, 0.2)
gamma_yaxis200_1 = c(0.05, 0.2)
gamma_yaxis1000_1 = c(0.03, 0.07)
gamma_yaxis50_2 = c(0.35, 0.45)
gamma_yaxis200_2 = c(0.3, 0.5)
gamma_yaxis1000_2 = c(0.15, 0.2)
# Seed 1, Pop size 50
plot(mcmc_R1.5_pop50_seed1$gamma, type = "l", main = "", xlab = "", ylab = "", ylim = gamma_yaxis50_1, cex.axis = number_size)
  # abline(v = burnIn, untf = FALSE)
text(iteration_place, (0.95*(max(gamma_yaxis50_1)-min(gamma_yaxis50_1)))+min(gamma_yaxis50_1), "A", cex = letter_size)
mtext(expression(gamma), 2, line = 3, cex = yaxis_size, outer = FALSE)
# Seed 1, Pop size 200
plot(mcmc_R1.5_pop200_seed1$gamma, type = "l", main = "", xlab = "", ylab = "", ylim = gamma_yaxis200_1, cex.axis = number_size)
  # abline(v = burnIn, untf = FALSE)
text(iteration_place, (0.95*(max(gamma_yaxis200_1)-min(gamma_yaxis200_1)))+min(gamma_yaxis200_1), "B", cex = letter_size)
# Seed 1, Pop size 1000
plot(mcmc_R1.5_pop1000_seed1$gamma, type = "l", main = "", xlab = "", ylab = "", ylim = gamma_yaxis1000_1, cex.axis = number_size)
  # abline(v = burnIn, untf = FALSE)
text(iteration_place, (0.95*(max(gamma_yaxis1000_1)-min(gamma_yaxis1000_1)))+min(gamma_yaxis1000_1), "C", cex = letter_size)
# Seed 1, Pop size 50
plot(mcmc_R6_pop50_seed1$gamma, type = "l", main = "", xlab = "", ylab = "", ylim = gamma_yaxis50_2, cex.axis = number_size)
  # abline(v = burnIn, untf = FALSE)
text(iteration_place, (0.95*(max(gamma_yaxis50_2)-min(gamma_yaxis50_2)))+min(gamma_yaxis50_2), "D", cex = letter_size)
mtext(expression(gamma), 2, line = 3, cex = yaxis_size, outer = FALSE)
mtext("Iteration (x1000)", 1, line = 4, cex = yaxis_size)
# Seed 1, Pop size 200
plot(mcmc_R6_pop200_seed1$gamma, type = "l", main = "", xlab = "", ylab = "", ylim = gamma_yaxis200_2, cex.axis = number_size)
  # abline(v = burnIn, untf = FALSE)
text(iteration_place, (0.95*(max(gamma_yaxis200_2)-min(gamma_yaxis200_2)))+min(gamma_yaxis200_2), "E", cex = letter_size)
mtext("Iteration (x1000)", 1, line = 4, cex = yaxis_size)
# Seed 1, Pop size 1000
plot(mcmc_R6_pop1000_seed1$gamma, type = "l", main = "", xlab = "", ylab = "", ylim = gamma_yaxis1000_2, cex.axis = number_size)
  # abline(v = burnIn, untf = FALSE)
text(iteration_place, (0.95*(max(gamma_yaxis1000_2)-min(gamma_yaxis1000_2)))+min(gamma_yaxis1000_2), "F", cex = letter_size)
mtext("Iteration (x1000)", 1, line = 4, cex = yaxis_size)
@
\caption{Traces of posterior $\gamma$, recovery rate, values throughout a 2.5 million iteration MCMC with a deterministic process. Panels A-C are for outbreaks with an $R_{0}$ of 1.5, while panels D-F are for outbreaks with an $R_{0}$ of 6. Panels A and D have a population size of 50, panels B and E have a population size of 200, and panels C and F have a population size of 1000.}
\label{det_mcmc_gamma}
\end{center}
\end{figure}
\FloatBarrier
\clearpage
When investigating the infectious curves produced by the inferred $\beta$ and $\gamma$ values, it can be seen that the location of the peak is misjudged for population size 50 and 1000 when $R_{0}$ is 1.5 (Fig. \ref{det_mcmc_infcurve}A, C), with the height of the peak also being underestimated for population size 50. For population size 200 and $R_{0}$ of 1.5 the height of the peak is underestimated, but its location is more similar to the true simulated peak (Fig 14B). For the outbreaks with an $R_{0}$ of 6, the size of the peak is greatly underestimated for population sizes 50 and 200, though the location of the peak in time is not fully dissimilar from the true peak's location (Fig. \ref{det_mcmc_infcurve}D-E). For population size 1000, the size of the peak is underestimated, but not as greatly as for the other two population sizes, though the location of the peak in time is estimated to be earlier than for the true curve (Fig. \ref{det_mcmc_infcurve}F).
\newline

\begin{figure}[!h]
\begin{center}
<<detmcmc, cache=TRUE, echo=FALSE, eval=TRUE, dpi=100, height=13, width=17, fig.cap="Data plot", fig=TRUE>>=
# RE example plots
library("deSolve")

true_R1.5_pop50_seed1 <- read.csv("data_pop50_R1.5_g0.15_14.csv")
true_R1.5_pop200_seed1 <- read.csv("data_pop200_R1.5_g0.15_81.csv")
true_R1.5_pop1000_seed1 <- read.csv("data_pop1000_R1.5_g0.15_177.csv")
true_R6_pop50_seed1 <- read.csv("data_pop50_R6_g0.15_2.csv")
true_R6_pop200_seed1 <- read.csv("data_pop200_R6_g0.15_6.csv")
true_R6_pop1000_seed1 <- read.csv("data_pop1000_R6_g0.15_11.csv")

dir <- "/home/evelina/OneDrive/MRes_BMR/Project_1/Work_folder/Data/server_data/"
re_R1.5_pop50_seed1 <- read.csv(paste(dir, "det_mcmc_pop50_R1.5_g0.15_14_loglik.csv", sep=""))
re_R1.5_pop200_seed1 <- read.csv(paste(dir,"det_mcmc_pop200_R1.5_g0.15_81_loglik.csv", sep=""))
re_R1.5_pop1000_seed1 <- read.csv(paste(dir,"det_mcmc_pop1000_R1.5_g0.15_177_loglik.csv", sep=""))
re_R6_pop50_seed1 <- read.csv(paste(dir,"det_mcmc_pop50_R6_g0.15_2_loglik.csv", sep=""))
re_R6_pop200_seed1 <- read.csv(paste(dir,"det_mcmc_pop200_R6_g0.15_6_loglik.csv", sep=""))
re_R6_pop1000_seed1 <- read.csv(paste(dir,"det_mcmc_pop1000_R6_g0.15_11_loglik.csv", sep=""))

burnIn <- 2500*0.23

sir <- function(time, state, param) {
  
  # define model parameters in term of the natural parameters
  beta <- param[1] 
  gamma <- param[2]
  
  with(as.list(c(state, param)), {
    
    dS <- -(beta * S * I) 
    dI <- (beta * S * I) -(gamma * I)
    dR <-  gamma * I
    
    return(list(c(dS, dI, dR)))
  })
}

pre_sir <- function(run_stoch, sse_data){
# Time
timestep <- run_stoch$time[2] - run_stoch$time[1]
end <- max(run_stoch$time)
times <- seq(0, end, by = timestep)

# Initial population: N-1 susceptible, 1 infectious, 0 removed
init.values = c(
  S = run_stoch$S[1],
  I = run_stoch$I[1],
  R = run_stoch$R[1]
)
N = sum(init.values)

det_sir <- as.data.frame(ode(y = init.values, times = times, func = sir, parms = sse_data[1:2]))
}

letter_size = 3
axis_size = 2.5
yaxis_size = 2
number_size = 2.5

par(mfrow = c(2, 3), mar=c(5,6.5,0.4,0.2))
# R = 1.5, Pop = 50
N = 50
plot(true_R1.5_pop50_seed1$I, ylim = c(0, N), type = "l", col = "white", xlab = "", ylab = "", cex.axis = number_size)
  lines(true_R1.5_pop50_seed1$I, ylim = c(0, N), type = "l", col = "red", lwd = 3)
for (i in burnIn:nrow(re_R1.5_pop50_seed1)){
  sir_data <- pre_sir(true_R1.5_pop50_seed1, re_R1.5_pop50_seed1[i,])
  lines(sir_data$I, type = "l", lty = 2, col = "grey", xlab = " ", ylab = " ", lwd = 0.5)
}
text(140, 0.95*N, "A", cex = letter_size)
mtext("Number infected/removed", 2, line = 4, cex = yaxis_size, outer = FALSE)
# legend(100, 0.8*N, c("True infected", "Deterministic infected"), pch = 1, col = c("red", "black"), bty = "n")
# R = 1.5, Pop = 200
N = 200
plot(true_R1.5_pop200_seed1$I, ylim = c(0, N), type = "l", col = "white", xlab = "", ylab = "", cex.axis = number_size)
lines(true_R1.5_pop200_seed1$I, ylim = c(0, N), type = "l", col = "red", lwd = 3)
for (i in burnIn:nrow(re_R1.5_pop200_seed1)){
  sir_data <- pre_sir(true_R1.5_pop200_seed1, re_R1.5_pop200_seed1[i,])
  lines(sir_data$I, type = "l", lty = 2, col = "grey", xlab = " ", ylab = " ", lwd = 0.5)
}
text(140, 0.95*N, "B", cex = letter_size)
# legend(100, 0.8*N, c("True infected", "Deterministic infected"), pch = 1, col = c("red", "black"), bty = "n")
# R = 1.5, Pop = 1000
N = 1000
plot(true_R1.5_pop1000_seed1$I, ylim = c(0, N), type = "l", col = "white", xlab = "", ylab = "", cex.axis = number_size)
lines(true_R1.5_pop1000_seed1$I, ylim = c(0, N), type = "l", col = "red", lwd = 3)
for (i in burnIn:nrow(re_R1.5_pop1000_seed1)){
  sir_data <- pre_sir(true_R1.5_pop1000_seed1, re_R1.5_pop1000_seed1[i,])
  lines(sir_data$I, type = "l", lty = 2, col = "grey", xlab = " ", ylab = " ", lwd = 0.5)
}
text(140, 0.95*N, "C", cex = letter_size)
# legend(100, 0.8*N, c("True infected", "Deterministic infected"), pch = 1, col = c("red", "black"), bty = "n")
# R = 6, Pop = 50
N = 50
plot(true_R6_pop50_seed1$I, ylim = c(0, N), type = "l", col = "white", xlab = "", ylab = "", cex.axis = number_size)
lines(true_R6_pop50_seed1$I, ylim = c(0, N), type = "l", col = "red", lwd = 3)
for (i in 2:nrow(re_R1.5_pop50_seed1)){
  sir_data <- pre_sir(true_R6_pop50_seed1, re_R6_pop50_seed1[i,])
  lines(sir_data$I, type = "l", lty = 2, col = "grey", xlab = " ", ylab = " ", lwd = 0.5)
}
text(140, 0.95*N, "D", cex = letter_size)
mtext("Timestep", 1, line = 4, cex = yaxis_size)
mtext("Number infected/removed", 2, line = 4, cex = yaxis_size, outer = FALSE)
# legend(100, 0.8*N, c("True infected", "Deterministic infected"), pch = 1, col = c("red", "black"), bty = "n")
# R = 6, Pop = 200
N = 200
plot(true_R6_pop200_seed1$I, ylim = c(0, N), type = "l", col = "white", xlab = "", ylab = "", cex.axis = number_size)
lines(true_R6_pop200_seed1$I, ylim = c(0, N), type = "l", col = "red", lwd = 3)
for (i in 2:nrow(re_R1.5_pop200_seed1)){
  sir_data <- pre_sir(true_R6_pop200_seed1, re_R6_pop200_seed1[i,])
  lines(sir_data$I, type = "l", lty = 2, col = "grey", xlab = " ", ylab = " ", lwd = 0.5)
}
text(140, 0.95*N, "E", cex = letter_size)
mtext("Timestep", 1, line = 4, cex = yaxis_size)
# legend(100, 0.8*N, c("True infected", "Deterministic infected"), pch = 1, col = c("red", "black"), bty = "n")
# R = 6, Pop = 1000
N = 1000
plot(true_R6_pop1000_seed1$I, ylim = c(0, N), type = "l", col = "white", xlab = "", ylab = "", cex.axis = number_size)
lines(true_R6_pop1000_seed1$I, ylim = c(0, N), type = "l", col = "red", lwd = 3)
for (i in 2:nrow(re_R6_pop1000_seed1)){
  sir_data <- pre_sir(true_R6_pop1000_seed1, re_R6_pop1000_seed1[i,])
  lines(sir_data$I, type = "l", lty = 2, col = "grey", xlab = " ", ylab = " ", lwd = 0.5)
}
text(140, 0.95*N, "F", cex = letter_size)
mtext("Timestep", 1, line = 4, cex = yaxis_size)
# legend(100, 0.8*N, c("True infected", "Deterministic infected"), pch = 1, col = c("red", "black"), bty = "n")
@
\caption{Comparing infectious curves for a 2.5 million iteration MCMC with a deterministic process (dotted grey line) and the true simulated infectious curves (red line). Panels A-C depict outbreaks with an $R_{0}$ of 1.5, while panels D-F depict outbreaks with an $R_{0}$ of 6. The outbreaks in panels A and D have a population size of 50, while panels B and E have a population size of 200, and D and F have a population size of 1000.}
\label{det_mcmc_infcurve}
\end{center}
\end{figure}

\FloatBarrier
%%%%%%%%%%%%%%%%
%% Discussion %%
%%%%%%%%%%%%%%%%
\clearpage
\newpage
\section{Discussion}
% 5-10 pages

%%%%%%%%%%
\subsection{Summary of findings}

Both RE and MCMC inference methods observe the same positive correlation between $\beta$ and $\gamma$ as was determined by the heatmap for RE (Fig. \ref{re_heatmap}) and the contour plot for the MCMC (Fig. \ref{mcmc_contourplot}). The positive correlation is likely due to there being less of a difference whether the rates of infection and recovery are both increased or both reduced, as the net number of infected individuals remains the same. Though both methods observe the same trend, it can be noted that the 95\% intervals obtained through bootstrapping for the RE are narrower than the 95\% credible interval of the MCMC. Possibly relating to this, the 95\% interval of the RE method includes the true $\beta$ and $\gamma$ values less often than the 95\% credible interval of the MCMC method (Table \ref{re_pop50}-3, Table \ref{mcmc_pop50}-\ref{mcmc_pop1000}). It could be that if the MCMCs would have been run for more iterations and until convergence looked more likely, the 95\% credible interval would have also narrowed. That being said, it is unlikely that even this narrower credible interval would have been as narrow as the RE interval, as the difference in interval width was still prevalent for a population size of 50 and $R_{0}$ of 6, though the runs seemed seemed to have converged during the 3.5 million iteration runs (Fig. \ref{mcmc_beta_trace}D, Fig. \ref{mcmc_gamma_trace}D).

While the $\beta$ 95\% intervals for the RE method tended to be above the true value when $R_{0}$ was 1.5 and below the true value when $R_{0}$ was 6, the $\gamma$ 95\% intervals included the true value more often for the higher $R_{0}$ than the lower $R_{0}$ and more often as population size increased (Table \ref{re_pop50}-\ref{re_pop200}). It is possible that the RE inference method's overestimations of $\beta$ for outbreaks with an $R_{0}$ of 1.5 are due to the exclusion of non-starting outbreaks. This would affect smaller population sizes more than larger ones due to there being fewer non-starting outbreaks with increasing population size (Fig. 4). The RE method gave ranges of $\beta$ and $\gamma$ that were closer to their respective true values for outbreaks with an $R_{0}$ of 6. Within this outbreak severity, the RE method fared even better with increasing population size (Tables 1-3). This could be due to stochasticity being reduced with increasing population size as seen in Figure 3, where outbreaks with a population size of 1000 had smoother epidemic curves than outbreaks in populations of 50.

Compared to the RE method, the MCMC method was more reliable with including the true $\beta$ and $\gamma$ values in its 95\% credible intervals (Table \ref{mcmc_pop50}-\ref{mcmc_pop1000}), though most runs would likely have benefitted from being run for more than 3.5 million or even 17.5 million iterations. The running time of the MCMC method was slower than that of the RE inference method. Running 1000 iterations of the RE method took approximately 20 minutes, while running 3.5 million iterations took over 20 hours and 17.5 million iterations over 120 hours with the running time increasing with population size. 

The MCMC with a deterministic process for inference did not perform as well as the RE or standard MCMC method for the cases considered in this project. Based on the posterior traces (Fig. \ref{det_mcmc_beta}, Fig. \ref{det_mcmc_gamma}), it could be that with finetuning of the proposal function, the method could provide better estimates for $\beta$ and $\gamma$ than the ones presented in this report, though whether these estimates would exceed those of the RE is not certain. Alternatively, other methods of applying deterministic processes within an MCMC could be attempted. 

In this project a model with only a deterministic infectious curve was explored, with the removed curve remaining stochastic and susceptible curve being calculated from the two other curves. A MCMC method used by Elderd et al. involved a more deterministic process. They used a completely deterministic SEIR model in their likelihood function and used an MCMC algorithm that allowed multidimensional parameter draws to be exchanged with lower-dimensional ones \citep{Elderd2006}. Adjusting my method to incorporate both a deterministic susceptible and infected curve might also help my MCMC with a deterministic process perform better as this could eliminate the issues associated with accounting for negative numbers of susceptible individuals in some suggested outbreaks during the course of the MCMC.

\subsection{Open questions for discussion}
Considering that the RE inference method experienced improved $\beta$ and $\gamma$ estimates with increasing population size and higher $R_{0}$, it could be that eventually the RE inference method may consistently include the true $\beta$ and $\gamma$ values if investigated for even higher population sizes and $R_{0}$s. As the running time and number of iterations required for convergence for the MCMC increases with increasing population size, eventually a trade-off of the narrower 95\% intervals for a much quicker running time might become a reasonable option. Further investigations with larger population sizes and $R_{0}$s could be conducted to gain a better understanding of when this trade-off would become cost effective. Other types of compartmental model could also be explored to see if the trends found in this report hold true for other models. The MCMC inference method devised in this project is also not optimised for efficiency, so it is possible that the running time of the MCMC could be shortened in further work.

\subsection{Limitations}
Only one type of compartmental model, a closed SIR model, was considered in this project. Additionally, the model is very simple with no spatial modelling or other complicating matters. It could be argued though that if the aim of a study is to conduct complicated models including spatial aspects, a stochastic process must be used regardless and thus the question of whether or not to use a deterministic inference method becomes obsolete. 

Additionally, as mentioned previously, the MCMCs could have been run for longer than 3.5 million or even 17.5 million iterations to reach full convergence. Furthermore, the MCMCs could have been repeated for the different outbreaks from different parameter starting points to investigate if the runs from different starting points converge at similar $\beta$ and $\gamma$ values. That being said, the aim of this project was to compare different inference methods rather than perfect one method completely.

\subsection{Conclusions}
When estimating the ranges of $\beta$ and $\gamma$ through 95\% credible intervals and 95\% intervals for the MCMC and RE inference methods respecively, the MCMC inference method's ranges included the true $\beta$ and $\gamma$ values more often than the RE method. Within the RE method, the 95\% intervals included the true $\beta$ and $\gamma$ values more consistently as population size increased and with a higher $R_{0}$. Further work comparing the inference methods for higher $R_{0}$s and population sizes may find a threshold for which the RE inference method's 95\% interval might also consistently include the true $\beta$ and $\gamma$ values while being quicker to run than a MCMC. 

%%%%%%%%%%%%%%%%
%% References %%
%%%%%%%%%%%%%%%%

\newpage
\phantomsection
\addcontentsline{toc}{section}{References}

% \bibliographystyle{ieeetr}
\bibliographystyle{unsrtnat}
\bibliography{/home/evelina/Documents/Mendeley/MRes_stochastic_deterministic.bib}

% citation()
% citation("deSolve")

% 5 pages

%%%%%%%%%%

%%%%%%%%%%%%%%
%% Appendix %%
%%%%%%%%%%%%%%

% \section{Appendix}

\end{document}